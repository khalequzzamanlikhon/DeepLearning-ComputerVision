{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have already discussed a little about sequence to sequence with RNNs. Now we will discuss a bit more in details.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sequence to sequence with RNNs\n",
    "say we are going to translate from one language to another language. For this purpose we will use encoder and decoder.\n",
    "\n",
    "**Input**: Sequence $x_1,...,x_T$\n",
    "\n",
    "**output**: sequence $y_1,....,y_T$\n",
    "\n",
    "**Encoder**: $h_t = f_w(x_t,h_{t-1})$\n",
    "\n",
    "**Decoder**: $s_t = g_u(y_{t-1},h_{t-1},c)$\n",
    "\n",
    "Encoder will summarise the words of sentences. From the final hidden state of encoder we will compute **Initial decoder state $s_0$** and **Context vector c**.(often $c=h_t$). This context vector will pass to every hidden state of decoder.\n",
    "At first time step, decoder will take the initial decoder state and context vector as well as the start token $y_0$ and will generate the first word of the output sentece. This process will continue. One thing to remember strictly that the context vector will be pass to each hidden state.\n",
    "\n",
    "##### Attention:\n",
    "This will work fine for one or a couple of sentences. Because it looks fine to summarize one or a couple of sentences into one context vector c. But when the sequence is so long say an entire book this is kind of problematic to summarize the whole book into one single contex vector. Instead we can compute context vector at each time step of decoder network. By doing so we are allowing the decoder to choose or reconstruct a new context vector that focuse on different part of the input sequence. The way we formalize this intuition is with a mechanism called **ATTENTION**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](images/a1.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequenc to Sequenc with RNNs and Attention\n",
    "Now we are going to write down some allignment function. This function will be parameterized as a fully connected network which will take two vectors one is the current hidden state of the encoder and other is one of the hidden states of the encoder. This function will then output a score to say how much should be attend to each hidden state of the encoder given the current hidden state of the decoder.\n",
    "\n",
    "Say at initial time step in decoder, by applying the function we will compare the $s_0$ and h1 and that will give us this allignment scalar score e11. That is basically how much the model think that the hidden state h1 will be necessary for predicting the word that comes after output hidden state $s_0$. Similarly we will run this function for every hidden state of the encoder and will get e12,e13....Then we will apply softmax to each of these allignment scores. Then the probability distribution of the scores will say for the first hidden state of the decoder how much weight do we want to put on each hidden state of the encoder. This distribution is called attention. Next we will take weighted sum of each of the hidden states of the encoded sequence and will sum them up according to these predicted probability scores. This will produce our context vector c1 that we are going to use for the firt tiem step of the decoder network.\n",
    "This means the network has sort of predicted for itself how much weight do we want to put on each of the hidden states on the input sequence and we can dynamically shift that weight around for each time step of the decoder network that will allow us to predict a different context vector at each decoder time step. So at the initial time step of decoder, it will take just computed context vector, first word and start token and it will output first predicted output of the sentence.\n",
    "\n",
    "**Intuition**: \n",
    "when we are trying to generate these ouput words of the output sentence then each word of these output sentence probably corresponds to one or multiple words in the input sentence. So, we're trying to dynamically generate this context vector that causes the ouput the decoder network to allow it to focus on different parts of the input of the encoder network for each time step. So if we are translating e.g. \"we are eating bread\" then the first word we want to translate is \"we are\". This means we want to put high weights on the first two words of the english sentence and relatively low weights on the latter two words. This will allow the decoder to focus on important part of the sentence. All the opeatations in the computational graph is differntiable which means that we don't need to supervise the network to focus, it will do itself. Then in the second time step decoder will take new hidden state $s_1$ and hidden states from encoder and will produce e21,e22,...next step will be same as before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](images/a2..PNG)\n",
    "\n",
    "![alt text](images/a3.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the input and output are both sequence. But we can actually apply the attention model for other type of input. For example in captioning where input is image. In the next section I will discuss about image captioning."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
