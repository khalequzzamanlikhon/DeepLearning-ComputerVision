{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient flow for vanilla RNN\n",
    "\n",
    "**Intro**:\n",
    "\n",
    "We already know, RNN takes previous state $h_{t-1}$ and input $x_t$ and pass the calculated result through tanh, $h_t = tanh(W_{hh}h_{t-1} + W_{xh}x_t)$\n",
    "for backpropagation we need to find out how the last time step affects the weights of the first time step $W_{hh}$\n",
    "\n",
    "**calculating gradients**:\n",
    "The partial derivative of $h_t$ w.r.t. $h_{t-1}$ is, $\\frac{\\partial h_t}{\\partial h_{t-1}} =  tanh^{'}(W_{hh}h_{t-1} + W_{xh}x_t)W_{hh}$\n",
    "\n",
    "We update $W_{hh}$ by getting derivative of loss at last time step w.r.t. $W_{hh}$.\n",
    "\n",
    "$\\begin{aligned}\n",
    "\\frac{\\partial L_{t}}{\\partial W_{hh}} = \\frac{\\partial L_{t}}{\\partial h_{t}} \\frac{\\partial h_{t}}{\\partial h_{t-1} } \\dots \\frac{\\partial h_{1}}{\\partial W_{hh}} \\\\\n",
    "= \\frac{\\partial L_{t}}{\\partial h_{t}}(\\prod_{t=2}^{T} \\frac{\\partial h_{t}}{\\partial  h_{t-1}})\\frac{\\partial h_{1}}{\\partial W_{hh}} \\\\\n",
    "= \\frac{\\partial L_{t}}{\\partial h_{t}}(\\prod_{t=2}^{T}  tanh^{'}(W_{hh}h_{t-1} + W_{xh}x_t)W_{hh}^{T-1})\\frac{\\partial h_{1}}{\\partial W_{hh}} \\\\\n",
    "\\end{aligned}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Description**\n",
    "\n",
    "![alt text](images/g0.PNG)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First gradient will flow backwards through tanh gate, then matrix multipication gate. When we backpropagate through matrix multiplication gate, we end up multiplying by some part of the weight matrix. So, if we are sticking many of these recurrent network cells, when we start computing the gradient of loss w.r.t. $h_0$ we need to backpropagate through each of these RNN cells and every time we backpropagate through these cells, we will pick up one of these W transpose factors. Thus, the final expression for the gradient on  $h_0$ will involve many many factors of this weight matrix which could be kind of bad. If we just imagine a case of just an scalar value, we multiply by this same scalar over and over again. If the hidden time steps is large then say more thant 100, multiplying a number again again can lead to two scenarios. \n",
    "- If the number is greater than one then gradient exploding will happen.\n",
    "-  If the number is less than one then vanishing gradient will happen.\n",
    "- **It will not happen if the number is exactly 1**. \n",
    "The same intuitinon is applied to matrix case. Bur rather than an absolute scalar, we will look at the largest singular value of this weight matrix.\n",
    "- If largest singular value> 1: Gradient exploding \n",
    "- If largest singular value<1: vanishing gradinet\n",
    "- if it's 1: None of the above case happens\n",
    "\n",
    "Based on this description we can also\n",
    "\n",
    "We have already know the gradient equation,\n",
    "\n",
    "$\\frac{\\partial L_{t}}{\\partial W_{hh}} = \\frac{\\partial L_{t}}{\\partial h_{t}}(\\prod_{t=2}^{T}  tanh^{'}(W_{hh}h_{t-1} + W_{xh}x_t)W_{hh}^{T-1})\\frac{\\partial h_{1}}{\\partial W_{hh}}$\n",
    "\n",
    "**Vanishing Gradients**: Since tanh is always between negative and positive one therefore $tanh^{'}(W_{hh}h_{t-1} + W_{xh}x_t)$ will always be less than one. Thus as time steep \"t\" gets larger the gradient tends to become zero at time of back propagating. This is known as vanishing gradient. This means gradients from future time step rarely impact gradients at the very first time step.\n",
    "\n",
    "**Removing Non-linearity**: if we remove non-linearity(tanh) then,\n",
    "  - **Exploding Gradeints**:\n",
    "    - If largest singular  value of $W_{hh}$ greater than 1 then exploding gradients\n",
    "    - **Solution**: Gradient clipping: clipping larger gradient values to a maximum threshhold\n",
    "  - **Vanishing gradients**:\n",
    "    - if the largest singular value of $W_{hh}$ is less than 1 then vanishing gradinet.\n",
    "    **Solution**: It still remains in RNN. That's why **LSTM** is introduced.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](images/g1.PNG) \n",
    "\n",
    "![alt text](images/g2.PNG) \n",
    "\n",
    "![alt text](images/g3.PNG) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next section, I will discuss about **LSTM**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
