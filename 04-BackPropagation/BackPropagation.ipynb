{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Recap**:\n",
    "\n",
    "Score Function: $s=\\sum(x,W)=Wx$\n",
    "\n",
    "Loss on a single example:  $L_i = \\sum_{j\\neq y_i} \\max(0, s_j - s_{y_i} + \\Delta)$\n",
    "\n",
    "Loss on the whole training data: $L = \\frac{1}{N} \\sum_i \\sum_{j\\neq y_i} \\left[ \\max(0, f(x_i; W)_j - f(x_i; W)_{y_i} + \\Delta) \\right] + \\lambda \\sum_k\\sum_l W_{k,l}^2$\n",
    "\n",
    "**Goal**: want  $\\nabla _wf (L)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation\n",
    "is a way of computing gradients of expressions through recursive application of **chain rule**.\n",
    "\n",
    "#### Problem Statement\n",
    "We are given some function f(x)\n",
    " where x\n",
    " is a vector of inputs and we are interested in computing the gradient of f\n",
    " at x\n",
    " (i.e. ∇f(x)\n",
    " ).\n",
    "\n",
    " #### Motivation\n",
    " Our target function finally f will eventuallt correspond to the loss function L.We condider input examples as constant and the Weights as variable. Our main goal in machine learning is to suit the parameter W as perfectly as possible so that our model performs well. We find this optimized weight through upadates. These updates are done by calculating gradient using  **Backpropagation** algorithm.\n",
    "\n",
    " **NB**: Although we don't calculate gradients of input datas. But it can be done and sometimes useful to see what NN might be doing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Simple expressions and interpretation of the gradient\n",
    " **Multiplication**: $f(x,y) = x y \\hspace{0.5in} \\rightarrow \\hspace{0.5in} \\frac{\\partial f}{\\partial x} = y \\hspace{0.5in} \\frac{\\partial f}{\\partial y} = x$\n",
    "\n",
    " **intuition**:the derivative on each variable tells you the sensitivity of the whole expression on its value. For example, $if x=4,y=−3$\n",
    " then $f(x,y)=−12$\n",
    " and the derivative on $ x\\frac{\\partial f}{\\partial x} = -3$\n",
    ". This tells us that if we were to increase the value of this variable by a tiny amount, the effect on the whole expression would be to decrease it (due to the negative sign), and by three times that amount.\n",
    "\n",
    "**Gradient**:As mentioned, the gradient ∇f\n",
    " is the vector of partial derivatives, so we have that $ \\nabla f = [\\frac{\\partial f}{\\partial x}, \\frac{\\partial f}{\\partial y}] = [y, x]$\n",
    "\n",
    " **Addition** $f(x,y) = x + y \\hspace{0.5in} \\rightarrow \\hspace{0.5in} \\frac{\\partial f}{\\partial x} = 1 \\hspace{0.5in} \\frac{\\partial f}{\\partial y} = 1$\n",
    "\n",
    " **Max gate(gradient router**:) $f(x,y) = \\max(x, y) \\hspace{0.5in} \\rightarrow \\hspace{0.5in} \\frac{\\partial f}{\\partial x} = \\mathbb{1}(x >= y) \\hspace{0.5in} \\frac{\\partial f}{\\partial y} = \\mathbb{1}(y >= x)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compound expressions with chain rule (Backpropagation Intuition)\n",
    "#### Computational Graphs\n",
    "A computational graph is a way to represent a math function in the language of graph theory. Recall the premise of graph theory: nodes are connected by edges, and everything in the graph is either a node or an edge.\n",
    "\n",
    "** Graph Node**: In simpler terms , every nodes on the computational graph performs math operations on its input(s) and results an output. We can calculate the **local gradient** of each node (change of the output of a node w.r.t. its input(s)). This local gradients will be useful in backporpagation which we are going to see in later section.\n",
    "\n",
    "**Why we use computational graph**?: In neural network eventually we oupts predictions based on many complex function operations. It is really tough to formulate them. On the other hand its easy to represent them in graphs. Additionally, using this compoutational graph it's easy to calculate **local gradients**.\n",
    "\n",
    "For example, consider the relatively simple expression: f(x, y, z) = (x + y) * z.\n",
    "\n",
    "![alt text](/Images/cg.png \"computational graph\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets see with an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set some inputs\n",
    "x = -2; y = 5; z = -4\n",
    "\n",
    "# perform the forward pass\n",
    "q = x + y # q becomes 3\n",
    "f = q * z # f becomes -12\n",
    "\n",
    "# perform the backward pass (backpropagation) in reverse order:\n",
    "# first backprop through f = q * z\n",
    "dfdz = q # df/dz = q, so gradient on z becomes 3\n",
    "dfdq = z # df/dq = z, so gradient on q becomes -4\n",
    "dqdx = 1.0\n",
    "dqdy = 1.0\n",
    "# now backprop through q = x + y\n",
    "dfdx = dfdq * dqdx  # The multiplication here is the chain rule!\n",
    "dfdy = dfdq * dqdy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are left with the gradient in the variables [dfdx,dfdy,dfdz], which tell us the sensitivity of the variables x,y,z on f!. This is the simplest example of backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
