{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Recap**:\n",
    "\n",
    "Score Function: $s=\\sum(x,W)=Wx$\n",
    "\n",
    "Loss on a single example:  $L_i = \\sum_{j\\neq y_i} \\max(0, s_j - s_{y_i} + \\Delta)$\n",
    "\n",
    "Loss on the whole training data: $L = \\frac{1}{N} \\sum_i \\sum_{j\\neq y_i} \\left[ \\max(0, f(x_i; W)_j - f(x_i; W)_{y_i} + \\Delta) \\right] + \\lambda \\sum_k\\sum_l W_{k,l}^2$\n",
    "\n",
    "**Goal**: want  $\\nabla _wf (L)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation\n",
    "is a way of computing gradients of expressions through recursive application of **chain rule**.\n",
    "\n",
    "#### Problem Statement\n",
    "We are given some function f(x)\n",
    " where x\n",
    " is a vector of inputs and we are interested in computing the gradient of f\n",
    " at x\n",
    " (i.e. ∇f(x)\n",
    " ).\n",
    "\n",
    " #### Motivation\n",
    " Our target function f will eventually correspond to the loss function L.We condider input examples as constant and the Weights as variable. Our main goal in machine learning is to suit the parameter W as perfectly as possible so that our model performs well. We find this optimized weight through upadates. These updates are done by calculating gradient using  **Backpropagation** algorithm.\n",
    "\n",
    " **NB**: Although we don't calculate gradients of input datas. But it can be done and sometimes useful to see what NN might be doing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Simple expressions and interpretation of the gradient\n",
    " **Multiplication**: $f(x,y) = x y \\hspace{0.5in} \\rightarrow \\hspace{0.5in} \\frac{\\partial f}{\\partial x} = y \\hspace{0.5in} \\frac{\\partial f}{\\partial y} = x$\n",
    "\n",
    " **intuition**:the derivative on each variable tells you the sensitivity of the whole expression on its value. For example, $if x=4,y=−3$\n",
    " then $f(x,y)=−12$\n",
    " and the derivative on $ x\\frac{\\partial f}{\\partial x} = -3$\n",
    ". This tells us that if we were to increase the value of this variable by a tiny amount, the effect on the whole expression would be to decrease it (due to the negative sign), and by three times that amount.\n",
    "\n",
    "**Gradient**:As mentioned, the gradient ∇f\n",
    " is the vector of partial derivatives, so we have that $ \\nabla f = [\\frac{\\partial f}{\\partial x}, \\frac{\\partial f}{\\partial y}] = [y, x]$\n",
    "\n",
    " **Addition** $f(x,y) = x + y \\hspace{0.5in} \\rightarrow \\hspace{0.5in} \\frac{\\partial f}{\\partial x} = 1 \\hspace{0.5in} \\frac{\\partial f}{\\partial y} = 1$\n",
    "\n",
    " **Max gate(gradient router**:) $f(x,y) = \\max(x, y) \\hspace{0.5in} \\rightarrow \\hspace{0.5in} \\frac{\\partial f}{\\partial x} = \\mathbb{1}(x >= y) \\hspace{0.5in} \\frac{\\partial f}{\\partial y} = \\mathbb{1}(y >= x)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compound expressions with chain rule (Backpropagation Intuition)\n",
    "#### Computational Graphs\n",
    "A computational graph is a way to represent a math function in the language of graph theory. Recall the premise of graph theory: nodes are connected by edges, and everything in the graph is either a node or an edge.\n",
    "\n",
    "** Graph Node**: In simpler terms , every nodes on the computational graph performs math operations on its input(s) and results an output. We can calculate the **local gradient** of each node (change of the output of a node w.r.t. its input(s)). This local gradients will be useful in backporpagation which we are going to see in later section.\n",
    "\n",
    "**Why we use computational graph**?: In neural network eventually we oupts predictions based on many complex function operations. It is really tough to formulate them. On the other hand its easy to represent them in graphs. Additionally, using this compoutational graph it's easy to calculate **local gradients**.\n",
    "\n",
    "For example, consider the relatively simple expression: $f(x, y, z) = (x + y) * z$.This expression can be broken down into two expressions: $q=x+y$\n",
    " and $f=qz$. According to demonstration given above there are two operational node. We can find derivatives(local gradients) of these two node w.r.t their input(s) on output separtely.\n",
    "\n",
    " f is just multiplication of q and z, so $\\frac{\\partial f}{\\partial q} = z, \\frac{\\partial f}{\\partial z} = q$, and q\n",
    " is addition of x and y so $\\frac{\\partial q}{\\partial x} = 1$, $\\frac{\\partial q}{\\partial y} = 1$\n",
    "\n",
    "\n",
    "![alt text](Images/cg.png \"computational graph\")\n",
    "\n",
    "\n",
    "***ULTIMATE GUIDE***: Every node computes two things--\n",
    "          - ouput\n",
    "          - loacal gradient(the change of its output w.r.t. its input(s))\n",
    "    However, once the forward pass is over, during backpropagation the gate will eventually learn about the gradient of its output value on the final output of the entire circuit. Chain rule says that the gate should take that gradient and multiply it into every gradient it normally computes for all of its inputs (**local gradient** X **upstream gradietnt**).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets see with an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set some inputs\n",
    "x = -2; y = 5; z = -4\n",
    "\n",
    "# perform the forward pass\n",
    "q = x + y # q becomes 3\n",
    "f = q * z # f becomes -12\n",
    "\n",
    "# perform the backward pass (backpropagation) in reverse order:\n",
    "# first backprop through f = q * z\n",
    "dfdz = q # df/dz = q, so gradient on z becomes 3\n",
    "dfdq = z # df/dq = z, so gradient on q becomes -4\n",
    "dqdx = 1.0\n",
    "dqdy = 1.0\n",
    "# now backprop through q = x + y\n",
    "dfdx = dfdq * dqdx  # The multiplication here is the chain rule!\n",
    "dfdy = dfdq * dqdy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are left with the gradient in the variables [dfdx,dfdy,dfdz], which tell us the sensitivity of the variables x,y,z on f!. This is the simplest example of backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modularity: Sigmoid example:\n",
    "Any kind of differentiable function can act as a gate, and we can group multiple gates into a single gate, or decompose a function into multiple gates whenever it is convenient. Lets look at another expression that illustrates this point: $f(w,x) = \\frac{1}{1+e^{-(w_0x_0 + w_1x_1 + w_2)}}$ . say , $f(w,x) = \\frac{1}{1+e^{-(w_0x_0 + w_1x_1 + w_2)}}$ this part is computed in one node. We can then consider another node(sigmoid node) that will take this as inut and perfomrs sigmoid opearation. or we can decompose the function of the sigmoid node inot multiple nodes.\n",
    "\n",
    "![alt text](Images/scg.PNG \"sigmoid functions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### derivative of sigmoid function($\\sigma(x)(1-\\sigma(x))$):\n",
    "$\\begin{align}\n",
    "\\frac{d}{dx}\\sigma(x) & = \\frac{d}{dx} \\frac{1}{1+e^{-x}}\\\\\n",
    "& = \\frac{d}{dx}\\big( 1+ e^{-x} \\big) ^{-1} \\quad[\\text{apply chain rule}]\\\\\n",
    "& = -(1 + e^{-x})^{-2} \\cdot \\frac{d}{dx}(1+e^{-x}) \\quad[\\text{apply sum rule}] \\\\\n",
    "& = -(1 + e^{-x})^{-2} \\cdot \\bigg(\\frac{d}{dx}1 + \\frac{d}{dx}e^{-x}\\bigg) \\\\\n",
    "& = -(1 + e^{-x})^{-2} \\cdot \\frac{d}{dx}e^{-x} \\quad[\\text{apply chain rule}]\\\\\n",
    "& = -(1 + e^{-x})^{-2} \\cdot e^{-x}\\frac{d}{dx} (-x)\\\\\n",
    "& = -(1 + e^{-x})^{-2} \\cdot \\big(- e^{-x} \\big)\\\\\n",
    "& = \\frac{1}{(1 + e^{-x})^{2}} \\cdot e^{-x} \\\\\n",
    "& = \\frac{e^{-x}}{(1 + e^{-x})^{2}}\n",
    "\\end{align}$\n",
    "\n",
    "We can further simplify the derivative to the expression $\\sigma(x)(1-\\sigma(x))$\n",
    "$\\begin{align}\n",
    " \\frac{e^{-x}}{(1 + e^{-x})^{2}} &= \\frac{e^{-x}}{1 + e^{-x}}\\cdot\\frac{1}{1 + e^{-x}}  \\\\\n",
    "& = \\frac{-1 + 1 + e^{-x}}{1 + e^{-x}}\\cdot\\frac{1}{1 + e^{-x}}\\\\\n",
    "& = \\bigg(\\frac{-1 }{1 + e^{-x}} + \\frac{1 + e^{-x} }{1 + e^{-x}} \\bigg)\\cdot\\frac{1}{1 + e^{-x}}\\\\\n",
    "& = \\bigg(\\frac{-1 }{1 + e^{-x}} + 1 \\bigg)\\cdot\\frac{1}{1 + e^{-x}}\\\\\n",
    "& = \\bigg(1 - \\frac{1 }{1 + e^{-x}}  \\bigg)\\cdot\\frac{1}{1 + e^{-x}}\\\\\n",
    "& = \\big(1-\\sigma(x)\\big) \\cdot \\sigma(x)\n",
    "\\end{align}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backpropagation of sigmoid neuron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "w = [2,-3,-3] # assume some random weights and data\n",
    "x = [-1, -2]\n",
    "\n",
    "# forward pass\n",
    "dot = w[0]*x[0] + w[1]*x[1] + w[2]\n",
    "f = 1.0 / (1 + math.exp(-dot)) # sigmoid function\n",
    "\n",
    "# backward pass through the neuron (backpropagation)\n",
    "ddot = (1 - f) * f # gradient on dot variable, using the sigmoid gradient derivation\n",
    "dx = [w[0] * ddot, w[1] * ddot] # backprop into x\n",
    "dw = [x[0] * ddot, x[1] * ddot, 1.0 * ddot] # backprop into w   1.0*ddot- for bias term\n",
    "# we're done! we have the gradients on the inputs to the circuit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backprop in practice: Staged computation\n",
    "$f(x,y) = \\frac{x + \\sigma(y)}{\\sigma(x) + (x+y)^2}$  computing its gradient is really a daunting task. But using staged computation and backpropagation its not that hard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Forward pass\n",
    "\n",
    "x = 3 # example values\n",
    "y = -4\n",
    "\n",
    "# forward pass\n",
    "sigy = 1.0 / (1 + math.exp(-y)) # sigmoid in numerator   #(1)\n",
    "num = x + sigy # numerator                               #(2)\n",
    "sigx = 1.0 / (1 + math.exp(-x)) # sigmoid in denominator #(3)\n",
    "xpy = x + y                                              #(4)\n",
    "xpysqr = xpy**2                                          #(5)\n",
    "den = sigx + xpysqr # denominator                        #(6)\n",
    "invden = 1.0 / den                                       #(7)\n",
    "f = num * invden # done!                                 #(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " by the end of the expression we have computed the forward pass. Notice that we have structured the code in such way that it contains multiple intermediate variables, each of which are only simple expressions for which we already know the local gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# backprop f = num * invden\n",
    "dnum = invden # gradient on numerator                             #(8)\n",
    "dinvden = num                                                     #(8)\n",
    "# backprop invden = 1.0 / den \n",
    "dden = (-1.0 / (den**2)) * dinvden                                #(7)\n",
    "# backprop den = sigx + xpysqr\n",
    "dsigx = (1) * dden                                                #(6)\n",
    "dxpysqr = (1) * dden                                              #(6)\n",
    "# backprop xpysqr = xpy**2\n",
    "dxpy = (2 * xpy) * dxpysqr                                        #(5)\n",
    "# backprop xpy = x + y\n",
    "dx = (1) * dxpy                                                   #(4)\n",
    "dy = (1) * dxpy                                                   #(4)\n",
    "# backprop sigx = 1.0 / (1 + math.exp(-x))\n",
    "dx += ((1 - sigx) * sigx) * dsigx # Notice += !! See notes below  #(3)\n",
    "# backprop num = x + sigy\n",
    "dx += (1) * dnum                                                  #(2)\n",
    "dsigy = (1) * dnum                                                #(2)\n",
    "# backprop sigy = 1.0 / (1 + math.exp(-y))\n",
    "dy += ((1 - sigy) * sigy) * dsigy                                 #(1)\n",
    "# done! phew"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cache forward pass variables:\n",
    "To compute the backward pass it is very helpful to have some of the variables that were used in the forward pass. In practice you want to structure your code so that you cache these variables, and so that they are available during backpropagation. If this is too difficult, it is possible (but wasteful) to recompute them.\n",
    "\n",
    "#### Gradients add up at forks:\n",
    "The forward expression involves the variables x,y multiple times, so when we perform backpropagation we must be careful to use += instead of = to accumulate the gradient on these variables (otherwise we would overwrite it). This follows the multivariable chain rule in Calculus, which states that if a variable branches out to different parts of the circuit, then the gradients that flow back to it will add."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradients for vectorized operations\n",
    "Matrix-Matrix multiply gradient. Possibly the most tricky operation is the matrix-matrix multiplication (which generalizes all matrix-vector and vector-vector) multiply operations:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# forward pass\n",
    "import numpy as np\n",
    "W = np.random.randn(5, 10)\n",
    "X = np.random.randn(10, 3)\n",
    "D = W.dot(X)\n",
    "\n",
    "# now suppose we had the gradient on D from above in the circuit\n",
    "dD = np.random.randn(*D.shape) # same shape as D\n",
    "dW = dD.dot(X.T) #.T gives the transpose of the matrix\n",
    "dX = W.T.dot(dD)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
