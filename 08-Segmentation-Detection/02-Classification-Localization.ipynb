{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "In classification problem we have an input image and our task is to label that image only. But sometimes we might want to know the label of the image as well as the location. This prroblem is known as classification and locaalizatioon. For example in our example the cat is on a field. Our task is to label the image as cat and the location as field. We already know the classifcation process. Here, localization refers to the bounding box of the object (in our example bounding box around the cat).\n",
    "\n",
    "The differences between object detection and \"classification and localization\" is that in the former problem we may have multiple object but in latter case there is only one object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Procedure\n",
    "\n",
    "We take our input image and pass it through a giant convolutional neural network(in our example: Alex Net) which will summarize the input image into a vector. Now unlike classification problem, we will feed this vector into two separate fully connceted layers instead of one. One will give class scores and the other will give four numbers height, width and the coordinates of the bounding box x and y. \n",
    "\n",
    "Now we will have two losses for this two different outputs. Moreover, we do this task in a fully supervised way. We assume that each of out training images are annoted with both a category label and also a ground truth bounding box for that category in the image. So, now we have two loss functions as well. One is famous softmax loss and the other is simply L2 loss. We might take other loss functions for the second case like L1, smooth L1 etc..\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](images/cl1.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have two scalars(two losses) for gradient computing and we want to minimize both. To do so we will take another hyperparameters that gives us some weights. Now we will take wighted sum of these loss functions to give our final scalar loss. Then we will take gradients w.r.t. this new scalar(weighted loss).\n",
    "\n",
    "This is also tricky because, this additional hyperparameter is to be set. We can apply different set of this hyperparameter and observe which performs better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This proess of classification and localization can be applied to human pose estimation. In this case, the input is an image of a person and we want to output the positions of the joints for that person. This will allow the network to prdict where the his arms were, where his legs were staff like that.\n",
    "\n",
    "We assume every person has same number of joints naturally. However it might not be the case all the time, but it works for the network. Generally the datasets for this problem defines a person's pose by 14 joint positions, their feet, knees, their hips etc. Therefore our task will be to output 14 numbers giving the (x,y) co-ordinates for each of the joints. We will apply regression loss(other than softmax and cross entropy loss) on each of those 14 predicted points and train the network with back propagation again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](images/cl2.PNG) ![alt text](images/cl3.PNG) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation\n",
    "\n",
    "Now, it's time for implementation of what we learned about calssification and localization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import tensorflow_datasets as tfds\n",
    "from tensorflow.keras import layers, Model\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load PASCAL VOC dataset\n",
    "(ds_train, ds_test), ds_info = tfds.load('voc',\n",
    " split=['train', 'validation'],\n",
    " with_info=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tfds.core.DatasetInfo(\n",
      "    name='voc',\n",
      "    full_name='voc/2007/4.0.0',\n",
      "    description=\"\"\"\n",
      "    This dataset contains the data from the PASCAL Visual Object Classes Challenge,\n",
      "    corresponding to the Classification and Detection competitions.\n",
      "    \n",
      "    In the Classification competition, the goal is to predict the set of labels\n",
      "    contained in the image, while in the Detection competition the goal is to\n",
      "    predict the bounding box and label of each individual object.\n",
      "    WARNING: As per the official dataset, the test set of VOC2012 does not contain\n",
      "    annotations.\n",
      "    \"\"\",\n",
      "    config_description=\"\"\"\n",
      "    This dataset contains the data from the PASCAL Visual Object Classes Challenge\n",
      "    2007, a.k.a. VOC2007.\n",
      "    \n",
      "    A total of 9963 images are included in this dataset, where each image\n",
      "    contains a set of objects, out of 20 different classes, making a total of\n",
      "    24640 annotated objects.\n",
      "    \n",
      "    \"\"\",\n",
      "    homepage='http://host.robots.ox.ac.uk/pascal/VOC/voc2007/',\n",
      "    data_path='C:\\\\Users\\\\klikh\\\\tensorflow_datasets\\\\voc\\\\2007\\\\4.0.0',\n",
      "    file_format=tfrecord,\n",
      "    download_size=868.85 MiB,\n",
      "    dataset_size=837.73 MiB,\n",
      "    features=FeaturesDict({\n",
      "        'image': Image(shape=(None, None, 3), dtype=uint8),\n",
      "        'image/filename': Text(shape=(), dtype=string),\n",
      "        'labels': Sequence(ClassLabel(shape=(), dtype=int64, num_classes=20)),\n",
      "        'labels_no_difficult': Sequence(ClassLabel(shape=(), dtype=int64, num_classes=20)),\n",
      "        'objects': Sequence({\n",
      "            'bbox': BBoxFeature(shape=(4,), dtype=float32),\n",
      "            'is_difficult': bool,\n",
      "            'is_truncated': bool,\n",
      "            'label': ClassLabel(shape=(), dtype=int64, num_classes=20),\n",
      "            'pose': ClassLabel(shape=(), dtype=int64, num_classes=5),\n",
      "        }),\n",
      "    }),\n",
      "    supervised_keys=None,\n",
      "    disable_shuffling=False,\n",
      "    splits={\n",
      "        'test': <SplitInfo num_examples=4952, num_shards=4>,\n",
      "        'train': <SplitInfo num_examples=2501, num_shards=2>,\n",
      "        'validation': <SplitInfo num_examples=2510, num_shards=2>,\n",
      "    },\n",
      "    citation=\"\"\"@misc{pascal-voc-2007,\n",
      "    \tauthor = \"Everingham, M. and Van~Gool, L. and Williams, C. K. I. and Winn, J. and Zisserman, A.\",\n",
      "    \ttitle = \"The {PASCAL} {V}isual {O}bject {C}lasses {C}hallenge 2007 {(VOC2007)} {R}esults\",\n",
      "    \thowpublished = \"http://www.pascal-network.org/challenges/VOC/voc2007/workshop/index.html\"}\"\"\",\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(ds_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to filter examples with a single object\n",
    "def filter_single_object_examples(dataset):\n",
    "    single_object_images = []\n",
    "    single_object_annotations = []\n",
    "    single_object_classes = []\n",
    "\n",
    "    for example in dataset:\n",
    "        if len(example['objects']['bbox']) == 1:  # Check if there's a single object\n",
    "            single_object_images.append(example['image'])\n",
    "            single_object_annotations.append(example['objects']['bbox'][0])  # Select the only annotation\n",
    "            single_object_classes.append(example['labels'])  # Assuming label is the key for the class\n",
    "\n",
    "    return single_object_images, single_object_classes, single_object_annotations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter train and test datasets for single object examples\n",
    "x_train, y_train_class,y_train_annotations= filter_single_object_examples(ds_train)\n",
    "x_test, y_test_class,y_test_annotations = filter_single_object_examples(ds_test)\n",
    "\n",
    "# one hot encoding for the class labels\n",
    "y_train_class=to_categorical(y_train_class,num_classes=20)\n",
    "y_test_class=to_categorical(y_test_class,num_classes=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess images (resize, normalize, etc.)\n",
    "def preprocess_images(images):\n",
    "    processed_images = []\n",
    "    for image in images:\n",
    "        processed_image = tf.image.resize(image, (224,224))  # Resize the image\n",
    "        # Add more preprocessing steps if required\n",
    "        processed_images.append(processed_image)\n",
    "    return processed_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess images\n",
    "x_train = np.array(preprocess_images(x_train))\n",
    "x_test = np.array(preprocess_images(x_test))\n",
    "\n",
    "# Convert annotations to numpy arrays\n",
    "# y_train = np.array(y_train)\n",
    "# y_test = np.array(y_test)\n",
    "\n",
    "y_train_class = np.array(y_train_class)\n",
    "y_train_annotations = np.array(y_train_annotations)\n",
    "y_test_class = np.array(y_test_class)\n",
    "y_test_annotations = np.array(y_test_annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(905, 224, 224, 3)\n",
      "(905, 20)\n",
      "(905, 4)\n",
      "(960, 4)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(y_train_class.shape)\n",
    "print(y_train_annotations.shape)\n",
    "print(y_test_annotations.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Images: 905, Train class: 905,  Train Annotations: 905\n",
      "Test Images: 960, Test class: 960 Test Annotations: 960\n"
     ]
    }
   ],
   "source": [
    "# Check if the shapes are consistent before proceeding to model training\n",
    "print(f\"Train Images: {len(x_train)}, Train class: {len(y_train_class)},  Train Annotations: {len(y_train_annotations)}\")\n",
    "print(f\"Test Images: {len(x_test)}, Test class: {len(y_test_class)} Test Annotations: {len(y_test_annotations)}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model architecture\n",
    "def create_classification_localization_model(input_shape):\n",
    "    input_layer = layers.Input(shape=input_shape)\n",
    "\n",
    "    # Convolutional base for feature extraction\n",
    "    base_model = tf.keras.applications.ResNet50(weights='imagenet', include_top=False, input_tensor=input_layer)\n",
    "\n",
    "    # Classification branch\n",
    "    classification_branch = layers.GlobalAveragePooling2D()(base_model.output)\n",
    "    classification_branch = layers.Dense(256, activation='relu')(classification_branch)\n",
    "    classification_output = layers.Dense(20, activation='softmax', name='class_output')(classification_branch)\n",
    "\n",
    "    # Localization branch\n",
    "    localization_branch = layers.GlobalAveragePooling2D()(base_model.output)\n",
    "    localization_branch = layers.Dense(128, activation='relu')(localization_branch)\n",
    "    localization_output = layers.Dense(4, name='loc_output')(localization_branch)  # 4 values for x, y, width, height\n",
    "\n",
    "    model = Model(inputs=input_layer, outputs=[classification_output, localization_output])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the combined loss function\n",
    "def combined_loss(w_closs, w_lloss):\n",
    "    def loss(y_true, y_pred):\n",
    "        # Access class and annotation tensors directly\n",
    "        y_class_true = y_true[0]\n",
    "        y_annotation_true = y_true[1]\n",
    "        y_class_pred = y_pred[0]\n",
    "        y_annotation_pred = y_pred[1]\n",
    "\n",
    "        # Classification loss\n",
    "        class_loss = tf.keras.losses.CategoricalCrossentropy()(y_class_true, y_class_pred)\n",
    "\n",
    "        # Localization loss\n",
    "        loc_loss = tf.keras.losses.MeanSquaredError()(y_annotation_true, y_annotation_pred)\n",
    "\n",
    "        # Combine both losses\n",
    "        total_loss = (w_closs * class_loss) + (w_lloss * loc_loss)\n",
    "        return total_loss\n",
    "\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "input_shape = (224, 224, 3)\n",
    "num_classes = 20\n",
    "w_closs=.5\n",
    "w_lloss=.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model\n",
    "model = create_classification_localization_model(input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    loss=combined_loss(w_closs=w_closs,w_lloss=w_lloss),\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=.001),\n",
    "    metrics=[\"accuracy\"],\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "29/29 - 396s - loss: 23.5030 - class_output_loss: 2.4033 - loc_output_loss: 21.0998 - class_output_accuracy: 0.1370 - loc_output_accuracy: 0.2000 - 396s/epoch - 14s/step\n",
      "Epoch 2/30\n",
      "29/29 - 345s - loss: 11.1334 - class_output_loss: 2.1523 - loc_output_loss: 8.9810 - class_output_accuracy: 0.1459 - loc_output_accuracy: 0.4840 - 345s/epoch - 12s/step\n",
      "Epoch 3/30\n",
      "29/29 - 335s - loss: 6.8068 - class_output_loss: 1.8455 - loc_output_loss: 4.9613 - class_output_accuracy: 0.1569 - loc_output_accuracy: 0.5238 - 335s/epoch - 12s/step\n",
      "Epoch 4/30\n",
      "29/29 - 334s - loss: 3.1176 - class_output_loss: 1.7436 - loc_output_loss: 1.3740 - class_output_accuracy: 0.1127 - loc_output_accuracy: 0.5249 - 334s/epoch - 12s/step\n",
      "Epoch 5/30\n",
      "29/29 - 334s - loss: 4.2058 - class_output_loss: 2.0108 - loc_output_loss: 2.1951 - class_output_accuracy: 0.1293 - loc_output_accuracy: 0.5669 - 334s/epoch - 12s/step\n",
      "Epoch 6/30\n",
      "29/29 - 335s - loss: 3.8247 - class_output_loss: 1.9173 - loc_output_loss: 1.9074 - class_output_accuracy: 0.1425 - loc_output_accuracy: 0.5613 - 335s/epoch - 12s/step\n",
      "Epoch 7/30\n",
      "29/29 - 334s - loss: 4.3173 - class_output_loss: 1.6839 - loc_output_loss: 2.6334 - class_output_accuracy: 0.1436 - loc_output_accuracy: 0.4718 - 334s/epoch - 12s/step\n",
      "Epoch 8/30\n",
      "29/29 - 334s - loss: 4.0766 - class_output_loss: 1.9172 - loc_output_loss: 2.1594 - class_output_accuracy: 0.1348 - loc_output_accuracy: 0.5138 - 334s/epoch - 12s/step\n",
      "Epoch 9/30\n",
      "29/29 - 333s - loss: 3.8085 - class_output_loss: 1.4513 - loc_output_loss: 2.3573 - class_output_accuracy: 0.1481 - loc_output_accuracy: 0.5536 - 333s/epoch - 11s/step\n",
      "Epoch 10/30\n",
      "29/29 - 334s - loss: 5.2490 - class_output_loss: 2.2192 - loc_output_loss: 3.0299 - class_output_accuracy: 0.1403 - loc_output_accuracy: 0.5403 - 334s/epoch - 12s/step\n",
      "Epoch 11/30\n",
      "29/29 - 334s - loss: 3.2253 - class_output_loss: 1.4351 - loc_output_loss: 1.7903 - class_output_accuracy: 0.1459 - loc_output_accuracy: 0.5271 - 334s/epoch - 12s/step\n",
      "Epoch 12/30\n",
      "29/29 - 333s - loss: 3.5594 - class_output_loss: 1.5687 - loc_output_loss: 1.9906 - class_output_accuracy: 0.1547 - loc_output_accuracy: 0.5602 - 333s/epoch - 11s/step\n",
      "Epoch 13/30\n",
      "29/29 - 334s - loss: 3.4366 - class_output_loss: 1.8484 - loc_output_loss: 1.5882 - class_output_accuracy: 0.1470 - loc_output_accuracy: 0.5370 - 334s/epoch - 12s/step\n",
      "Epoch 14/30\n",
      "29/29 - 334s - loss: 3.6372 - class_output_loss: 1.6959 - loc_output_loss: 1.9414 - class_output_accuracy: 0.1492 - loc_output_accuracy: 0.5138 - 334s/epoch - 12s/step\n",
      "Epoch 15/30\n",
      "29/29 - 333s - loss: 3.3101 - class_output_loss: 1.4063 - loc_output_loss: 1.9039 - class_output_accuracy: 0.1492 - loc_output_accuracy: 0.5492 - 333s/epoch - 11s/step\n",
      "Epoch 16/30\n",
      "29/29 - 334s - loss: 2.8382 - class_output_loss: 1.4660 - loc_output_loss: 1.3721 - class_output_accuracy: 0.1558 - loc_output_accuracy: 0.5017 - 334s/epoch - 12s/step\n",
      "Epoch 17/30\n",
      "29/29 - 334s - loss: 3.1875 - class_output_loss: 1.3096 - loc_output_loss: 1.8780 - class_output_accuracy: 0.1481 - loc_output_accuracy: 0.5171 - 334s/epoch - 12s/step\n",
      "Epoch 18/30\n",
      "29/29 - 333s - loss: 3.7451 - class_output_loss: 2.1876 - loc_output_loss: 1.5575 - class_output_accuracy: 0.1414 - loc_output_accuracy: 0.5392 - 333s/epoch - 11s/step\n",
      "Epoch 19/30\n",
      "29/29 - 334s - loss: 2.7535 - class_output_loss: 1.3513 - loc_output_loss: 1.4022 - class_output_accuracy: 0.1370 - loc_output_accuracy: 0.5337 - 334s/epoch - 12s/step\n",
      "Epoch 20/30\n",
      "29/29 - 334s - loss: 4.5104 - class_output_loss: 2.1471 - loc_output_loss: 2.3632 - class_output_accuracy: 0.1392 - loc_output_accuracy: 0.5536 - 334s/epoch - 12s/step\n",
      "Epoch 21/30\n",
      "29/29 - 347s - loss: 3.4004 - class_output_loss: 1.5868 - loc_output_loss: 1.8136 - class_output_accuracy: 0.1359 - loc_output_accuracy: 0.5713 - 347s/epoch - 12s/step\n",
      "Epoch 22/30\n",
      "29/29 - 345s - loss: 3.2872 - class_output_loss: 1.6109 - loc_output_loss: 1.6763 - class_output_accuracy: 0.1359 - loc_output_accuracy: 0.5669 - 345s/epoch - 12s/step\n",
      "Epoch 23/30\n",
      "29/29 - 340s - loss: 3.0297 - class_output_loss: 1.5127 - loc_output_loss: 1.5170 - class_output_accuracy: 0.1359 - loc_output_accuracy: 0.5702 - 340s/epoch - 12s/step\n",
      "Epoch 24/30\n",
      "29/29 - 361s - loss: 3.1263 - class_output_loss: 1.6226 - loc_output_loss: 1.5037 - class_output_accuracy: 0.1425 - loc_output_accuracy: 0.5602 - 361s/epoch - 12s/step\n",
      "Epoch 25/30\n",
      "29/29 - 348s - loss: 4.1713 - class_output_loss: 1.7062 - loc_output_loss: 2.4651 - class_output_accuracy: 0.1370 - loc_output_accuracy: 0.5536 - 348s/epoch - 12s/step\n",
      "Epoch 26/30\n",
      "29/29 - 332s - loss: 3.3036 - class_output_loss: 1.4773 - loc_output_loss: 1.8263 - class_output_accuracy: 0.1514 - loc_output_accuracy: 0.5503 - 332s/epoch - 11s/step\n",
      "Epoch 27/30\n",
      "29/29 - 332s - loss: 3.1287 - class_output_loss: 1.3991 - loc_output_loss: 1.7296 - class_output_accuracy: 0.1436 - loc_output_accuracy: 0.5580 - 332s/epoch - 11s/step\n",
      "Epoch 28/30\n",
      "29/29 - 329s - loss: 3.5952 - class_output_loss: 1.7677 - loc_output_loss: 1.8275 - class_output_accuracy: 0.1536 - loc_output_accuracy: 0.5602 - 329s/epoch - 11s/step\n",
      "Epoch 29/30\n",
      "29/29 - 327s - loss: 2.7601 - class_output_loss: 1.5053 - loc_output_loss: 1.2547 - class_output_accuracy: 0.1514 - loc_output_accuracy: 0.5436 - 327s/epoch - 11s/step\n",
      "Epoch 30/30\n",
      "29/29 - 327s - loss: 3.0351 - class_output_loss: 1.3777 - loc_output_loss: 1.6574 - class_output_accuracy: 0.1547 - loc_output_accuracy: 0.5414 - 327s/epoch - 11s/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x12dc5fc50d0>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model fit\n",
    "model.fit(x_train,[y_train_class,y_train_annotations],batch_size=32, epochs=30, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30/30 - 74s - loss: 2.9551 - class_output_loss: 1.5868 - loc_output_loss: 1.3683 - class_output_accuracy: 0.1125 - loc_output_accuracy: 0.5635 - 74s/epoch - 2s/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2.955080270767212,\n",
       " 1.5867671966552734,\n",
       " 1.368312954902649,\n",
       " 0.11249999701976776,\n",
       " 0.5635416507720947]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model evaluate\n",
    "model.evaluate(x_test,[y_test_class,y_test_annotations],batch_size=32, verbose=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
