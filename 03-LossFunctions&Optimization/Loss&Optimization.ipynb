{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Classification\n",
    "In the last section we introduced image classification using K-NN algorithm. K-NN has disadvantages.\n",
    " - The classifier must remember all of the training data and store it for future comparisons with the test data. This is space inefficient because datasets may easily be gigabytes in size.\n",
    " - Classifying a test image is expensive since it requires a comparison to all training images.\n",
    "\n",
    "\n",
    " **Overview**:\n",
    "  We are now going to develop a more powerful approach to image classification that we will eventually naturally extend to entire Neural Networks and Convolutional Neural Networks. The approach will have two major components: a score function that maps the raw data to class scores, and a loss function that quantifies the agreement between the predicted scores and the ground truth labels. We will then cast this as an optimization problem in which we will minimize the loss function with respect to the parameters of the score function.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameterized mapping from images to label scores\n",
    "- **score** function\n",
    "- **loss** function\n",
    "\n",
    "let’s assume a training dataset of images $x_i \\in R^D$\n",
    ", each associated with a label yi\n",
    ". Here $ i=1…N $\n",
    " and $ yi∈1…K $\n",
    ". That is, we have N examples (each with a dimensionality D) and K distinct categories. For example, in CIFAR-10 we have a training set of N = 50,000 images, each with D = 32 x 32 x 3 = 3072 pixels, and K = 10, since there are 10 distinct classes (dog, cat, car, etc). We will now define the score function $ f:R^D\\mapsto R^K$\n",
    " that maps the raw image pixels to class scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Classifier\n",
    " $ f(x_i, W, b) =  W x_i + b$\n",
    "\n",
    "\n",
    " In the above equation, we are assuming that the image xi\n",
    " has all of its pixels flattened out to a single column vector of shape [D x 1]. The matrix W (of size [K x D]), and the vector b (of size [K x 1]) are the parameters of the function. In CIFAR-10, xi\n",
    " contains all pixels in the i-th image flattened into a single [3072 x 1] column, W is [10 x 3072] and b is [10 x 1], so 3072 numbers come into the function (the raw pixel values) and 10 numbers come out (the class scores). The parameters in W are often called the weights, and b is called the bias vector because it influences the output scores, but without interacting with the actual data xi\n",
    ". However, you will often hear people use the terms weights and parameters interchangeably."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpreting a linear classifier as template matching\n",
    "- each row of weight vector is responsible for a specified class. Therefore each row can be said to be a template.\n",
    "\n",
    "- To label an image we need to comparee the image with all the template and see which template matches with the image.\n",
    "\n",
    "**Problem**\n",
    "- ex1: suppose a dataset has different color of image and having maximum colors with red. Then the template of car class would mostly recognize red car but not the cars of other color.\n",
    "\n",
    "**solution**\n",
    "- introducing neural network.a neural network will be able to develop intermediate neurons in its hidden layers that could detect specific car types (e.g. green car facing left, blue car facing front, etc.), and neurons on the next layer could combine these into a more accurate car score through a weighted sum of the individual car detectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bias trick\n",
    " it is cumbersome to keep track of two sets of parameters. Therefore we combine these two into one. For this-\n",
    "\n",
    " -by extending the vector xi\n",
    " with one additional dimension that always holds the constant 1 and a default bias dimension.\n",
    "\n",
    " Now the score function looks like:  $f(x_i,W)=W x_i$\n",
    "\n",
    " **New Dimensions**\n",
    " $x_i$ =[3073 X 1] , $W$=[10 X 3073]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image data processing\n",
    "Its always recommended to normalize data before training.\n",
    "\n",
    "**Concept**: Centering the dataset by subtracting the mean from every feature. \n",
    "- range of the values of dataset:\n",
    "\n",
    "        - [-1,1] :zero mean centering\n",
    "        - [-127,127] we will do in each image processing\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss and Optimization\n",
    "## Loss\n",
    "In the previous lecture we learned to work with data having a set of hyperpaarameters. For linear classifier weights are hyperparameters. For different set of hyperparameters we get different predictions. The questions is , which hyperparameters should we use in our model. In other words, which parameter set is best to use in our model. To get intuition about this question \"loss\" comes to the picture. In machine learning we want our model to predict as much as perfect. Say we want our a model to classify an image as cat. We want our model that every time it encounters an image it correctly predict as cat or not a cat. When model fails to predict correctly we consider it as loss. Therefore we can define loss as the difference between the predicted value and the actual value for a single example. Now the answer is, we will select that hyperparameter set that results the least loss.\n",
    "\n",
    "##### Types of loss function\n",
    "There are different types of loss function. Depending on the problem set and algorithm we use. In image classification for multiclass problem we can use two types of loss function.\n",
    "  - Multiclass Support Vector Machine Loss\n",
    "  - Cross Entropy Loss\n",
    "**Multiclass SVM loss(hinge loss)**:\n",
    "    - **Goal**:  The SVM loss is set up so that the SVM “wants” the correct class for each image to a have a score higher than the incorrect classes by some fixed margin Δ\n",
    "\n",
    "    $L_i = \\sum_{j\\neq y_i} \\max(0, s_j - s_{y_i} + \\Delta)$\n",
    "    \n",
    "  Since we are working with linear score function we can write it as: $L_i = \\sum_{j\\neq y_i} \\max(0, w_j^T x_i - w_{y_i}^T x_i + \\Delta)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation of loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_i_unvectorized(x, y, W):\n",
    "  \"\"\"\n",
    "  unvectorized version. Compute the multiclass svm loss for a single example (x,y)\n",
    "  - x is a column vector representing an image (e.g. 3073 x 1 in CIFAR-10)\n",
    "    with an appended bias dimension in the 3073-rd position (i.e. bias trick)\n",
    "  - y is an integer giving index of correct class (e.g. between 0 and 9 in CIFAR-10)\n",
    "  - W is the weight matrix (e.g. 10 x 3073 in CIFAR-10)\n",
    "  \"\"\"\n",
    "  delta = 1.0 # see notes about delta later in this section\n",
    "  scores = W.dot(x) # scores becomes of size 10 x 1, the scores for each class\n",
    "  correct_class_score = scores[y]\n",
    "  D = W.shape[0] # number of classes, e.g. 10\n",
    "  loss_i = 0.0\n",
    "  for j in range(D): # iterate over all wrong classes\n",
    "    if j == y: #here y is an integer\n",
    "      # skip for the true class to only loop over incorrect classes\n",
    "      continue\n",
    "    # accumulate loss for the i-th example\n",
    "    loss_i += max(0, scores[j] - correct_class_score + delta)\n",
    "  return loss_i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation**: takes input only a single imae as a (3073 X 1) vector ,a single integer value y{y=0 to 9 : number of classes} and a weight matrix of shape(num of levels, num of features). calculates inner product wx and returns a vector of size (num of lavels,1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def L_i_half_vectorized(x, y, W):\n",
    "  \"\"\"\n",
    "  A faster half-vectorized implementation. half-vectorized\n",
    "  refers to the fact that for a single example the implementation contains\n",
    "  no for loops, but there is still one loop over the examples (outside this function)\n",
    "  \"\"\"\n",
    "  delta = 1.0\n",
    "  scores = W.dot(x)\n",
    "  # compute the margins for all classes in one vector operation\n",
    "  margins = np.maximum(0, scores - scores[y] + delta)\n",
    "  # on y-th position scores[y] - scores[y] canceled and gave delta. We want\n",
    "  # to ignore the y-th position and only consider margin on max wrong class\n",
    "  margins[y] = 0\n",
    "  loss_i = np.sum(margins)\n",
    "  return loss_i\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note** vecotr- single scalar value =vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "after minus the new val is  [[0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]]\n"
     ]
    }
   ],
   "source": [
    "v=np.ones((4,1))\n",
    "print(v)\n",
    "print(\"after minus the new val is \",v-.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "## Fully vectorized implementation\n",
    "def L(X,y,W):\n",
    "     \n",
    "     \"\"\"\n",
    "    Fully-vectorized implementation of the loss function.\n",
    "    - X: Holds all the training examples as columns (e.g., 3073 x 50,000 in CIFAR-10)\n",
    "    - y: Array of integers specifying the correct class (e.g., 50,000-D array)\n",
    "    - W: Weights (e.g., 10 x 3073)\n",
    "    \"\"\"\n",
    "     #compute scores for all classes\n",
    "     scores=np.dot(W,X)\n",
    "\n",
    "     #select only the scores for the correct class for each example\n",
    "     correct_class_scores=scores[y,np.arange(X.shape[1])] # y is an array indicating row indices and np.arrange(X.shape[1] is indicationg col indices 1-49999). correct classs score dim(50000,1)\n",
    "     #compute the hinge loss for all classes\n",
    "     margins=np.maximum(0,scores-correct_class_scores+1)\n",
    "     margins[y,np.arange(X.shape[1])]=0  #set loss for correct class 0\n",
    "     #compute the overall loss as the average of all hinge losses\n",
    "     loss=np.mean(margins)\n",
    "     return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**dimensions**:\n",
    "- X- (3073 X 50000) represents the dataset. each colum represents an example and each row represents number of features. this dim is for CIFAR-10 dataset \n",
    "- y -(50000,1) is a 1-D array holding the values of index of the correct class for each training example\n",
    "- W-(num of class levels=10,num of features=3073) #actually 3072, we have added intercept term using bias trick\n",
    "-scores-(num of class levesl=10,number of training examples=500000)\n",
    "- correct_class_scores - (50000,) /1-D array having 50000 values: Unlike y it doesn't contain the index label rather it contains the score values of the correct classes.\n",
    "- margins- (shape of scores(10,50000)).This can be understood from below numpy operational code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]]\n",
      "[[ 0. -1. -2.]\n",
      " [ 0. -1. -2.]\n",
      " [ 0. -1. -2.]\n",
      " [ 0. -1. -2.]\n",
      " [ 0. -1. -2.]]\n"
     ]
    }
   ],
   "source": [
    "# if num of columns of two numpy array equal then they can be subtracted elementwise\n",
    "a=np.ones((5,3))\n",
    "b=[1,2,3]\n",
    "print(a)\n",
    "print(a-b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q&A(Multiclass SVM loss function)\n",
    "\n",
    "**Q1**: Suppose we get L_i=0 for a specified class. What happens if we change a littele bit score corresponding to this class?\n",
    "\n",
    " - **Answer**: The loss will still be same(0 in this case). Because its score value is already greter than the other scores of other classes.\n",
    "\n",
    "**Q2**:Whata is the min max possibe loss?\n",
    "\n",
    " - **Answer**: Min=0 and max = infinity. \n",
    "      - Min: Because if the differences between the scores is negative then the function returns max value zero.\n",
    "      - Max: Because from the equation we can say that if the correct scores gets very very negative score. Then we accumulate infinity loss.\n",
    "\n",
    "**Q3**: At initialization W is small so all s ≈ 0.What is the loss? \n",
    " - **Answer**: Number of classes minus one (C-1). The predicted score and the original class score will be same .therfore we will get error 1 for the delta term. And remember we don't calculate for when  $j=y_i$. Thats why minus one.\n",
    "\n",
    "**Q4**:What if the sum was over all classes?(including $j = y_i$)?\n",
    " - **Answer**: Loss increases by 1. Because for this correct class loss=0 but we get the delta value 1.\n",
    "\n",
    "**Q5**:What if we used mean instead ofsum? ?\n",
    " - **Answer**: Doesn't change. because the number of classes remains constant all the time. It only rescale the loss.\n",
    "\n",
    "**Q6**:What if we use suared of the loss function ?\n",
    " - **Answer**: would be a different loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization\n",
    "\n",
    "Why do we use regularization?\n",
    "- To avoid overfitting \n",
    "- To generalize\n",
    "- model should be simple.So that works better on test data\n",
    "\n",
    "**Concept**:\n",
    "When the number of features is high then the training data fits the model perfectly. But doesn't show a good result in test case. This phenomenon is known as overfitting(High Variance). We know that the trainig examples ($x_i,y_i$) is fixed. We can not change them. Thus some features may have values much more higher than other features. So these exteme features can have great impact on the score function. To overcome this situation we can only penalize thsese values only by penalizing the wieghts. Because by penalizing weights we can control features. There are many regularized loss term availabe. But we will use squared L2 regularized term. $R(W) = \\sum_k\\sum_l W_{k,l}^2$. Byadding this term to loss function we have now two parts in loss function. One is **data loss** and the **regularization loss**\n",
    "\n",
    "$L =  \\underbrace{ \\frac{1}{N} \\sum_i L_i }_\\text{data loss} + \\underbrace{ \\lambda R(W) }_\\text{regularization loss} \\\\\\\\$\n",
    "\n",
    "Expanding this out in its full form:\n",
    "$L = \\frac{1}{N} \\sum_i \\sum_{j\\neq y_i} \\left[ \\max(0, f(x_i; W)_j - f(x_i; W)_{y_i} + \\Delta) \\right] + \\lambda \\sum_k\\sum_l W_{k,l}^2$\n",
    "\n",
    "**Example** Suppose we have $x=[1,1,1,1]$ ,$w1=[1,0,0,0]$, $w2=[0.25,0.25,0.25,0.25]$. \n",
    "Here,$w_1^Tx = w_2^Tx = 1$ But according to L2 regularization w2 is preferable. Because the sum of the suqred values of w2(.5)< w1(1). This demonstrates that regularized loss for w2 is less than w1.\n",
    "\n",
    "**Lambda**: Hyperparameters which plays role for trade off between data loss and regualrization loss.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implemetation of loss with regularized term\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "## Fully vectorized implementation\n",
    "#so far this is the final svm loss and we will use it in future when to need\n",
    "def svm_loss(X,y,W,Lambda):\n",
    "     \n",
    "     \"\"\"\n",
    "    Fully-vectorized implementation of the loss function.\n",
    "    - X: Holds all the training examples as columns (e.g., 3073 x 50,000 in CIFAR-10)\n",
    "    - y: Array of integers specifying the correct class (e.g., 50,000-D array)\n",
    "    - W: Weights (e.g., 10 x 3073)\n",
    "    \"\"\"\n",
    "     #compute scores for all classes\n",
    "     scores=np.dot(W,X)\n",
    "     #select only the scores for the correct class for each example\n",
    "     correct_class_scores=scores[y,np.arange(X.shape[1])] # y is an array indicating row indices and np.arrange(X.shape[1] is indicationg col indices 1-49999). correct classs score dim(50000,1)\n",
    "     #compute the hinge loss for all classes\n",
    "     margins=np.maximum(0,scores-correct_class_scores+1)\n",
    "     margins[y,np.arange(X.shape[1])]=0  #set loss for correct class 0\n",
    "     #compute the overall loss as the average of all hinge losses\n",
    "     loss=np.mean(margins)\n",
    "\n",
    "     # calclute the regularized term :squared sum of all the weights\n",
    "     reg_term=Lambda*(np.sum(W * W))  #we can also multiply .5 for convenience\n",
    "     reg_loss=loss+reg_term\n",
    "     return reg_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Setting delta**\n",
    "we can set delta always 1. The hyperparameters Δ\n",
    " and λ\n",
    " seem like two different hyperparameters, but in fact they both control the same tradeoff: The tradeoff between the data loss and the regularization loss in the objective. The key to understanding this is that the magnitude of the weights W\n",
    " has direct effect on the scores (and hence also their differences): As we shrink all values inside W\n",
    " the score differences will become lower, and as we scale up the weights the score differences will all become higher. Therefore, the exact value of the margin between the scores (e.g. Δ=1\n",
    ", or Δ=100\n",
    ") is in some sense meaningless because the weights can shrink or stretch the differences arbitrarily. Hence, the only real tradeoff is how large we allow the weights to grow (through the regularization strength λ\n",
    ")."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**other Multiclass SVM formulations:**\n",
    "- OneVsAll\n",
    "  -  trains an independent binary SVM for each class vs. all other classes.\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax Classifier (Multinomial Logistic Rregression)\n",
    "the Softmax classifier is generalization of binary logistic regression classifier to multiple classes.\n",
    "scores=unnormalized log probabilities of the classse  ; $s=f(x_i,W)$\n",
    "**Steps**\n",
    "  - we take the score from $x=f(x_i,W)$\n",
    "  - we exponentiate them so that they become positive\n",
    "  - we normalize them by sum of the exponents\n",
    "\n",
    "The last two steps is done by a function named softmax function. This function ends up to a vector of probabilities corresponding to each classes. The sum of all the elements is 1.\n",
    "\n",
    "$P(Y=k | X=x_i)= \\frac{e^{s_k}}{\\sum_k e^{s_j}}$ ; outputs the pobability of class k (k=1 to K)\n",
    "\n",
    "Here, $f_j(z) = \\frac{e^{z_j}}{\\sum_k e^{z_k}}$ is the softmax function\n",
    "\n",
    "\n",
    "##### Softmax loss Function(Cross Entropy Loss):\n",
    "  **Target** : Want to maximize the log likelihood, or (for a loss function) to minimize the negative log likelihood of the correct class. In other words the loss fucntion tries to make the correct class probability near to 1 and the other class probability near to 0. Thus the loss functin is Negative log of the probability of the correct class.\n",
    "  $L_i = -\\log\\left(\\frac{e^{f_{y_i}}}{ \\sum_j e^{f_j} }\\right) \\hspace{0.5in} \\text{or equivalently} \\hspace{0.5in} L_i = -f_{y_i} + \\log\\sum_j e^{f_j}$\n",
    "  \n",
    "  The equation says, minimize the negative log likelihood of the correct class which can be interpreted as performing Maximum Likelihood Estimation (MLE).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Practical Issues:\n",
    "The intermediate terms $e^{f_{y_i}}$ and $ \\sum_j e^{f_j}$  may be very large due to the exponentials. Dividing large numbers can be numerically unstable. for stability we multiply neumeratkor and denominator by a constant C and we get,\n",
    "$\\frac{e^{f_{y_i}}}{\\sum_j e^{f_j}}\n",
    "= \\frac{Ce^{f_{y_i}}}{C\\sum_j e^{f_j}}\n",
    "= \\frac{e^{f_{y_i} + \\log C}}{\\sum_j e^{f_j + \\log C}}$\n",
    "\n",
    "**Value of C**: we can take any value. A common choice is to set $logC= -max_j f_j$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other forms of cross-entropy loss\n",
    "- **binary Classificaion**: $L(y_{\\text{pred}}, y_{\\text{true}}) = - (y_{\\text{true}} \\cdot \\log(y_{\\text{pred}}) + (1 - y_{\\text{true}}) \\cdot \\log(1 - y_{\\text{pred}}))\n",
    "$\n",
    "\n",
    "- **Multiclass classification**: $L(y_{\\text{pred}}, y_{\\text{true}}) = - \\sum_{i} y_{\\text{true}} \\cdot \\log(y_{\\text{pred}})\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(X):\n",
    "    \"\"\"takes a input matrix and applies exponent to each element and then find the probility of each \n",
    "    element corresponding a class\n",
    "    \"\"\"\n",
    "    #here X is score fuction\n",
    "    X=np.exp(X)\n",
    "    den=np.sum(X,axis=0) # dim ()\n",
    "    prob_scores=X/den  # now X is a matrix of shape(10,50000) which contains probability of the each class for each value\n",
    "    return prob_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will implement softmax function considering practical issues with the updated equation .\n",
    "Updated equation: $\\frac{e^{f_{y_i}}}{\\sum_j e^{f_j}}\n",
    "= \\frac{Ce^{f_{y_i}}}{C\\sum_j e^{f_j}}\n",
    "= \\frac{e^{f_{y_i} + \\log C}}{\\sum_j e^{f_j + \\log C}}$\n",
    "\n",
    "**Goal**: make every value of each class (maximum 0)for a training example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stable softmax function\n",
    "def s_softmax(X):\n",
    "    X-=np.max(X,axis=0)\n",
    "    X=np.exp(X)\n",
    "    den=np.sum(X,axis=0)\n",
    "    prob_scores=X/den\n",
    "    return prob_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#without numeric stability and regularized terms\n",
    "#implementing cross entropy loss for multinomial logistic regression(softmax calssifier\n",
    "def cross_entropy_loss(X,y,W):\n",
    "    raw_scores=np.dot(W,X) #score funcion\n",
    "    #get each class probability value by applying softmax function\n",
    "    prob_scores=s_softmax(raw_scores)\n",
    "    # Now we will write code for first two forms described above\n",
    "    ce_loss_1=np.mean(-np.log(prob_scores[y,np.arange(prob_scores.shape[1])])) #first form of loss function\n",
    "    #ce_loss_2=np.mean(-np.sum(raw_scores[y,np.arange(raw_scores.shape[0])]) + np.sum(np.log(np.sum(prob_scores,axis=0))))\n",
    "    return ce_loss_1 #only one is enough .but computed  to see which perfoms better"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "here we used unstable softmax function. we can also use stable one and check which performs better in our model. will apply in future...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naming convention:\n",
    " Multiclass SVM loss/ hinge loss is sometimes called max-margin loss. Technically softmax function is a squashing function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM vs Softmax:\n",
    "**SVM**\n",
    " - through score function wants that the correct class score value is higher than the other class scores by a margin.\n",
    " -The SVM does not care about the details of the individual scores\n",
    "**Softmax**\n",
    " - The Softmax classifier instead interprets the scores as (unnormalized) log probabilities for each class and then encourages the (normalized) log probability of the correct class to be high (equivalently the negative of it to be low)\n",
    " - the Softmax classifier allows us to compute “probabilities” for all labels. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization\n",
    "\n",
    "### Introduction:\n",
    "\n",
    "**linear score function** : $f(x_i, W) =  W x_i$\n",
    "\n",
    "**SVM loss**: $L=1N∑i∑j≠yi[max(0,f(xi;W)j−f(xi;W)yi+1)]+αR(W)$\n",
    "\n",
    "We saw that a setting of the parameters Wthat produced predictions for examples xiconsistent with their ground truth labels yiwould also have a very low loss L. We are now going to introduce the third and last key component: optimization. Optimization is the process of finding the set of parameters W that minimize the loss function.\n",
    "\n",
    "**Goal**: understanding the interaction among **score** function, **loss** fucntion and **optimization** these three components."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualising the loss function\n",
    "\n",
    "$L_i = \\sum_{j\\neq y_i} \\left[ \\max(0, w_j^Tx_i - w_{y_i}^Tx_i + 1) \\right]$\n",
    "\n",
    "It is clear from the equation that the data loss for each example is a sum of (zero-thresholded due to the max(0,−) function) linear functions of W\n",
    "\n",
    "**Sign in front of W**:\n",
    "- +ve sign: it means that the value of weight is positively corelated with the likelihood of the corresponding class. In other words increasing the value of w will increase the chances of being classified as the correct class.\n",
    "- -ve sign: weights are negatively correlated.\n",
    "\n",
    "The above equation involves data loss and thresholding, the sign of the weight determines whether the contribution of that weight is considered in the calculation of the loss. Positive weights associated with incorrect class labels (where the model predicted the wrong class) contribute positively to the data loss. On the other hand, negative weights associated with correct class labels contribute negatively to the data loss, reducing it.\n",
    "\n",
    "**Shape of loss function**:\n",
    "\n",
    "SVM loss is convex function. Once we extend our score functions f to Neural Networks our objective functions will become non-convex, and the visualizations of those non-convex function will not feature bowls but complex, bumpy terrains.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization\n",
    "\n",
    "**Goal** : The goal of optimization is to find W that minimizes the loss function. We will now motivate and slowly develop an approach to optimizing the loss function. There are different optimization function based on the type of loss function. For example, for convex function we can use gradient optimization. Our final goal is to optimize neural networks where we can't easily find any of the tools developed in the convex optimization literature.\n",
    "\n",
    "First we will talk for convex function. In this case for hinge loss.\n",
    "Approaches we will try-\n",
    " - Random Search\n",
    " - Random Local Search\n",
    " - Following the gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Strategy #1: A first very bad idea solution: Random search\n",
    "Since it is so simple to check how good a given set of parameters W is, the first (very bad) idea that may come to mind is to simply try out many different random weights and keep track of what works best. This procedure might look as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we need to import our data and split it into train test\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpickle(file):\n",
    "    \"\"\"load the cifar-10 data\"\"\"\n",
    "\n",
    "    with open(file, 'rb') as fo:\n",
    "        data = pickle.load(fo, encoding='bytes')\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_cifar_10_data(data_dir, negatives=False):\n",
    "    \"\"\"\n",
    "    Return train_data, train_filenames, train_labels, test_data, test_filenames, test_labels\n",
    "    \"\"\"\n",
    "\n",
    "    # get the meta_data_dict\n",
    "    # num_cases_per_batch: 1000\n",
    "    # label_names: ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "    # num_vis: :3072\n",
    "\n",
    "    meta_data_dict = unpickle(data_dir + \"/batches.meta\")\n",
    "    cifar_label_names = meta_data_dict[b'label_names']\n",
    "    cifar_label_names = np.array(cifar_label_names)\n",
    "\n",
    "    # training data\n",
    "    cifar_train_data = None\n",
    "    cifar_train_filenames = []\n",
    "    cifar_train_labels = []\n",
    "\n",
    "    # cifar_train_data_dict\n",
    "    # 'batch_label': 'training batch 5 of 5'\n",
    "    # 'data': ndarray\n",
    "    # 'filenames': list\n",
    "    # 'labels': list\n",
    "\n",
    "    for i in range(1, 6):\n",
    "        cifar_train_data_dict = unpickle(data_dir + \"/data_batch_{}\".format(i))\n",
    "        if i == 1:\n",
    "            cifar_train_data = cifar_train_data_dict[b'data']\n",
    "        else:\n",
    "            cifar_train_data = np.vstack((cifar_train_data, cifar_train_data_dict[b'data']))\n",
    "        cifar_train_filenames += cifar_train_data_dict[b'filenames']\n",
    "        cifar_train_labels += cifar_train_data_dict[b'labels']\n",
    "\n",
    "    cifar_train_data = cifar_train_data.reshape((len(cifar_train_data), 3, 32, 32))\n",
    "    if negatives:\n",
    "        cifar_train_data = cifar_train_data.transpose(0, 2, 3, 1).astype(np.float32)\n",
    "    else:\n",
    "        cifar_train_data = np.rollaxis(cifar_train_data, 1, 4)\n",
    "    cifar_train_filenames = np.array(cifar_train_filenames)\n",
    "    cifar_train_labels = np.array(cifar_train_labels)\n",
    "\n",
    "    # test data\n",
    "    # cifar_test_data_dict\n",
    "    # 'batch_label': 'testing batch 1 of 1'\n",
    "    # 'data': ndarray\n",
    "    # 'filenames': list\n",
    "    # 'labels': list\n",
    "\n",
    "    cifar_test_data_dict = unpickle(data_dir + \"/test_batch\")\n",
    "    cifar_test_data = cifar_test_data_dict[b'data']\n",
    "    cifar_test_filenames = cifar_test_data_dict[b'filenames']\n",
    "    cifar_test_labels = cifar_test_data_dict[b'labels']\n",
    "\n",
    "    cifar_test_data = cifar_test_data.reshape((len(cifar_test_data), 3, 32, 32))\n",
    "    if negatives:\n",
    "        cifar_test_data = cifar_test_data.transpose(0, 2, 3, 1).astype(np.float32)\n",
    "    else:\n",
    "        cifar_test_data = np.rollaxis(cifar_test_data, 1, 4)\n",
    "    cifar_test_filenames = np.array(cifar_test_filenames)\n",
    "    cifar_test_labels = np.array(cifar_test_labels)\n",
    "\n",
    "    return cifar_train_data, cifar_train_filenames, cifar_train_labels, \\\n",
    "        cifar_test_data, cifar_test_filenames, cifar_test_labels, cifar_label_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data:  (50000, 32, 32, 3)\n",
      "Train filenames:  (50000,)\n",
      "Train labels:  (50000,)\n",
      "Test data:  (10000, 32, 32, 3)\n",
      "Test filenames:  (10000,)\n",
      "Test labels:  (10000,)\n",
      "Label names:  (10,)\n"
     ]
    }
   ],
   "source": [
    "cifar_10_dir = 'dataset\\cifar10'\n",
    "train_data, train_filenames, train_labels, test_data, test_filenames, test_labels, label_names = load_cifar_10_data(cifar_10_dir)\n",
    "print(\"Train data: \", train_data.shape)\n",
    "print(\"Train filenames: \", train_filenames.shape)\n",
    "print(\"Train labels: \", train_labels.shape)\n",
    "print(\"Test data: \", test_data.shape)\n",
    "print(\"Test filenames: \", test_filenames.shape)\n",
    "print(\"Test labels: \", test_labels.shape)\n",
    "print(\"Label names: \", label_names.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Flatten out all the images to be one dimensional\n",
    "Xtr_rows = train_data.reshape(train_data.shape[0], 32 * 32 * 3) # Xtr_rows becomes 50000 x 3072\n",
    "Xte_rows = test_data.reshape(test_data.shape[0], 32 * 32 * 3) # Xte_rows becomes 10000 x 3072"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 3072)\n",
      "(10000, 3072)\n"
     ]
    }
   ],
   "source": [
    "print(Xtr_rows.shape)  \n",
    "print(Xte_rows.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding bias term\n",
    "bias_term=np.ones((50000,1))\n",
    "#np.expand_dims(bias_term,axis=0)\n",
    "Xtr_rows=np.hstack((Xtr_rows,bias_term))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NB**: Adding bias term refers to adding a an extra feature to each example and its value is 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 3073)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtr_rows.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Earlier we implemented two types of loss funciton. One is multiclass svm loss and another is cross entropy loss. Now we will claculate loss using both funcion and compare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in attempt 1 the loss is 0.915069, best 0.915069\n",
      "in attempt 2 the loss is 0.911076, best 0.911076\n",
      "in attempt 3 the loss is 0.992709, best 0.911076\n",
      "in attempt 4 the loss is 0.935788, best 0.911076\n",
      "in attempt 5 the loss is 1.045258, best 0.911076\n",
      "in attempt 6 the loss is 0.950936, best 0.911076\n",
      "in attempt 7 the loss is 1.041655, best 0.911076\n",
      "in attempt 8 the loss is 0.986177, best 0.911076\n",
      "in attempt 9 the loss is 1.052637, best 0.911076\n",
      "in attempt 10 the loss is 0.994203, best 0.911076\n",
      "in attempt 11 the loss is 1.066598, best 0.911076\n",
      "in attempt 12 the loss is 1.001684, best 0.911076\n",
      "in attempt 13 the loss is 0.995007, best 0.911076\n",
      "in attempt 14 the loss is 0.925100, best 0.911076\n",
      "in attempt 15 the loss is 1.032068, best 0.911076\n",
      "in attempt 16 the loss is 0.956066, best 0.911076\n",
      "in attempt 17 the loss is 0.952031, best 0.911076\n",
      "in attempt 18 the loss is 0.941887, best 0.911076\n",
      "in attempt 19 the loss is 0.982763, best 0.911076\n",
      "in attempt 20 the loss is 1.021510, best 0.911076\n",
      "in attempt 21 the loss is 1.058591, best 0.911076\n",
      "in attempt 22 the loss is 1.082892, best 0.911076\n",
      "in attempt 23 the loss is 0.962984, best 0.911076\n",
      "in attempt 24 the loss is 0.984090, best 0.911076\n",
      "in attempt 25 the loss is 0.992968, best 0.911076\n",
      "in attempt 26 the loss is 1.043459, best 0.911076\n",
      "in attempt 27 the loss is 0.965116, best 0.911076\n",
      "in attempt 28 the loss is 0.966973, best 0.911076\n",
      "in attempt 29 the loss is 0.985189, best 0.911076\n",
      "in attempt 30 the loss is 1.035269, best 0.911076\n",
      "in attempt 31 the loss is 0.915604, best 0.911076\n",
      "in attempt 32 the loss is 0.947200, best 0.911076\n",
      "in attempt 33 the loss is 0.986756, best 0.911076\n",
      "in attempt 34 the loss is 0.940684, best 0.911076\n",
      "in attempt 35 the loss is 0.994910, best 0.911076\n",
      "in attempt 36 the loss is 1.025620, best 0.911076\n",
      "in attempt 37 the loss is 1.093719, best 0.911076\n",
      "in attempt 38 the loss is 0.963897, best 0.911076\n",
      "in attempt 39 the loss is 1.011061, best 0.911076\n",
      "in attempt 40 the loss is 1.021320, best 0.911076\n",
      "in attempt 41 the loss is 1.100224, best 0.911076\n",
      "in attempt 42 the loss is 0.940754, best 0.911076\n",
      "in attempt 43 the loss is 0.969979, best 0.911076\n",
      "in attempt 44 the loss is 1.002921, best 0.911076\n",
      "in attempt 45 the loss is 0.986059, best 0.911076\n",
      "in attempt 46 the loss is 0.985410, best 0.911076\n",
      "in attempt 47 the loss is 0.967337, best 0.911076\n",
      "in attempt 48 the loss is 0.925596, best 0.911076\n",
      "in attempt 49 the loss is 0.977932, best 0.911076\n",
      "in attempt 50 the loss is 0.939351, best 0.911076\n",
      "in attempt 51 the loss is 0.937953, best 0.911076\n",
      "in attempt 52 the loss is 0.967969, best 0.911076\n",
      "in attempt 53 the loss is 1.053258, best 0.911076\n",
      "in attempt 54 the loss is 1.005124, best 0.911076\n",
      "in attempt 55 the loss is 1.135192, best 0.911076\n",
      "in attempt 56 the loss is 0.951747, best 0.911076\n",
      "in attempt 57 the loss is 0.993269, best 0.911076\n",
      "in attempt 58 the loss is 1.052798, best 0.911076\n",
      "in attempt 59 the loss is 1.031514, best 0.911076\n",
      "in attempt 60 the loss is 0.965763, best 0.911076\n",
      "in attempt 61 the loss is 0.952162, best 0.911076\n",
      "in attempt 62 the loss is 0.999505, best 0.911076\n",
      "in attempt 63 the loss is 1.028256, best 0.911076\n",
      "in attempt 64 the loss is 1.003138, best 0.911076\n",
      "in attempt 65 the loss is 1.035802, best 0.911076\n",
      "in attempt 66 the loss is 0.964465, best 0.911076\n",
      "in attempt 67 the loss is 0.969912, best 0.911076\n",
      "in attempt 68 the loss is 0.904096, best 0.904096\n",
      "in attempt 69 the loss is 0.968243, best 0.904096\n",
      "in attempt 70 the loss is 1.022195, best 0.904096\n",
      "in attempt 71 the loss is 1.125034, best 0.904096\n",
      "in attempt 72 the loss is 1.003022, best 0.904096\n",
      "in attempt 73 the loss is 0.969833, best 0.904096\n",
      "in attempt 74 the loss is 0.927591, best 0.904096\n",
      "in attempt 75 the loss is 1.136154, best 0.904096\n",
      "in attempt 76 the loss is 0.991918, best 0.904096\n",
      "in attempt 77 the loss is 0.942983, best 0.904096\n",
      "in attempt 78 the loss is 0.930289, best 0.904096\n",
      "in attempt 79 the loss is 0.915835, best 0.904096\n",
      "in attempt 80 the loss is 0.931727, best 0.904096\n",
      "in attempt 81 the loss is 0.986649, best 0.904096\n",
      "in attempt 82 the loss is 0.975902, best 0.904096\n",
      "in attempt 83 the loss is 0.915460, best 0.904096\n",
      "in attempt 84 the loss is 1.000375, best 0.904096\n",
      "in attempt 85 the loss is 1.038458, best 0.904096\n",
      "in attempt 86 the loss is 0.917490, best 0.904096\n",
      "in attempt 87 the loss is 0.907274, best 0.904096\n",
      "in attempt 88 the loss is 0.969230, best 0.904096\n",
      "in attempt 89 the loss is 0.971068, best 0.904096\n",
      "in attempt 90 the loss is 0.924548, best 0.904096\n",
      "in attempt 91 the loss is 0.982723, best 0.904096\n",
      "in attempt 92 the loss is 0.961055, best 0.904096\n",
      "in attempt 93 the loss is 0.943956, best 0.904096\n",
      "in attempt 94 the loss is 0.976234, best 0.904096\n",
      "in attempt 95 the loss is 0.935021, best 0.904096\n",
      "in attempt 96 the loss is 1.065045, best 0.904096\n",
      "in attempt 97 the loss is 1.031829, best 0.904096\n",
      "in attempt 98 the loss is 1.025644, best 0.904096\n",
      "in attempt 99 the loss is 0.964278, best 0.904096\n",
      "in attempt 100 the loss is 0.959699, best 0.904096\n",
      "in attempt 101 the loss is 0.989030, best 0.904096\n",
      "in attempt 102 the loss is 0.965426, best 0.904096\n",
      "in attempt 103 the loss is 0.981875, best 0.904096\n",
      "in attempt 104 the loss is 1.064161, best 0.904096\n",
      "in attempt 105 the loss is 0.966962, best 0.904096\n",
      "in attempt 106 the loss is 0.969614, best 0.904096\n",
      "in attempt 107 the loss is 1.106814, best 0.904096\n",
      "in attempt 108 the loss is 0.939069, best 0.904096\n",
      "in attempt 109 the loss is 0.944562, best 0.904096\n",
      "in attempt 110 the loss is 1.025119, best 0.904096\n",
      "in attempt 111 the loss is 0.964972, best 0.904096\n",
      "in attempt 112 the loss is 0.994059, best 0.904096\n",
      "in attempt 113 the loss is 0.971432, best 0.904096\n",
      "in attempt 114 the loss is 1.013321, best 0.904096\n",
      "in attempt 115 the loss is 0.921141, best 0.904096\n",
      "in attempt 116 the loss is 0.951845, best 0.904096\n",
      "in attempt 117 the loss is 0.973922, best 0.904096\n",
      "in attempt 118 the loss is 0.984994, best 0.904096\n",
      "in attempt 119 the loss is 0.940675, best 0.904096\n",
      "in attempt 120 the loss is 0.982377, best 0.904096\n",
      "in attempt 121 the loss is 1.086357, best 0.904096\n",
      "in attempt 122 the loss is 0.950287, best 0.904096\n",
      "in attempt 123 the loss is 1.007476, best 0.904096\n",
      "in attempt 124 the loss is 0.974880, best 0.904096\n",
      "in attempt 125 the loss is 0.943425, best 0.904096\n",
      "in attempt 126 the loss is 0.945474, best 0.904096\n",
      "in attempt 127 the loss is 0.944548, best 0.904096\n",
      "in attempt 128 the loss is 0.993475, best 0.904096\n",
      "in attempt 129 the loss is 1.001796, best 0.904096\n",
      "in attempt 130 the loss is 0.978828, best 0.904096\n",
      "in attempt 131 the loss is 0.983486, best 0.904096\n",
      "in attempt 132 the loss is 1.037423, best 0.904096\n",
      "in attempt 133 the loss is 0.965083, best 0.904096\n",
      "in attempt 134 the loss is 0.989392, best 0.904096\n",
      "in attempt 135 the loss is 1.089753, best 0.904096\n",
      "in attempt 136 the loss is 1.075330, best 0.904096\n",
      "in attempt 137 the loss is 0.972421, best 0.904096\n",
      "in attempt 138 the loss is 0.937274, best 0.904096\n",
      "in attempt 139 the loss is 0.960723, best 0.904096\n",
      "in attempt 140 the loss is 0.956409, best 0.904096\n",
      "in attempt 141 the loss is 0.940632, best 0.904096\n",
      "in attempt 142 the loss is 0.968408, best 0.904096\n",
      "in attempt 143 the loss is 0.996833, best 0.904096\n",
      "in attempt 144 the loss is 0.992032, best 0.904096\n",
      "in attempt 145 the loss is 0.970858, best 0.904096\n",
      "in attempt 146 the loss is 0.959592, best 0.904096\n",
      "in attempt 147 the loss is 0.965353, best 0.904096\n",
      "in attempt 148 the loss is 1.024586, best 0.904096\n",
      "in attempt 149 the loss is 0.971707, best 0.904096\n",
      "in attempt 150 the loss is 0.936865, best 0.904096\n",
      "in attempt 151 the loss is 0.903076, best 0.903076\n",
      "in attempt 152 the loss is 0.933558, best 0.903076\n",
      "in attempt 153 the loss is 0.985143, best 0.903076\n",
      "in attempt 154 the loss is 1.031120, best 0.903076\n",
      "in attempt 155 the loss is 1.028919, best 0.903076\n",
      "in attempt 156 the loss is 0.930227, best 0.903076\n",
      "in attempt 157 the loss is 0.998833, best 0.903076\n",
      "in attempt 158 the loss is 1.000043, best 0.903076\n",
      "in attempt 159 the loss is 0.896394, best 0.896394\n",
      "in attempt 160 the loss is 0.916846, best 0.896394\n",
      "in attempt 161 the loss is 0.999406, best 0.896394\n",
      "in attempt 162 the loss is 0.995249, best 0.896394\n",
      "in attempt 163 the loss is 0.931812, best 0.896394\n",
      "in attempt 164 the loss is 0.996589, best 0.896394\n",
      "in attempt 165 the loss is 0.918723, best 0.896394\n",
      "in attempt 166 the loss is 0.995424, best 0.896394\n",
      "in attempt 167 the loss is 0.963141, best 0.896394\n",
      "in attempt 168 the loss is 0.993556, best 0.896394\n",
      "in attempt 169 the loss is 1.010707, best 0.896394\n",
      "in attempt 170 the loss is 0.930165, best 0.896394\n",
      "in attempt 171 the loss is 0.990869, best 0.896394\n",
      "in attempt 172 the loss is 0.947742, best 0.896394\n",
      "in attempt 173 the loss is 0.985791, best 0.896394\n",
      "in attempt 174 the loss is 0.971866, best 0.896394\n",
      "in attempt 175 the loss is 0.996528, best 0.896394\n",
      "in attempt 176 the loss is 1.036535, best 0.896394\n",
      "in attempt 177 the loss is 0.976898, best 0.896394\n",
      "in attempt 178 the loss is 1.031957, best 0.896394\n",
      "in attempt 179 the loss is 1.016072, best 0.896394\n",
      "in attempt 180 the loss is 1.048371, best 0.896394\n",
      "in attempt 181 the loss is 0.963702, best 0.896394\n",
      "in attempt 182 the loss is 0.912734, best 0.896394\n",
      "in attempt 183 the loss is 0.901710, best 0.896394\n",
      "in attempt 184 the loss is 0.968760, best 0.896394\n",
      "in attempt 185 the loss is 0.943820, best 0.896394\n",
      "in attempt 186 the loss is 0.971570, best 0.896394\n",
      "in attempt 187 the loss is 1.023723, best 0.896394\n",
      "in attempt 188 the loss is 0.920116, best 0.896394\n",
      "in attempt 189 the loss is 0.943209, best 0.896394\n",
      "in attempt 190 the loss is 0.940334, best 0.896394\n",
      "in attempt 191 the loss is 0.987306, best 0.896394\n",
      "in attempt 192 the loss is 0.967183, best 0.896394\n",
      "in attempt 193 the loss is 0.955451, best 0.896394\n",
      "in attempt 194 the loss is 0.957623, best 0.896394\n",
      "in attempt 195 the loss is 0.929573, best 0.896394\n",
      "in attempt 196 the loss is 0.972905, best 0.896394\n",
      "in attempt 197 the loss is 0.906658, best 0.896394\n",
      "in attempt 198 the loss is 0.951682, best 0.896394\n",
      "in attempt 199 the loss is 0.978662, best 0.896394\n",
      "in attempt 200 the loss is 0.958157, best 0.896394\n",
      "in attempt 201 the loss is 0.925167, best 0.896394\n",
      "in attempt 202 the loss is 1.069912, best 0.896394\n",
      "in attempt 203 the loss is 1.024343, best 0.896394\n",
      "in attempt 204 the loss is 0.953629, best 0.896394\n",
      "in attempt 205 the loss is 1.040244, best 0.896394\n",
      "in attempt 206 the loss is 0.927091, best 0.896394\n",
      "in attempt 207 the loss is 1.095221, best 0.896394\n",
      "in attempt 208 the loss is 0.963867, best 0.896394\n",
      "in attempt 209 the loss is 0.935637, best 0.896394\n",
      "in attempt 210 the loss is 0.926323, best 0.896394\n",
      "in attempt 211 the loss is 1.027638, best 0.896394\n",
      "in attempt 212 the loss is 1.072257, best 0.896394\n",
      "in attempt 213 the loss is 0.965239, best 0.896394\n",
      "in attempt 214 the loss is 1.045533, best 0.896394\n",
      "in attempt 215 the loss is 0.926580, best 0.896394\n",
      "in attempt 216 the loss is 0.965542, best 0.896394\n",
      "in attempt 217 the loss is 1.047267, best 0.896394\n",
      "in attempt 218 the loss is 0.954020, best 0.896394\n",
      "in attempt 219 the loss is 0.942483, best 0.896394\n",
      "in attempt 220 the loss is 1.011799, best 0.896394\n",
      "in attempt 221 the loss is 1.023385, best 0.896394\n",
      "in attempt 222 the loss is 0.981620, best 0.896394\n",
      "in attempt 223 the loss is 0.926302, best 0.896394\n",
      "in attempt 224 the loss is 0.950487, best 0.896394\n",
      "in attempt 225 the loss is 1.012070, best 0.896394\n",
      "in attempt 226 the loss is 0.997065, best 0.896394\n",
      "in attempt 227 the loss is 0.949778, best 0.896394\n",
      "in attempt 228 the loss is 1.069390, best 0.896394\n",
      "in attempt 229 the loss is 0.935564, best 0.896394\n",
      "in attempt 230 the loss is 0.990253, best 0.896394\n",
      "in attempt 231 the loss is 0.945053, best 0.896394\n",
      "in attempt 232 the loss is 0.901376, best 0.896394\n",
      "in attempt 233 the loss is 1.018981, best 0.896394\n",
      "in attempt 234 the loss is 0.909366, best 0.896394\n",
      "in attempt 235 the loss is 0.960363, best 0.896394\n",
      "in attempt 236 the loss is 1.065939, best 0.896394\n",
      "in attempt 237 the loss is 0.970349, best 0.896394\n",
      "in attempt 238 the loss is 1.108145, best 0.896394\n",
      "in attempt 239 the loss is 1.068771, best 0.896394\n",
      "in attempt 240 the loss is 0.994615, best 0.896394\n",
      "in attempt 241 the loss is 0.927325, best 0.896394\n",
      "in attempt 242 the loss is 0.994811, best 0.896394\n",
      "in attempt 243 the loss is 0.969865, best 0.896394\n",
      "in attempt 244 the loss is 1.057122, best 0.896394\n",
      "in attempt 245 the loss is 1.003440, best 0.896394\n",
      "in attempt 246 the loss is 0.936496, best 0.896394\n",
      "in attempt 247 the loss is 1.005948, best 0.896394\n",
      "in attempt 248 the loss is 1.079653, best 0.896394\n",
      "in attempt 249 the loss is 0.946494, best 0.896394\n",
      "in attempt 250 the loss is 1.047285, best 0.896394\n",
      "in attempt 251 the loss is 0.927775, best 0.896394\n",
      "in attempt 252 the loss is 0.942052, best 0.896394\n",
      "in attempt 253 the loss is 0.928275, best 0.896394\n",
      "in attempt 254 the loss is 0.946655, best 0.896394\n",
      "in attempt 255 the loss is 0.932645, best 0.896394\n",
      "in attempt 256 the loss is 1.093503, best 0.896394\n",
      "in attempt 257 the loss is 1.087136, best 0.896394\n",
      "in attempt 258 the loss is 0.983619, best 0.896394\n",
      "in attempt 259 the loss is 0.941720, best 0.896394\n",
      "in attempt 260 the loss is 0.960985, best 0.896394\n",
      "in attempt 261 the loss is 0.944965, best 0.896394\n",
      "in attempt 262 the loss is 0.939133, best 0.896394\n",
      "in attempt 263 the loss is 0.963371, best 0.896394\n",
      "in attempt 264 the loss is 0.928844, best 0.896394\n",
      "in attempt 265 the loss is 1.018032, best 0.896394\n",
      "in attempt 266 the loss is 1.007250, best 0.896394\n",
      "in attempt 267 the loss is 0.926783, best 0.896394\n",
      "in attempt 268 the loss is 0.954431, best 0.896394\n",
      "in attempt 269 the loss is 1.127560, best 0.896394\n",
      "in attempt 270 the loss is 0.947998, best 0.896394\n",
      "in attempt 271 the loss is 0.928184, best 0.896394\n",
      "in attempt 272 the loss is 0.936092, best 0.896394\n",
      "in attempt 273 the loss is 1.034101, best 0.896394\n",
      "in attempt 274 the loss is 1.017331, best 0.896394\n",
      "in attempt 275 the loss is 1.036786, best 0.896394\n",
      "in attempt 276 the loss is 1.013662, best 0.896394\n",
      "in attempt 277 the loss is 0.914609, best 0.896394\n",
      "in attempt 278 the loss is 0.967912, best 0.896394\n",
      "in attempt 279 the loss is 1.009935, best 0.896394\n",
      "in attempt 280 the loss is 1.006841, best 0.896394\n",
      "in attempt 281 the loss is 1.104160, best 0.896394\n",
      "in attempt 282 the loss is 1.095285, best 0.896394\n",
      "in attempt 283 the loss is 0.939590, best 0.896394\n",
      "in attempt 284 the loss is 1.041928, best 0.896394\n",
      "in attempt 285 the loss is 0.987461, best 0.896394\n",
      "in attempt 286 the loss is 1.041901, best 0.896394\n",
      "in attempt 287 the loss is 0.921015, best 0.896394\n",
      "in attempt 288 the loss is 0.990851, best 0.896394\n",
      "in attempt 289 the loss is 1.051419, best 0.896394\n",
      "in attempt 290 the loss is 0.961596, best 0.896394\n",
      "in attempt 291 the loss is 1.027878, best 0.896394\n",
      "in attempt 292 the loss is 0.914157, best 0.896394\n",
      "in attempt 293 the loss is 1.009816, best 0.896394\n",
      "in attempt 294 the loss is 0.948407, best 0.896394\n",
      "in attempt 295 the loss is 0.909916, best 0.896394\n",
      "in attempt 296 the loss is 0.992973, best 0.896394\n",
      "in attempt 297 the loss is 1.066323, best 0.896394\n",
      "in attempt 298 the loss is 0.983013, best 0.896394\n",
      "in attempt 299 the loss is 0.981244, best 0.896394\n",
      "in attempt 300 the loss is 0.966275, best 0.896394\n",
      "in attempt 301 the loss is 0.964307, best 0.896394\n",
      "in attempt 302 the loss is 1.022629, best 0.896394\n",
      "in attempt 303 the loss is 0.966345, best 0.896394\n",
      "in attempt 304 the loss is 0.940684, best 0.896394\n",
      "in attempt 305 the loss is 1.063003, best 0.896394\n",
      "in attempt 306 the loss is 1.015629, best 0.896394\n",
      "in attempt 307 the loss is 0.950082, best 0.896394\n",
      "in attempt 308 the loss is 1.071647, best 0.896394\n",
      "in attempt 309 the loss is 0.999798, best 0.896394\n",
      "in attempt 310 the loss is 1.029806, best 0.896394\n",
      "in attempt 311 the loss is 1.005076, best 0.896394\n",
      "in attempt 312 the loss is 1.011718, best 0.896394\n",
      "in attempt 313 the loss is 0.916748, best 0.896394\n",
      "in attempt 314 the loss is 0.900272, best 0.896394\n",
      "in attempt 315 the loss is 1.054954, best 0.896394\n",
      "in attempt 316 the loss is 1.071287, best 0.896394\n",
      "in attempt 317 the loss is 0.916291, best 0.896394\n",
      "in attempt 318 the loss is 0.970096, best 0.896394\n",
      "in attempt 319 the loss is 0.918318, best 0.896394\n",
      "in attempt 320 the loss is 1.072758, best 0.896394\n",
      "in attempt 321 the loss is 1.047983, best 0.896394\n",
      "in attempt 322 the loss is 1.017043, best 0.896394\n",
      "in attempt 323 the loss is 0.958660, best 0.896394\n",
      "in attempt 324 the loss is 0.929163, best 0.896394\n",
      "in attempt 325 the loss is 0.944844, best 0.896394\n",
      "in attempt 326 the loss is 1.048139, best 0.896394\n",
      "in attempt 327 the loss is 0.962561, best 0.896394\n",
      "in attempt 328 the loss is 1.019086, best 0.896394\n",
      "in attempt 329 the loss is 1.005403, best 0.896394\n",
      "in attempt 330 the loss is 0.940547, best 0.896394\n",
      "in attempt 331 the loss is 0.941310, best 0.896394\n",
      "in attempt 332 the loss is 0.921519, best 0.896394\n",
      "in attempt 333 the loss is 1.036780, best 0.896394\n",
      "in attempt 334 the loss is 1.033525, best 0.896394\n",
      "in attempt 335 the loss is 1.039268, best 0.896394\n",
      "in attempt 336 the loss is 1.036775, best 0.896394\n",
      "in attempt 337 the loss is 1.009450, best 0.896394\n",
      "in attempt 338 the loss is 0.949032, best 0.896394\n",
      "in attempt 339 the loss is 0.939970, best 0.896394\n",
      "in attempt 340 the loss is 0.948172, best 0.896394\n",
      "in attempt 341 the loss is 0.987510, best 0.896394\n",
      "in attempt 342 the loss is 1.009603, best 0.896394\n",
      "in attempt 343 the loss is 1.151553, best 0.896394\n",
      "in attempt 344 the loss is 0.926069, best 0.896394\n",
      "in attempt 345 the loss is 0.975059, best 0.896394\n",
      "in attempt 346 the loss is 1.017955, best 0.896394\n",
      "in attempt 347 the loss is 1.033965, best 0.896394\n",
      "in attempt 348 the loss is 1.026073, best 0.896394\n",
      "in attempt 349 the loss is 0.999768, best 0.896394\n",
      "in attempt 350 the loss is 1.021048, best 0.896394\n",
      "in attempt 351 the loss is 1.014657, best 0.896394\n",
      "in attempt 352 the loss is 0.937194, best 0.896394\n",
      "in attempt 353 the loss is 1.037733, best 0.896394\n",
      "in attempt 354 the loss is 0.931519, best 0.896394\n",
      "in attempt 355 the loss is 0.969719, best 0.896394\n",
      "in attempt 356 the loss is 1.091901, best 0.896394\n",
      "in attempt 357 the loss is 0.987335, best 0.896394\n",
      "in attempt 358 the loss is 0.897121, best 0.896394\n",
      "in attempt 359 the loss is 0.907561, best 0.896394\n",
      "in attempt 360 the loss is 1.028092, best 0.896394\n",
      "in attempt 361 the loss is 0.985896, best 0.896394\n",
      "in attempt 362 the loss is 0.989277, best 0.896394\n",
      "in attempt 363 the loss is 0.962782, best 0.896394\n",
      "in attempt 364 the loss is 0.970046, best 0.896394\n",
      "in attempt 365 the loss is 0.981510, best 0.896394\n",
      "in attempt 366 the loss is 0.950251, best 0.896394\n",
      "in attempt 367 the loss is 0.973256, best 0.896394\n",
      "in attempt 368 the loss is 0.977173, best 0.896394\n",
      "in attempt 369 the loss is 1.087554, best 0.896394\n",
      "in attempt 370 the loss is 0.906298, best 0.896394\n",
      "in attempt 371 the loss is 1.034795, best 0.896394\n",
      "in attempt 372 the loss is 0.951989, best 0.896394\n",
      "in attempt 373 the loss is 0.981114, best 0.896394\n",
      "in attempt 374 the loss is 1.113656, best 0.896394\n",
      "in attempt 375 the loss is 1.061843, best 0.896394\n",
      "in attempt 376 the loss is 0.983854, best 0.896394\n",
      "in attempt 377 the loss is 0.940573, best 0.896394\n",
      "in attempt 378 the loss is 0.995972, best 0.896394\n",
      "in attempt 379 the loss is 0.929287, best 0.896394\n",
      "in attempt 380 the loss is 0.978531, best 0.896394\n",
      "in attempt 381 the loss is 0.960510, best 0.896394\n",
      "in attempt 382 the loss is 0.927365, best 0.896394\n",
      "in attempt 383 the loss is 0.932739, best 0.896394\n",
      "in attempt 384 the loss is 0.939983, best 0.896394\n",
      "in attempt 385 the loss is 1.015110, best 0.896394\n",
      "in attempt 386 the loss is 0.946281, best 0.896394\n",
      "in attempt 387 the loss is 0.999191, best 0.896394\n",
      "in attempt 388 the loss is 0.936676, best 0.896394\n",
      "in attempt 389 the loss is 1.019634, best 0.896394\n",
      "in attempt 390 the loss is 0.984028, best 0.896394\n",
      "in attempt 391 the loss is 0.922038, best 0.896394\n",
      "in attempt 392 the loss is 1.009313, best 0.896394\n",
      "in attempt 393 the loss is 0.938166, best 0.896394\n",
      "in attempt 394 the loss is 0.947102, best 0.896394\n",
      "in attempt 395 the loss is 0.942229, best 0.896394\n",
      "in attempt 396 the loss is 0.959007, best 0.896394\n",
      "in attempt 397 the loss is 1.020037, best 0.896394\n",
      "in attempt 398 the loss is 0.998138, best 0.896394\n",
      "in attempt 399 the loss is 0.990570, best 0.896394\n",
      "in attempt 400 the loss is 0.937099, best 0.896394\n",
      "in attempt 401 the loss is 0.964095, best 0.896394\n",
      "in attempt 402 the loss is 0.909561, best 0.896394\n",
      "in attempt 403 the loss is 1.041621, best 0.896394\n",
      "in attempt 404 the loss is 0.944241, best 0.896394\n",
      "in attempt 405 the loss is 0.989388, best 0.896394\n",
      "in attempt 406 the loss is 1.021256, best 0.896394\n",
      "in attempt 407 the loss is 0.932534, best 0.896394\n",
      "in attempt 408 the loss is 1.071527, best 0.896394\n",
      "in attempt 409 the loss is 0.995483, best 0.896394\n",
      "in attempt 410 the loss is 1.027653, best 0.896394\n",
      "in attempt 411 the loss is 1.104200, best 0.896394\n",
      "in attempt 412 the loss is 0.944732, best 0.896394\n",
      "in attempt 413 the loss is 0.985677, best 0.896394\n",
      "in attempt 414 the loss is 0.972454, best 0.896394\n",
      "in attempt 415 the loss is 0.993367, best 0.896394\n",
      "in attempt 416 the loss is 1.072664, best 0.896394\n",
      "in attempt 417 the loss is 0.905205, best 0.896394\n",
      "in attempt 418 the loss is 1.015221, best 0.896394\n",
      "in attempt 419 the loss is 0.939299, best 0.896394\n",
      "in attempt 420 the loss is 0.952189, best 0.896394\n",
      "in attempt 421 the loss is 0.933256, best 0.896394\n",
      "in attempt 422 the loss is 1.087510, best 0.896394\n",
      "in attempt 423 the loss is 0.960459, best 0.896394\n",
      "in attempt 424 the loss is 0.902731, best 0.896394\n",
      "in attempt 425 the loss is 1.044915, best 0.896394\n",
      "in attempt 426 the loss is 0.916872, best 0.896394\n",
      "in attempt 427 the loss is 0.966074, best 0.896394\n",
      "in attempt 428 the loss is 1.026645, best 0.896394\n",
      "in attempt 429 the loss is 1.009939, best 0.896394\n",
      "in attempt 430 the loss is 1.010794, best 0.896394\n",
      "in attempt 431 the loss is 1.041736, best 0.896394\n",
      "in attempt 432 the loss is 0.959344, best 0.896394\n",
      "in attempt 433 the loss is 0.929923, best 0.896394\n",
      "in attempt 434 the loss is 1.110787, best 0.896394\n",
      "in attempt 435 the loss is 0.960226, best 0.896394\n",
      "in attempt 436 the loss is 0.939661, best 0.896394\n",
      "in attempt 437 the loss is 1.017227, best 0.896394\n",
      "in attempt 438 the loss is 0.943020, best 0.896394\n",
      "in attempt 439 the loss is 0.990160, best 0.896394\n",
      "in attempt 440 the loss is 0.943847, best 0.896394\n",
      "in attempt 441 the loss is 1.004149, best 0.896394\n",
      "in attempt 442 the loss is 0.996381, best 0.896394\n",
      "in attempt 443 the loss is 0.865049, best 0.865049\n",
      "in attempt 444 the loss is 0.969115, best 0.865049\n",
      "in attempt 445 the loss is 0.961651, best 0.865049\n",
      "in attempt 446 the loss is 0.999014, best 0.865049\n",
      "in attempt 447 the loss is 0.926652, best 0.865049\n",
      "in attempt 448 the loss is 1.006877, best 0.865049\n",
      "in attempt 449 the loss is 1.023310, best 0.865049\n",
      "in attempt 450 the loss is 1.072587, best 0.865049\n",
      "in attempt 451 the loss is 1.029421, best 0.865049\n",
      "in attempt 452 the loss is 0.963639, best 0.865049\n",
      "in attempt 453 the loss is 0.973950, best 0.865049\n",
      "in attempt 454 the loss is 1.050840, best 0.865049\n",
      "in attempt 455 the loss is 0.974486, best 0.865049\n",
      "in attempt 456 the loss is 1.042715, best 0.865049\n",
      "in attempt 457 the loss is 0.997196, best 0.865049\n",
      "in attempt 458 the loss is 1.038648, best 0.865049\n",
      "in attempt 459 the loss is 0.996685, best 0.865049\n",
      "in attempt 460 the loss is 0.959535, best 0.865049\n",
      "in attempt 461 the loss is 0.951651, best 0.865049\n",
      "in attempt 462 the loss is 1.073649, best 0.865049\n",
      "in attempt 463 the loss is 0.976913, best 0.865049\n",
      "in attempt 464 the loss is 1.075287, best 0.865049\n",
      "in attempt 465 the loss is 1.040345, best 0.865049\n",
      "in attempt 466 the loss is 0.983475, best 0.865049\n",
      "in attempt 467 the loss is 1.014732, best 0.865049\n",
      "in attempt 468 the loss is 0.998914, best 0.865049\n",
      "in attempt 469 the loss is 0.993969, best 0.865049\n",
      "in attempt 470 the loss is 0.966251, best 0.865049\n",
      "in attempt 471 the loss is 0.966167, best 0.865049\n",
      "in attempt 472 the loss is 0.947928, best 0.865049\n",
      "in attempt 473 the loss is 1.109678, best 0.865049\n",
      "in attempt 474 the loss is 0.929124, best 0.865049\n",
      "in attempt 475 the loss is 0.925846, best 0.865049\n",
      "in attempt 476 the loss is 1.071964, best 0.865049\n",
      "in attempt 477 the loss is 0.944909, best 0.865049\n",
      "in attempt 478 the loss is 0.944234, best 0.865049\n",
      "in attempt 479 the loss is 1.031793, best 0.865049\n",
      "in attempt 480 the loss is 0.955377, best 0.865049\n",
      "in attempt 481 the loss is 1.112764, best 0.865049\n",
      "in attempt 482 the loss is 1.018914, best 0.865049\n",
      "in attempt 483 the loss is 1.062707, best 0.865049\n",
      "in attempt 484 the loss is 1.033771, best 0.865049\n",
      "in attempt 485 the loss is 0.969145, best 0.865049\n",
      "in attempt 486 the loss is 1.008698, best 0.865049\n",
      "in attempt 487 the loss is 0.929024, best 0.865049\n",
      "in attempt 488 the loss is 1.125563, best 0.865049\n",
      "in attempt 489 the loss is 0.921658, best 0.865049\n",
      "in attempt 490 the loss is 1.098071, best 0.865049\n",
      "in attempt 491 the loss is 1.031423, best 0.865049\n",
      "in attempt 492 the loss is 0.927999, best 0.865049\n",
      "in attempt 493 the loss is 0.925667, best 0.865049\n",
      "in attempt 494 the loss is 0.996611, best 0.865049\n",
      "in attempt 495 the loss is 0.945698, best 0.865049\n",
      "in attempt 496 the loss is 0.990568, best 0.865049\n",
      "in attempt 497 the loss is 0.968019, best 0.865049\n",
      "in attempt 498 the loss is 0.975646, best 0.865049\n",
      "in attempt 499 the loss is 0.955794, best 0.865049\n",
      "in attempt 500 the loss is 0.975417, best 0.865049\n",
      "in attempt 501 the loss is 1.020137, best 0.865049\n",
      "in attempt 502 the loss is 0.978729, best 0.865049\n",
      "in attempt 503 the loss is 0.973504, best 0.865049\n",
      "in attempt 504 the loss is 1.024425, best 0.865049\n",
      "in attempt 505 the loss is 0.948028, best 0.865049\n",
      "in attempt 506 the loss is 0.979234, best 0.865049\n",
      "in attempt 507 the loss is 0.966212, best 0.865049\n",
      "in attempt 508 the loss is 0.963451, best 0.865049\n",
      "in attempt 509 the loss is 0.959323, best 0.865049\n",
      "in attempt 510 the loss is 0.991152, best 0.865049\n",
      "in attempt 511 the loss is 0.981216, best 0.865049\n",
      "in attempt 512 the loss is 1.089899, best 0.865049\n",
      "in attempt 513 the loss is 1.008589, best 0.865049\n",
      "in attempt 514 the loss is 0.968728, best 0.865049\n",
      "in attempt 515 the loss is 0.988614, best 0.865049\n",
      "in attempt 516 the loss is 0.964548, best 0.865049\n",
      "in attempt 517 the loss is 0.918350, best 0.865049\n",
      "in attempt 518 the loss is 1.024296, best 0.865049\n",
      "in attempt 519 the loss is 0.957376, best 0.865049\n",
      "in attempt 520 the loss is 1.018566, best 0.865049\n",
      "in attempt 521 the loss is 0.964824, best 0.865049\n",
      "in attempt 522 the loss is 1.008615, best 0.865049\n",
      "in attempt 523 the loss is 0.966713, best 0.865049\n",
      "in attempt 524 the loss is 0.949089, best 0.865049\n",
      "in attempt 525 the loss is 0.947823, best 0.865049\n",
      "in attempt 526 the loss is 1.000462, best 0.865049\n",
      "in attempt 527 the loss is 0.982744, best 0.865049\n",
      "in attempt 528 the loss is 0.963631, best 0.865049\n",
      "in attempt 529 the loss is 0.964694, best 0.865049\n",
      "in attempt 530 the loss is 0.934786, best 0.865049\n",
      "in attempt 531 the loss is 0.914928, best 0.865049\n",
      "in attempt 532 the loss is 1.000633, best 0.865049\n",
      "in attempt 533 the loss is 1.003372, best 0.865049\n",
      "in attempt 534 the loss is 0.970957, best 0.865049\n",
      "in attempt 535 the loss is 0.914008, best 0.865049\n",
      "in attempt 536 the loss is 1.107473, best 0.865049\n",
      "in attempt 537 the loss is 0.944833, best 0.865049\n",
      "in attempt 538 the loss is 0.918157, best 0.865049\n",
      "in attempt 539 the loss is 1.057428, best 0.865049\n",
      "in attempt 540 the loss is 0.974191, best 0.865049\n",
      "in attempt 541 the loss is 0.992697, best 0.865049\n",
      "in attempt 542 the loss is 0.979783, best 0.865049\n",
      "in attempt 543 the loss is 1.039431, best 0.865049\n",
      "in attempt 544 the loss is 0.955630, best 0.865049\n",
      "in attempt 545 the loss is 0.985981, best 0.865049\n",
      "in attempt 546 the loss is 0.960119, best 0.865049\n",
      "in attempt 547 the loss is 0.960133, best 0.865049\n",
      "in attempt 548 the loss is 1.019861, best 0.865049\n",
      "in attempt 549 the loss is 1.006435, best 0.865049\n",
      "in attempt 550 the loss is 1.026801, best 0.865049\n",
      "in attempt 551 the loss is 0.981266, best 0.865049\n",
      "in attempt 552 the loss is 1.062911, best 0.865049\n",
      "in attempt 553 the loss is 0.929040, best 0.865049\n",
      "in attempt 554 the loss is 1.019712, best 0.865049\n",
      "in attempt 555 the loss is 0.956584, best 0.865049\n",
      "in attempt 556 the loss is 0.988608, best 0.865049\n",
      "in attempt 557 the loss is 0.977576, best 0.865049\n",
      "in attempt 558 the loss is 0.943204, best 0.865049\n",
      "in attempt 559 the loss is 0.998539, best 0.865049\n",
      "in attempt 560 the loss is 0.966092, best 0.865049\n",
      "in attempt 561 the loss is 1.041918, best 0.865049\n",
      "in attempt 562 the loss is 0.992288, best 0.865049\n",
      "in attempt 563 the loss is 0.953057, best 0.865049\n",
      "in attempt 564 the loss is 0.927878, best 0.865049\n",
      "in attempt 565 the loss is 1.015375, best 0.865049\n",
      "in attempt 566 the loss is 0.921441, best 0.865049\n",
      "in attempt 567 the loss is 0.993381, best 0.865049\n",
      "in attempt 568 the loss is 1.019498, best 0.865049\n",
      "in attempt 569 the loss is 0.954694, best 0.865049\n",
      "in attempt 570 the loss is 0.981859, best 0.865049\n",
      "in attempt 571 the loss is 0.969655, best 0.865049\n",
      "in attempt 572 the loss is 1.033143, best 0.865049\n",
      "in attempt 573 the loss is 0.915407, best 0.865049\n",
      "in attempt 574 the loss is 1.056283, best 0.865049\n",
      "in attempt 575 the loss is 0.984988, best 0.865049\n",
      "in attempt 576 the loss is 0.949514, best 0.865049\n",
      "in attempt 577 the loss is 1.019345, best 0.865049\n",
      "in attempt 578 the loss is 0.929125, best 0.865049\n",
      "in attempt 579 the loss is 1.036822, best 0.865049\n",
      "in attempt 580 the loss is 0.967443, best 0.865049\n",
      "in attempt 581 the loss is 0.977111, best 0.865049\n",
      "in attempt 582 the loss is 1.038138, best 0.865049\n",
      "in attempt 583 the loss is 0.929381, best 0.865049\n",
      "in attempt 584 the loss is 0.995680, best 0.865049\n",
      "in attempt 585 the loss is 0.961179, best 0.865049\n",
      "in attempt 586 the loss is 1.011768, best 0.865049\n",
      "in attempt 587 the loss is 1.010766, best 0.865049\n",
      "in attempt 588 the loss is 0.978978, best 0.865049\n",
      "in attempt 589 the loss is 0.967124, best 0.865049\n",
      "in attempt 590 the loss is 1.038589, best 0.865049\n",
      "in attempt 591 the loss is 0.912475, best 0.865049\n",
      "in attempt 592 the loss is 0.994680, best 0.865049\n",
      "in attempt 593 the loss is 0.977017, best 0.865049\n",
      "in attempt 594 the loss is 1.058774, best 0.865049\n",
      "in attempt 595 the loss is 0.964679, best 0.865049\n",
      "in attempt 596 the loss is 0.960349, best 0.865049\n",
      "in attempt 597 the loss is 0.968424, best 0.865049\n",
      "in attempt 598 the loss is 0.986060, best 0.865049\n",
      "in attempt 599 the loss is 1.113417, best 0.865049\n",
      "in attempt 600 the loss is 0.983671, best 0.865049\n",
      "in attempt 601 the loss is 0.945779, best 0.865049\n",
      "in attempt 602 the loss is 0.983435, best 0.865049\n",
      "in attempt 603 the loss is 0.990765, best 0.865049\n",
      "in attempt 604 the loss is 0.940858, best 0.865049\n",
      "in attempt 605 the loss is 0.944061, best 0.865049\n",
      "in attempt 606 the loss is 0.982707, best 0.865049\n",
      "in attempt 607 the loss is 1.121988, best 0.865049\n",
      "in attempt 608 the loss is 1.001785, best 0.865049\n",
      "in attempt 609 the loss is 0.974262, best 0.865049\n",
      "in attempt 610 the loss is 0.925752, best 0.865049\n",
      "in attempt 611 the loss is 0.984822, best 0.865049\n",
      "in attempt 612 the loss is 0.995552, best 0.865049\n",
      "in attempt 613 the loss is 0.950474, best 0.865049\n",
      "in attempt 614 the loss is 0.966780, best 0.865049\n",
      "in attempt 615 the loss is 1.026643, best 0.865049\n",
      "in attempt 616 the loss is 0.972225, best 0.865049\n",
      "in attempt 617 the loss is 1.143911, best 0.865049\n",
      "in attempt 618 the loss is 1.042295, best 0.865049\n",
      "in attempt 619 the loss is 1.047381, best 0.865049\n",
      "in attempt 620 the loss is 1.029774, best 0.865049\n",
      "in attempt 621 the loss is 1.001959, best 0.865049\n",
      "in attempt 622 the loss is 1.054576, best 0.865049\n",
      "in attempt 623 the loss is 0.954038, best 0.865049\n",
      "in attempt 624 the loss is 0.930769, best 0.865049\n",
      "in attempt 625 the loss is 0.977853, best 0.865049\n",
      "in attempt 626 the loss is 0.983586, best 0.865049\n",
      "in attempt 627 the loss is 0.956403, best 0.865049\n",
      "in attempt 628 the loss is 1.048851, best 0.865049\n",
      "in attempt 629 the loss is 0.960978, best 0.865049\n",
      "in attempt 630 the loss is 0.941214, best 0.865049\n",
      "in attempt 631 the loss is 0.912609, best 0.865049\n",
      "in attempt 632 the loss is 1.104873, best 0.865049\n",
      "in attempt 633 the loss is 0.962699, best 0.865049\n",
      "in attempt 634 the loss is 0.912065, best 0.865049\n",
      "in attempt 635 the loss is 0.964677, best 0.865049\n",
      "in attempt 636 the loss is 0.954998, best 0.865049\n",
      "in attempt 637 the loss is 0.949232, best 0.865049\n",
      "in attempt 638 the loss is 1.116117, best 0.865049\n",
      "in attempt 639 the loss is 0.982822, best 0.865049\n",
      "in attempt 640 the loss is 0.965802, best 0.865049\n",
      "in attempt 641 the loss is 0.937334, best 0.865049\n",
      "in attempt 642 the loss is 0.943208, best 0.865049\n",
      "in attempt 643 the loss is 0.955490, best 0.865049\n",
      "in attempt 644 the loss is 0.937095, best 0.865049\n",
      "in attempt 645 the loss is 1.030021, best 0.865049\n",
      "in attempt 646 the loss is 0.962594, best 0.865049\n",
      "in attempt 647 the loss is 1.014641, best 0.865049\n",
      "in attempt 648 the loss is 0.992156, best 0.865049\n",
      "in attempt 649 the loss is 0.973455, best 0.865049\n",
      "in attempt 650 the loss is 0.962252, best 0.865049\n",
      "in attempt 651 the loss is 1.069600, best 0.865049\n",
      "in attempt 652 the loss is 0.907871, best 0.865049\n",
      "in attempt 653 the loss is 1.026976, best 0.865049\n",
      "in attempt 654 the loss is 0.923936, best 0.865049\n",
      "in attempt 655 the loss is 1.022662, best 0.865049\n",
      "in attempt 656 the loss is 1.016139, best 0.865049\n",
      "in attempt 657 the loss is 1.014644, best 0.865049\n",
      "in attempt 658 the loss is 0.946593, best 0.865049\n",
      "in attempt 659 the loss is 1.001385, best 0.865049\n",
      "in attempt 660 the loss is 1.009097, best 0.865049\n",
      "in attempt 661 the loss is 0.903702, best 0.865049\n",
      "in attempt 662 the loss is 1.031352, best 0.865049\n",
      "in attempt 663 the loss is 1.028063, best 0.865049\n",
      "in attempt 664 the loss is 0.969013, best 0.865049\n",
      "in attempt 665 the loss is 0.956662, best 0.865049\n",
      "in attempt 666 the loss is 0.950052, best 0.865049\n",
      "in attempt 667 the loss is 0.928441, best 0.865049\n",
      "in attempt 668 the loss is 1.024233, best 0.865049\n",
      "in attempt 669 the loss is 1.022897, best 0.865049\n",
      "in attempt 670 the loss is 0.956916, best 0.865049\n",
      "in attempt 671 the loss is 1.000819, best 0.865049\n",
      "in attempt 672 the loss is 1.033958, best 0.865049\n",
      "in attempt 673 the loss is 1.016056, best 0.865049\n",
      "in attempt 674 the loss is 0.941592, best 0.865049\n",
      "in attempt 675 the loss is 0.917770, best 0.865049\n",
      "in attempt 676 the loss is 0.952279, best 0.865049\n",
      "in attempt 677 the loss is 0.942189, best 0.865049\n",
      "in attempt 678 the loss is 0.940126, best 0.865049\n",
      "in attempt 679 the loss is 0.940227, best 0.865049\n",
      "in attempt 680 the loss is 0.957474, best 0.865049\n",
      "in attempt 681 the loss is 0.952261, best 0.865049\n",
      "in attempt 682 the loss is 0.975464, best 0.865049\n",
      "in attempt 683 the loss is 1.039792, best 0.865049\n",
      "in attempt 684 the loss is 0.925018, best 0.865049\n",
      "in attempt 685 the loss is 0.939271, best 0.865049\n",
      "in attempt 686 the loss is 1.048001, best 0.865049\n",
      "in attempt 687 the loss is 0.989683, best 0.865049\n",
      "in attempt 688 the loss is 1.037846, best 0.865049\n",
      "in attempt 689 the loss is 1.029761, best 0.865049\n",
      "in attempt 690 the loss is 0.923642, best 0.865049\n",
      "in attempt 691 the loss is 0.943229, best 0.865049\n",
      "in attempt 692 the loss is 1.093668, best 0.865049\n",
      "in attempt 693 the loss is 0.925138, best 0.865049\n",
      "in attempt 694 the loss is 0.953540, best 0.865049\n",
      "in attempt 695 the loss is 1.034251, best 0.865049\n",
      "in attempt 696 the loss is 1.028612, best 0.865049\n",
      "in attempt 697 the loss is 0.935429, best 0.865049\n",
      "in attempt 698 the loss is 0.992848, best 0.865049\n",
      "in attempt 699 the loss is 0.987520, best 0.865049\n",
      "in attempt 700 the loss is 1.005710, best 0.865049\n",
      "in attempt 701 the loss is 1.009825, best 0.865049\n",
      "in attempt 702 the loss is 1.081673, best 0.865049\n",
      "in attempt 703 the loss is 1.046851, best 0.865049\n",
      "in attempt 704 the loss is 0.976935, best 0.865049\n",
      "in attempt 705 the loss is 1.016718, best 0.865049\n",
      "in attempt 706 the loss is 0.925706, best 0.865049\n",
      "in attempt 707 the loss is 1.007365, best 0.865049\n",
      "in attempt 708 the loss is 0.963428, best 0.865049\n",
      "in attempt 709 the loss is 1.002407, best 0.865049\n",
      "in attempt 710 the loss is 0.989694, best 0.865049\n",
      "in attempt 711 the loss is 1.045569, best 0.865049\n",
      "in attempt 712 the loss is 1.045418, best 0.865049\n",
      "in attempt 713 the loss is 0.971031, best 0.865049\n",
      "in attempt 714 the loss is 0.921214, best 0.865049\n",
      "in attempt 715 the loss is 1.022090, best 0.865049\n",
      "in attempt 716 the loss is 0.947539, best 0.865049\n",
      "in attempt 717 the loss is 0.952535, best 0.865049\n",
      "in attempt 718 the loss is 0.929701, best 0.865049\n",
      "in attempt 719 the loss is 1.035778, best 0.865049\n",
      "in attempt 720 the loss is 0.976031, best 0.865049\n",
      "in attempt 721 the loss is 1.058350, best 0.865049\n",
      "in attempt 722 the loss is 1.102087, best 0.865049\n",
      "in attempt 723 the loss is 1.096919, best 0.865049\n",
      "in attempt 724 the loss is 0.969356, best 0.865049\n",
      "in attempt 725 the loss is 1.013174, best 0.865049\n",
      "in attempt 726 the loss is 1.013605, best 0.865049\n",
      "in attempt 727 the loss is 0.974017, best 0.865049\n",
      "in attempt 728 the loss is 1.087653, best 0.865049\n",
      "in attempt 729 the loss is 0.933258, best 0.865049\n",
      "in attempt 730 the loss is 0.904209, best 0.865049\n",
      "in attempt 731 the loss is 1.028630, best 0.865049\n",
      "in attempt 732 the loss is 0.923735, best 0.865049\n",
      "in attempt 733 the loss is 1.042450, best 0.865049\n",
      "in attempt 734 the loss is 0.943242, best 0.865049\n",
      "in attempt 735 the loss is 0.962947, best 0.865049\n",
      "in attempt 736 the loss is 0.936895, best 0.865049\n",
      "in attempt 737 the loss is 1.006876, best 0.865049\n",
      "in attempt 738 the loss is 0.943613, best 0.865049\n",
      "in attempt 739 the loss is 0.930160, best 0.865049\n",
      "in attempt 740 the loss is 0.945333, best 0.865049\n",
      "in attempt 741 the loss is 1.046823, best 0.865049\n",
      "in attempt 742 the loss is 0.963641, best 0.865049\n",
      "in attempt 743 the loss is 1.089389, best 0.865049\n",
      "in attempt 744 the loss is 1.079572, best 0.865049\n",
      "in attempt 745 the loss is 0.948364, best 0.865049\n",
      "in attempt 746 the loss is 0.924350, best 0.865049\n",
      "in attempt 747 the loss is 1.010908, best 0.865049\n",
      "in attempt 748 the loss is 0.962622, best 0.865049\n",
      "in attempt 749 the loss is 0.937199, best 0.865049\n",
      "in attempt 750 the loss is 0.953498, best 0.865049\n",
      "in attempt 751 the loss is 0.984672, best 0.865049\n",
      "in attempt 752 the loss is 0.999380, best 0.865049\n",
      "in attempt 753 the loss is 0.918986, best 0.865049\n",
      "in attempt 754 the loss is 0.990417, best 0.865049\n",
      "in attempt 755 the loss is 0.981609, best 0.865049\n",
      "in attempt 756 the loss is 1.097494, best 0.865049\n",
      "in attempt 757 the loss is 0.981765, best 0.865049\n",
      "in attempt 758 the loss is 0.993091, best 0.865049\n",
      "in attempt 759 the loss is 1.047643, best 0.865049\n",
      "in attempt 760 the loss is 1.046158, best 0.865049\n",
      "in attempt 761 the loss is 0.999517, best 0.865049\n",
      "in attempt 762 the loss is 1.033104, best 0.865049\n",
      "in attempt 763 the loss is 0.954966, best 0.865049\n",
      "in attempt 764 the loss is 0.962266, best 0.865049\n",
      "in attempt 765 the loss is 0.943290, best 0.865049\n",
      "in attempt 766 the loss is 0.974190, best 0.865049\n",
      "in attempt 767 the loss is 1.005153, best 0.865049\n",
      "in attempt 768 the loss is 0.958582, best 0.865049\n",
      "in attempt 769 the loss is 1.004120, best 0.865049\n",
      "in attempt 770 the loss is 1.000639, best 0.865049\n",
      "in attempt 771 the loss is 0.919734, best 0.865049\n",
      "in attempt 772 the loss is 1.076349, best 0.865049\n",
      "in attempt 773 the loss is 0.997339, best 0.865049\n",
      "in attempt 774 the loss is 1.066808, best 0.865049\n",
      "in attempt 775 the loss is 0.978187, best 0.865049\n",
      "in attempt 776 the loss is 0.960740, best 0.865049\n",
      "in attempt 777 the loss is 1.027165, best 0.865049\n",
      "in attempt 778 the loss is 0.938436, best 0.865049\n",
      "in attempt 779 the loss is 1.074621, best 0.865049\n",
      "in attempt 780 the loss is 0.992789, best 0.865049\n",
      "in attempt 781 the loss is 0.984649, best 0.865049\n",
      "in attempt 782 the loss is 1.016663, best 0.865049\n",
      "in attempt 783 the loss is 0.892416, best 0.865049\n",
      "in attempt 784 the loss is 0.936902, best 0.865049\n",
      "in attempt 785 the loss is 0.907440, best 0.865049\n",
      "in attempt 786 the loss is 0.987742, best 0.865049\n",
      "in attempt 787 the loss is 0.914213, best 0.865049\n",
      "in attempt 788 the loss is 0.984149, best 0.865049\n",
      "in attempt 789 the loss is 0.997405, best 0.865049\n",
      "in attempt 790 the loss is 1.063443, best 0.865049\n",
      "in attempt 791 the loss is 0.920278, best 0.865049\n",
      "in attempt 792 the loss is 1.024991, best 0.865049\n",
      "in attempt 793 the loss is 0.925399, best 0.865049\n",
      "in attempt 794 the loss is 0.990087, best 0.865049\n",
      "in attempt 795 the loss is 0.988636, best 0.865049\n",
      "in attempt 796 the loss is 0.990407, best 0.865049\n",
      "in attempt 797 the loss is 0.956584, best 0.865049\n",
      "in attempt 798 the loss is 0.906510, best 0.865049\n",
      "in attempt 799 the loss is 0.970882, best 0.865049\n",
      "in attempt 800 the loss is 0.979881, best 0.865049\n",
      "in attempt 801 the loss is 0.964570, best 0.865049\n",
      "in attempt 802 the loss is 0.982028, best 0.865049\n",
      "in attempt 803 the loss is 0.991505, best 0.865049\n",
      "in attempt 804 the loss is 0.961864, best 0.865049\n",
      "in attempt 805 the loss is 1.088600, best 0.865049\n",
      "in attempt 806 the loss is 1.000258, best 0.865049\n",
      "in attempt 807 the loss is 1.064726, best 0.865049\n",
      "in attempt 808 the loss is 0.963144, best 0.865049\n",
      "in attempt 809 the loss is 1.002105, best 0.865049\n",
      "in attempt 810 the loss is 1.021002, best 0.865049\n",
      "in attempt 811 the loss is 0.980012, best 0.865049\n",
      "in attempt 812 the loss is 0.989450, best 0.865049\n",
      "in attempt 813 the loss is 0.948529, best 0.865049\n",
      "in attempt 814 the loss is 0.937390, best 0.865049\n",
      "in attempt 815 the loss is 1.016971, best 0.865049\n",
      "in attempt 816 the loss is 0.988232, best 0.865049\n",
      "in attempt 817 the loss is 1.057992, best 0.865049\n",
      "in attempt 818 the loss is 1.012147, best 0.865049\n",
      "in attempt 819 the loss is 0.975551, best 0.865049\n",
      "in attempt 820 the loss is 1.080720, best 0.865049\n",
      "in attempt 821 the loss is 1.004758, best 0.865049\n",
      "in attempt 822 the loss is 0.985096, best 0.865049\n",
      "in attempt 823 the loss is 0.942934, best 0.865049\n",
      "in attempt 824 the loss is 0.986292, best 0.865049\n",
      "in attempt 825 the loss is 0.945926, best 0.865049\n",
      "in attempt 826 the loss is 1.026531, best 0.865049\n",
      "in attempt 827 the loss is 0.972844, best 0.865049\n",
      "in attempt 828 the loss is 0.986847, best 0.865049\n",
      "in attempt 829 the loss is 0.960044, best 0.865049\n",
      "in attempt 830 the loss is 0.975450, best 0.865049\n",
      "in attempt 831 the loss is 0.946262, best 0.865049\n",
      "in attempt 832 the loss is 1.032731, best 0.865049\n",
      "in attempt 833 the loss is 0.959690, best 0.865049\n",
      "in attempt 834 the loss is 1.071451, best 0.865049\n",
      "in attempt 835 the loss is 1.023742, best 0.865049\n",
      "in attempt 836 the loss is 0.985531, best 0.865049\n",
      "in attempt 837 the loss is 0.966421, best 0.865049\n",
      "in attempt 838 the loss is 0.973388, best 0.865049\n",
      "in attempt 839 the loss is 0.969715, best 0.865049\n",
      "in attempt 840 the loss is 0.885500, best 0.865049\n",
      "in attempt 841 the loss is 0.960717, best 0.865049\n",
      "in attempt 842 the loss is 0.976835, best 0.865049\n",
      "in attempt 843 the loss is 1.011176, best 0.865049\n",
      "in attempt 844 the loss is 0.943177, best 0.865049\n",
      "in attempt 845 the loss is 1.023224, best 0.865049\n",
      "in attempt 846 the loss is 0.972663, best 0.865049\n",
      "in attempt 847 the loss is 0.927998, best 0.865049\n",
      "in attempt 848 the loss is 0.929187, best 0.865049\n",
      "in attempt 849 the loss is 0.961612, best 0.865049\n",
      "in attempt 850 the loss is 0.955257, best 0.865049\n",
      "in attempt 851 the loss is 0.953314, best 0.865049\n",
      "in attempt 852 the loss is 0.980546, best 0.865049\n",
      "in attempt 853 the loss is 0.973427, best 0.865049\n",
      "in attempt 854 the loss is 0.911910, best 0.865049\n",
      "in attempt 855 the loss is 0.952406, best 0.865049\n",
      "in attempt 856 the loss is 0.977018, best 0.865049\n",
      "in attempt 857 the loss is 0.985120, best 0.865049\n",
      "in attempt 858 the loss is 0.935448, best 0.865049\n",
      "in attempt 859 the loss is 1.044795, best 0.865049\n",
      "in attempt 860 the loss is 0.931573, best 0.865049\n",
      "in attempt 861 the loss is 0.861914, best 0.861914\n",
      "in attempt 862 the loss is 0.966190, best 0.861914\n",
      "in attempt 863 the loss is 1.007564, best 0.861914\n",
      "in attempt 864 the loss is 0.889558, best 0.861914\n",
      "in attempt 865 the loss is 1.000278, best 0.861914\n",
      "in attempt 866 the loss is 0.956693, best 0.861914\n",
      "in attempt 867 the loss is 1.058131, best 0.861914\n",
      "in attempt 868 the loss is 0.931629, best 0.861914\n",
      "in attempt 869 the loss is 0.972625, best 0.861914\n",
      "in attempt 870 the loss is 0.927575, best 0.861914\n",
      "in attempt 871 the loss is 0.967177, best 0.861914\n",
      "in attempt 872 the loss is 1.048326, best 0.861914\n",
      "in attempt 873 the loss is 0.984946, best 0.861914\n",
      "in attempt 874 the loss is 1.090691, best 0.861914\n",
      "in attempt 875 the loss is 1.012451, best 0.861914\n",
      "in attempt 876 the loss is 0.995556, best 0.861914\n",
      "in attempt 877 the loss is 0.951690, best 0.861914\n",
      "in attempt 878 the loss is 0.956770, best 0.861914\n",
      "in attempt 879 the loss is 1.029128, best 0.861914\n",
      "in attempt 880 the loss is 1.046208, best 0.861914\n",
      "in attempt 881 the loss is 1.060716, best 0.861914\n",
      "in attempt 882 the loss is 1.041200, best 0.861914\n",
      "in attempt 883 the loss is 0.963872, best 0.861914\n",
      "in attempt 884 the loss is 0.996940, best 0.861914\n",
      "in attempt 885 the loss is 0.937985, best 0.861914\n",
      "in attempt 886 the loss is 1.109608, best 0.861914\n",
      "in attempt 887 the loss is 1.033447, best 0.861914\n",
      "in attempt 888 the loss is 0.990575, best 0.861914\n",
      "in attempt 889 the loss is 0.941819, best 0.861914\n",
      "in attempt 890 the loss is 0.945309, best 0.861914\n",
      "in attempt 891 the loss is 0.923720, best 0.861914\n",
      "in attempt 892 the loss is 0.946639, best 0.861914\n",
      "in attempt 893 the loss is 1.038970, best 0.861914\n",
      "in attempt 894 the loss is 1.042288, best 0.861914\n",
      "in attempt 895 the loss is 1.040728, best 0.861914\n",
      "in attempt 896 the loss is 0.955280, best 0.861914\n",
      "in attempt 897 the loss is 1.016170, best 0.861914\n",
      "in attempt 898 the loss is 0.976396, best 0.861914\n",
      "in attempt 899 the loss is 0.991216, best 0.861914\n",
      "in attempt 900 the loss is 0.945731, best 0.861914\n",
      "in attempt 901 the loss is 1.056265, best 0.861914\n",
      "in attempt 902 the loss is 1.055784, best 0.861914\n",
      "in attempt 903 the loss is 0.922470, best 0.861914\n",
      "in attempt 904 the loss is 1.037892, best 0.861914\n",
      "in attempt 905 the loss is 0.943696, best 0.861914\n",
      "in attempt 906 the loss is 0.961667, best 0.861914\n",
      "in attempt 907 the loss is 0.938975, best 0.861914\n",
      "in attempt 908 the loss is 0.952548, best 0.861914\n",
      "in attempt 909 the loss is 1.078510, best 0.861914\n",
      "in attempt 910 the loss is 0.945826, best 0.861914\n",
      "in attempt 911 the loss is 1.009693, best 0.861914\n",
      "in attempt 912 the loss is 0.956322, best 0.861914\n",
      "in attempt 913 the loss is 0.965664, best 0.861914\n",
      "in attempt 914 the loss is 0.948678, best 0.861914\n",
      "in attempt 915 the loss is 1.015291, best 0.861914\n",
      "in attempt 916 the loss is 0.979490, best 0.861914\n",
      "in attempt 917 the loss is 0.956438, best 0.861914\n",
      "in attempt 918 the loss is 0.983210, best 0.861914\n",
      "in attempt 919 the loss is 1.012257, best 0.861914\n",
      "in attempt 920 the loss is 0.961814, best 0.861914\n",
      "in attempt 921 the loss is 1.018485, best 0.861914\n",
      "in attempt 922 the loss is 0.983141, best 0.861914\n",
      "in attempt 923 the loss is 1.037841, best 0.861914\n",
      "in attempt 924 the loss is 1.034340, best 0.861914\n",
      "in attempt 925 the loss is 1.085908, best 0.861914\n",
      "in attempt 926 the loss is 0.973669, best 0.861914\n",
      "in attempt 927 the loss is 0.898896, best 0.861914\n",
      "in attempt 928 the loss is 1.009086, best 0.861914\n",
      "in attempt 929 the loss is 0.971904, best 0.861914\n",
      "in attempt 930 the loss is 1.024955, best 0.861914\n",
      "in attempt 931 the loss is 1.060637, best 0.861914\n",
      "in attempt 932 the loss is 0.936725, best 0.861914\n",
      "in attempt 933 the loss is 0.910513, best 0.861914\n",
      "in attempt 934 the loss is 1.008627, best 0.861914\n",
      "in attempt 935 the loss is 1.170414, best 0.861914\n",
      "in attempt 936 the loss is 0.970311, best 0.861914\n",
      "in attempt 937 the loss is 0.889859, best 0.861914\n",
      "in attempt 938 the loss is 1.050059, best 0.861914\n",
      "in attempt 939 the loss is 0.953739, best 0.861914\n",
      "in attempt 940 the loss is 1.053466, best 0.861914\n",
      "in attempt 941 the loss is 1.010136, best 0.861914\n",
      "in attempt 942 the loss is 0.949357, best 0.861914\n",
      "in attempt 943 the loss is 0.931144, best 0.861914\n",
      "in attempt 944 the loss is 1.014772, best 0.861914\n",
      "in attempt 945 the loss is 0.951416, best 0.861914\n",
      "in attempt 946 the loss is 0.961095, best 0.861914\n",
      "in attempt 947 the loss is 0.938880, best 0.861914\n",
      "in attempt 948 the loss is 1.058606, best 0.861914\n",
      "in attempt 949 the loss is 0.955053, best 0.861914\n",
      "in attempt 950 the loss is 0.991734, best 0.861914\n",
      "in attempt 951 the loss is 1.007263, best 0.861914\n",
      "in attempt 952 the loss is 0.952031, best 0.861914\n",
      "in attempt 953 the loss is 0.923028, best 0.861914\n",
      "in attempt 954 the loss is 1.012564, best 0.861914\n",
      "in attempt 955 the loss is 0.984564, best 0.861914\n",
      "in attempt 956 the loss is 0.955747, best 0.861914\n",
      "in attempt 957 the loss is 0.899968, best 0.861914\n",
      "in attempt 958 the loss is 0.976440, best 0.861914\n",
      "in attempt 959 the loss is 0.910615, best 0.861914\n",
      "in attempt 960 the loss is 0.994553, best 0.861914\n",
      "in attempt 961 the loss is 0.951964, best 0.861914\n",
      "in attempt 962 the loss is 1.041199, best 0.861914\n",
      "in attempt 963 the loss is 1.130202, best 0.861914\n",
      "in attempt 964 the loss is 0.923669, best 0.861914\n",
      "in attempt 965 the loss is 0.987492, best 0.861914\n",
      "in attempt 966 the loss is 0.971910, best 0.861914\n",
      "in attempt 967 the loss is 1.040175, best 0.861914\n",
      "in attempt 968 the loss is 1.038599, best 0.861914\n",
      "in attempt 969 the loss is 1.006627, best 0.861914\n",
      "in attempt 970 the loss is 0.920389, best 0.861914\n",
      "in attempt 971 the loss is 0.973905, best 0.861914\n",
      "in attempt 972 the loss is 0.940553, best 0.861914\n",
      "in attempt 973 the loss is 0.955455, best 0.861914\n",
      "in attempt 974 the loss is 0.968668, best 0.861914\n",
      "in attempt 975 the loss is 0.972286, best 0.861914\n",
      "in attempt 976 the loss is 1.036293, best 0.861914\n",
      "in attempt 977 the loss is 1.006375, best 0.861914\n",
      "in attempt 978 the loss is 0.953469, best 0.861914\n",
      "in attempt 979 the loss is 0.974503, best 0.861914\n",
      "in attempt 980 the loss is 1.051095, best 0.861914\n",
      "in attempt 981 the loss is 1.066422, best 0.861914\n",
      "in attempt 982 the loss is 1.124847, best 0.861914\n",
      "in attempt 983 the loss is 0.968259, best 0.861914\n",
      "in attempt 984 the loss is 0.888248, best 0.861914\n",
      "in attempt 985 the loss is 0.935188, best 0.861914\n",
      "in attempt 986 the loss is 0.914676, best 0.861914\n",
      "in attempt 987 the loss is 0.954697, best 0.861914\n",
      "in attempt 988 the loss is 1.040030, best 0.861914\n",
      "in attempt 989 the loss is 1.048915, best 0.861914\n",
      "in attempt 990 the loss is 0.979709, best 0.861914\n",
      "in attempt 991 the loss is 0.968325, best 0.861914\n",
      "in attempt 992 the loss is 0.964512, best 0.861914\n",
      "in attempt 993 the loss is 0.980570, best 0.861914\n",
      "in attempt 994 the loss is 0.968970, best 0.861914\n",
      "in attempt 995 the loss is 0.994686, best 0.861914\n",
      "in attempt 996 the loss is 1.189771, best 0.861914\n",
      "in attempt 997 the loss is 0.935502, best 0.861914\n",
      "in attempt 998 the loss is 0.984383, best 0.861914\n",
      "in attempt 999 the loss is 1.026964, best 0.861914\n",
      "in attempt 1000 the loss is 1.045656, best 0.861914\n"
     ]
    }
   ],
   "source": [
    "Lambda=.001\n",
    "best_loss_svm=float(\"inf\")\n",
    "num_labels=10  #for CIFAR-10 data there are 10 classes\n",
    "num_features=Xtr_rows.shape[1]\n",
    "for num in range(1000):\n",
    "    W_svm=np.random.randn(num_labels,num_features)*.0001\n",
    "    cost_svm=svm_loss(Xtr_rows.T,train_labels,W_svm,Lambda)\n",
    "    if cost_svm< best_loss_svm:\n",
    "        best_loss_svm=cost_svm\n",
    "        best_W_svm=W_svm\n",
    "    print(\"in attempt %d the loss is %f, best %f\"%(num+1,cost_svm,best_loss_svm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the best W we get from the above iterations, we take it and try it out on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 3072)\n"
     ]
    }
   ],
   "source": [
    "print(Xte_rows.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 3073)\n"
     ]
    }
   ],
   "source": [
    "#adding intercept term to the test set\n",
    "Xte_rows=np.hstack((Xte_rows,np.ones((10000,1))))\n",
    "print(Xte_rows.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "By random search optimization the accuracy is : 12.47 %\n"
     ]
    }
   ],
   "source": [
    "#calculating scores on the optimized W and see its performance\n",
    "scores_svm=np.dot(best_W_svm,Xte_rows.T)\n",
    "#find the index with max score in each column(the predicted class)\n",
    "y_pred_svm=np.argmax(scores_svm,axis=0)\n",
    "# calculating the accuracy(fractions of predictions that are correct)\n",
    "acc_svm=np.mean(y_pred_svm==test_labels) \n",
    "print(f\"By random search optimization the accuracy is : {acc_svm*100:.2f} %\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets calculate accuray using cross-entropy loss and random search optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in attepmt 1: loss=2.4426735868463045,best:2.4426735868463045\n",
      "in attepmt 2: loss=2.5617952275027425,best:2.4426735868463045\n",
      "in attepmt 3: loss=2.5806271097589533,best:2.4426735868463045\n",
      "in attepmt 4: loss=2.6800355716125583,best:2.4426735868463045\n",
      "in attepmt 5: loss=2.4126602600613505,best:2.4126602600613505\n",
      "in attepmt 6: loss=2.5582645872469394,best:2.4126602600613505\n",
      "in attepmt 7: loss=2.979802991327935,best:2.4126602600613505\n",
      "in attepmt 8: loss=2.502732896778505,best:2.4126602600613505\n",
      "in attepmt 9: loss=2.5351292884053493,best:2.4126602600613505\n",
      "in attepmt 10: loss=2.565437951397319,best:2.4126602600613505\n",
      "in attepmt 11: loss=2.399962840768322,best:2.399962840768322\n",
      "in attepmt 12: loss=2.6071630117814544,best:2.399962840768322\n",
      "in attepmt 13: loss=2.4896330487148703,best:2.399962840768322\n",
      "in attepmt 14: loss=2.546857836396865,best:2.399962840768322\n",
      "in attepmt 15: loss=2.549954304369954,best:2.399962840768322\n",
      "in attepmt 16: loss=2.5248069219549043,best:2.399962840768322\n",
      "in attepmt 17: loss=2.5347326422933647,best:2.399962840768322\n",
      "in attepmt 18: loss=2.5650227550193345,best:2.399962840768322\n",
      "in attepmt 19: loss=2.8001261367721915,best:2.399962840768322\n",
      "in attepmt 20: loss=2.431544542251008,best:2.399962840768322\n",
      "in attepmt 21: loss=2.6039540513791484,best:2.399962840768322\n",
      "in attepmt 22: loss=2.8576670469347825,best:2.399962840768322\n",
      "in attepmt 23: loss=2.468084269135994,best:2.399962840768322\n",
      "in attepmt 24: loss=2.4005825376504033,best:2.399962840768322\n",
      "in attepmt 25: loss=2.5040911410111497,best:2.399962840768322\n",
      "in attepmt 26: loss=2.63466679514897,best:2.399962840768322\n",
      "in attepmt 27: loss=2.658134712659033,best:2.399962840768322\n",
      "in attepmt 28: loss=2.502547788604498,best:2.399962840768322\n",
      "in attepmt 29: loss=2.505567149005648,best:2.399962840768322\n",
      "in attepmt 30: loss=2.7496441121396265,best:2.399962840768322\n",
      "in attepmt 31: loss=2.588951705414827,best:2.399962840768322\n",
      "in attepmt 32: loss=2.3796888427270284,best:2.3796888427270284\n",
      "in attepmt 33: loss=2.4468340626470857,best:2.3796888427270284\n",
      "in attepmt 34: loss=2.5166254515407647,best:2.3796888427270284\n",
      "in attepmt 35: loss=2.6395364232877316,best:2.3796888427270284\n",
      "in attepmt 36: loss=2.502437679903257,best:2.3796888427270284\n",
      "in attepmt 37: loss=2.4163917974825466,best:2.3796888427270284\n",
      "in attepmt 38: loss=2.732772099211422,best:2.3796888427270284\n",
      "in attepmt 39: loss=2.5746763363228427,best:2.3796888427270284\n",
      "in attepmt 40: loss=2.723562342025365,best:2.3796888427270284\n",
      "in attepmt 41: loss=2.5220256623582844,best:2.3796888427270284\n",
      "in attepmt 42: loss=2.5583879489645955,best:2.3796888427270284\n",
      "in attepmt 43: loss=2.5384004104212763,best:2.3796888427270284\n",
      "in attepmt 44: loss=2.6877645553707645,best:2.3796888427270284\n",
      "in attepmt 45: loss=2.500035050592642,best:2.3796888427270284\n",
      "in attepmt 46: loss=2.639196966484243,best:2.3796888427270284\n",
      "in attepmt 47: loss=2.6642024853987563,best:2.3796888427270284\n",
      "in attepmt 48: loss=2.6789271503290113,best:2.3796888427270284\n",
      "in attepmt 49: loss=2.5350220403908352,best:2.3796888427270284\n",
      "in attepmt 50: loss=2.6421001490718914,best:2.3796888427270284\n",
      "in attepmt 51: loss=2.530046404885548,best:2.3796888427270284\n",
      "in attepmt 52: loss=2.588149110373412,best:2.3796888427270284\n",
      "in attepmt 53: loss=2.5735121830125385,best:2.3796888427270284\n",
      "in attepmt 54: loss=2.6188130394063283,best:2.3796888427270284\n",
      "in attepmt 55: loss=2.6060069662058587,best:2.3796888427270284\n",
      "in attepmt 56: loss=2.5580348221270177,best:2.3796888427270284\n",
      "in attepmt 57: loss=2.4850197023701566,best:2.3796888427270284\n",
      "in attepmt 58: loss=2.500543404111329,best:2.3796888427270284\n",
      "in attepmt 59: loss=2.562406757677946,best:2.3796888427270284\n",
      "in attepmt 60: loss=2.6611158872274503,best:2.3796888427270284\n",
      "in attepmt 61: loss=2.523852317886608,best:2.3796888427270284\n",
      "in attepmt 62: loss=2.5433331744713605,best:2.3796888427270284\n",
      "in attepmt 63: loss=2.5438205150497013,best:2.3796888427270284\n",
      "in attepmt 64: loss=2.4645294728097724,best:2.3796888427270284\n",
      "in attepmt 65: loss=2.5617115743615684,best:2.3796888427270284\n",
      "in attepmt 66: loss=2.543902870081747,best:2.3796888427270284\n",
      "in attepmt 67: loss=2.461106100587934,best:2.3796888427270284\n",
      "in attepmt 68: loss=2.5387695140092212,best:2.3796888427270284\n",
      "in attepmt 69: loss=2.7499425975800014,best:2.3796888427270284\n",
      "in attepmt 70: loss=2.6822122421917336,best:2.3796888427270284\n",
      "in attepmt 71: loss=2.7993577671857626,best:2.3796888427270284\n",
      "in attepmt 72: loss=2.4653194712521014,best:2.3796888427270284\n",
      "in attepmt 73: loss=2.572642050449062,best:2.3796888427270284\n",
      "in attepmt 74: loss=2.4929027392155367,best:2.3796888427270284\n",
      "in attepmt 75: loss=2.514437481762001,best:2.3796888427270284\n",
      "in attepmt 76: loss=2.704648322447415,best:2.3796888427270284\n",
      "in attepmt 77: loss=2.547666354140154,best:2.3796888427270284\n",
      "in attepmt 78: loss=2.4643633904259543,best:2.3796888427270284\n",
      "in attepmt 79: loss=2.648295293453873,best:2.3796888427270284\n",
      "in attepmt 80: loss=2.6034629865911842,best:2.3796888427270284\n",
      "in attepmt 81: loss=2.4906097422349003,best:2.3796888427270284\n",
      "in attepmt 82: loss=2.4848211815932473,best:2.3796888427270284\n",
      "in attepmt 83: loss=2.550150825306456,best:2.3796888427270284\n",
      "in attepmt 84: loss=2.4484732609139006,best:2.3796888427270284\n",
      "in attepmt 85: loss=2.592051348092433,best:2.3796888427270284\n",
      "in attepmt 86: loss=2.483073316566364,best:2.3796888427270284\n",
      "in attepmt 87: loss=2.7079695745407313,best:2.3796888427270284\n",
      "in attepmt 88: loss=2.4670537866587727,best:2.3796888427270284\n",
      "in attepmt 89: loss=2.4654649366137313,best:2.3796888427270284\n",
      "in attepmt 90: loss=2.649473226641501,best:2.3796888427270284\n",
      "in attepmt 91: loss=2.6754137014202453,best:2.3796888427270284\n",
      "in attepmt 92: loss=2.4099912612566485,best:2.3796888427270284\n",
      "in attepmt 93: loss=2.5299674455167764,best:2.3796888427270284\n",
      "in attepmt 94: loss=2.4829161349855506,best:2.3796888427270284\n",
      "in attepmt 95: loss=2.425818251286292,best:2.3796888427270284\n",
      "in attepmt 96: loss=2.426566609876606,best:2.3796888427270284\n",
      "in attepmt 97: loss=2.6270116720296373,best:2.3796888427270284\n",
      "in attepmt 98: loss=2.6177149735448997,best:2.3796888427270284\n",
      "in attepmt 99: loss=2.4688457526770744,best:2.3796888427270284\n",
      "in attepmt 100: loss=2.476911460187077,best:2.3796888427270284\n",
      "in attepmt 101: loss=2.4905145282371985,best:2.3796888427270284\n",
      "in attepmt 102: loss=2.477130319061624,best:2.3796888427270284\n",
      "in attepmt 103: loss=2.501245487010223,best:2.3796888427270284\n",
      "in attepmt 104: loss=2.4686690179084687,best:2.3796888427270284\n",
      "in attepmt 105: loss=2.66963061656364,best:2.3796888427270284\n",
      "in attepmt 106: loss=2.591524678304884,best:2.3796888427270284\n",
      "in attepmt 107: loss=2.5282016226396355,best:2.3796888427270284\n",
      "in attepmt 108: loss=2.452449274145886,best:2.3796888427270284\n",
      "in attepmt 109: loss=2.4730574378241275,best:2.3796888427270284\n",
      "in attepmt 110: loss=2.54139248442327,best:2.3796888427270284\n",
      "in attepmt 111: loss=2.5104598769328175,best:2.3796888427270284\n",
      "in attepmt 112: loss=2.4277897866903038,best:2.3796888427270284\n",
      "in attepmt 113: loss=2.5628005849526208,best:2.3796888427270284\n",
      "in attepmt 114: loss=2.6561118008490583,best:2.3796888427270284\n",
      "in attepmt 115: loss=2.5312762158956095,best:2.3796888427270284\n",
      "in attepmt 116: loss=2.560368045884132,best:2.3796888427270284\n",
      "in attepmt 117: loss=2.522322625020599,best:2.3796888427270284\n",
      "in attepmt 118: loss=2.5060319787378544,best:2.3796888427270284\n",
      "in attepmt 119: loss=2.570002458048749,best:2.3796888427270284\n",
      "in attepmt 120: loss=2.522340960788654,best:2.3796888427270284\n",
      "in attepmt 121: loss=2.6703928453842973,best:2.3796888427270284\n",
      "in attepmt 122: loss=2.6998589414565743,best:2.3796888427270284\n",
      "in attepmt 123: loss=2.4045275156751016,best:2.3796888427270284\n",
      "in attepmt 124: loss=2.4566458029648865,best:2.3796888427270284\n",
      "in attepmt 125: loss=2.5169924661538308,best:2.3796888427270284\n",
      "in attepmt 126: loss=2.43299722236264,best:2.3796888427270284\n",
      "in attepmt 127: loss=2.5315619581249194,best:2.3796888427270284\n",
      "in attepmt 128: loss=2.518350998792955,best:2.3796888427270284\n",
      "in attepmt 129: loss=2.8154697594965117,best:2.3796888427270284\n",
      "in attepmt 130: loss=2.468691191346936,best:2.3796888427270284\n",
      "in attepmt 131: loss=2.6375374794436066,best:2.3796888427270284\n",
      "in attepmt 132: loss=2.402174716846388,best:2.3796888427270284\n",
      "in attepmt 133: loss=2.3907605401718905,best:2.3796888427270284\n",
      "in attepmt 134: loss=2.528573224188669,best:2.3796888427270284\n",
      "in attepmt 135: loss=2.531656075431678,best:2.3796888427270284\n",
      "in attepmt 136: loss=2.5936091872478912,best:2.3796888427270284\n",
      "in attepmt 137: loss=2.5542820625227307,best:2.3796888427270284\n",
      "in attepmt 138: loss=2.492471387377483,best:2.3796888427270284\n",
      "in attepmt 139: loss=2.4588252780556696,best:2.3796888427270284\n",
      "in attepmt 140: loss=2.526613308045359,best:2.3796888427270284\n",
      "in attepmt 141: loss=2.486327615415283,best:2.3796888427270284\n",
      "in attepmt 142: loss=2.5236397762375007,best:2.3796888427270284\n",
      "in attepmt 143: loss=2.720835905269995,best:2.3796888427270284\n",
      "in attepmt 144: loss=2.4853497862119243,best:2.3796888427270284\n",
      "in attepmt 145: loss=2.544217351303081,best:2.3796888427270284\n",
      "in attepmt 146: loss=2.5080827398353778,best:2.3796888427270284\n",
      "in attepmt 147: loss=2.353503288215445,best:2.353503288215445\n",
      "in attepmt 148: loss=2.504108585543788,best:2.353503288215445\n",
      "in attepmt 149: loss=2.5424385684051916,best:2.353503288215445\n",
      "in attepmt 150: loss=2.608183952441134,best:2.353503288215445\n",
      "in attepmt 151: loss=2.4619017721242904,best:2.353503288215445\n",
      "in attepmt 152: loss=2.4536672200111203,best:2.353503288215445\n",
      "in attepmt 153: loss=2.452918268065712,best:2.353503288215445\n",
      "in attepmt 154: loss=2.6628147626833902,best:2.353503288215445\n",
      "in attepmt 155: loss=2.4064793261954858,best:2.353503288215445\n",
      "in attepmt 156: loss=2.5078501098361112,best:2.353503288215445\n",
      "in attepmt 157: loss=2.58838767128845,best:2.353503288215445\n",
      "in attepmt 158: loss=2.6852271677203,best:2.353503288215445\n",
      "in attepmt 159: loss=2.785280515071249,best:2.353503288215445\n",
      "in attepmt 160: loss=2.5378010902641734,best:2.353503288215445\n",
      "in attepmt 161: loss=2.680650204614933,best:2.353503288215445\n",
      "in attepmt 162: loss=2.4340987194404384,best:2.353503288215445\n",
      "in attepmt 163: loss=3.0526872041557733,best:2.353503288215445\n",
      "in attepmt 164: loss=2.53735403158344,best:2.353503288215445\n",
      "in attepmt 165: loss=2.393978223404187,best:2.353503288215445\n",
      "in attepmt 166: loss=2.601051723803245,best:2.353503288215445\n",
      "in attepmt 167: loss=2.5013904481485665,best:2.353503288215445\n",
      "in attepmt 168: loss=2.4746332513908578,best:2.353503288215445\n",
      "in attepmt 169: loss=2.509944820022982,best:2.353503288215445\n",
      "in attepmt 170: loss=2.670159877552786,best:2.353503288215445\n",
      "in attepmt 171: loss=2.852937177416481,best:2.353503288215445\n",
      "in attepmt 172: loss=2.641669680456925,best:2.353503288215445\n",
      "in attepmt 173: loss=2.515182217228478,best:2.353503288215445\n",
      "in attepmt 174: loss=2.570103700342256,best:2.353503288215445\n",
      "in attepmt 175: loss=2.473435531649149,best:2.353503288215445\n",
      "in attepmt 176: loss=2.48804163685706,best:2.353503288215445\n",
      "in attepmt 177: loss=2.501282425855183,best:2.353503288215445\n",
      "in attepmt 178: loss=2.4849956290715407,best:2.353503288215445\n",
      "in attepmt 179: loss=3.0752375796825944,best:2.353503288215445\n",
      "in attepmt 180: loss=2.706538751224082,best:2.353503288215445\n",
      "in attepmt 181: loss=2.4926186021607335,best:2.353503288215445\n",
      "in attepmt 182: loss=2.5239431798169814,best:2.353503288215445\n",
      "in attepmt 183: loss=2.508310733010438,best:2.353503288215445\n",
      "in attepmt 184: loss=2.402944979644304,best:2.353503288215445\n",
      "in attepmt 185: loss=2.7187219725633742,best:2.353503288215445\n",
      "in attepmt 186: loss=2.5430215674865355,best:2.353503288215445\n",
      "in attepmt 187: loss=2.677886322885741,best:2.353503288215445\n",
      "in attepmt 188: loss=2.5620910807325052,best:2.353503288215445\n",
      "in attepmt 189: loss=2.431516107916241,best:2.353503288215445\n",
      "in attepmt 190: loss=2.4735515504240073,best:2.353503288215445\n",
      "in attepmt 191: loss=2.444767323528919,best:2.353503288215445\n",
      "in attepmt 192: loss=2.514974742121074,best:2.353503288215445\n",
      "in attepmt 193: loss=2.5435691943539207,best:2.353503288215445\n",
      "in attepmt 194: loss=2.4328036143174137,best:2.353503288215445\n",
      "in attepmt 195: loss=2.5366194359549414,best:2.353503288215445\n",
      "in attepmt 196: loss=2.4724381698879845,best:2.353503288215445\n",
      "in attepmt 197: loss=2.4673919923543486,best:2.353503288215445\n",
      "in attepmt 198: loss=2.6868179099467313,best:2.353503288215445\n",
      "in attepmt 199: loss=2.5252501301095633,best:2.353503288215445\n",
      "in attepmt 200: loss=2.7579984251038807,best:2.353503288215445\n",
      "in attepmt 201: loss=2.6022814993428907,best:2.353503288215445\n",
      "in attepmt 202: loss=2.5555056736395914,best:2.353503288215445\n",
      "in attepmt 203: loss=2.7997926040819294,best:2.353503288215445\n",
      "in attepmt 204: loss=2.619014163600798,best:2.353503288215445\n",
      "in attepmt 205: loss=2.5930612668861595,best:2.353503288215445\n",
      "in attepmt 206: loss=2.5073153184007646,best:2.353503288215445\n",
      "in attepmt 207: loss=2.461199375932337,best:2.353503288215445\n",
      "in attepmt 208: loss=2.489060115556617,best:2.353503288215445\n",
      "in attepmt 209: loss=2.5254183003025172,best:2.353503288215445\n",
      "in attepmt 210: loss=2.4218899558078744,best:2.353503288215445\n",
      "in attepmt 211: loss=2.4582368314649736,best:2.353503288215445\n",
      "in attepmt 212: loss=2.677992070921022,best:2.353503288215445\n",
      "in attepmt 213: loss=2.7916975685137952,best:2.353503288215445\n",
      "in attepmt 214: loss=2.522312706027168,best:2.353503288215445\n",
      "in attepmt 215: loss=2.492609178696023,best:2.353503288215445\n",
      "in attepmt 216: loss=2.5219919888188773,best:2.353503288215445\n",
      "in attepmt 217: loss=2.630339793447715,best:2.353503288215445\n",
      "in attepmt 218: loss=2.5679838335907377,best:2.353503288215445\n",
      "in attepmt 219: loss=2.6851369747991494,best:2.353503288215445\n",
      "in attepmt 220: loss=2.7290583917475653,best:2.353503288215445\n",
      "in attepmt 221: loss=2.6316292400981016,best:2.353503288215445\n",
      "in attepmt 222: loss=2.5162867080119797,best:2.353503288215445\n",
      "in attepmt 223: loss=2.5028284994886767,best:2.353503288215445\n",
      "in attepmt 224: loss=2.4909358630613903,best:2.353503288215445\n",
      "in attepmt 225: loss=2.572687807151822,best:2.353503288215445\n",
      "in attepmt 226: loss=2.4238313273655736,best:2.353503288215445\n",
      "in attepmt 227: loss=2.5650533767166994,best:2.353503288215445\n",
      "in attepmt 228: loss=2.5543304060564656,best:2.353503288215445\n",
      "in attepmt 229: loss=2.644859198507333,best:2.353503288215445\n",
      "in attepmt 230: loss=2.38878909993016,best:2.353503288215445\n",
      "in attepmt 231: loss=2.4576422605586705,best:2.353503288215445\n",
      "in attepmt 232: loss=2.52329543353529,best:2.353503288215445\n",
      "in attepmt 233: loss=2.429199755891991,best:2.353503288215445\n",
      "in attepmt 234: loss=2.584342842449112,best:2.353503288215445\n",
      "in attepmt 235: loss=2.5225677076960458,best:2.353503288215445\n",
      "in attepmt 236: loss=2.5312122803062516,best:2.353503288215445\n",
      "in attepmt 237: loss=2.583205380996558,best:2.353503288215445\n",
      "in attepmt 238: loss=2.5115385289983,best:2.353503288215445\n",
      "in attepmt 239: loss=2.6818294769903424,best:2.353503288215445\n",
      "in attepmt 240: loss=2.6608520216066345,best:2.353503288215445\n",
      "in attepmt 241: loss=2.632444741480023,best:2.353503288215445\n",
      "in attepmt 242: loss=2.4897806712867863,best:2.353503288215445\n",
      "in attepmt 243: loss=2.646227319419642,best:2.353503288215445\n",
      "in attepmt 244: loss=2.5233456832228156,best:2.353503288215445\n",
      "in attepmt 245: loss=2.4507302392600634,best:2.353503288215445\n",
      "in attepmt 246: loss=2.385032063009849,best:2.353503288215445\n",
      "in attepmt 247: loss=2.5225448845214227,best:2.353503288215445\n",
      "in attepmt 248: loss=2.5336355939826647,best:2.353503288215445\n",
      "in attepmt 249: loss=2.460193044009051,best:2.353503288215445\n",
      "in attepmt 250: loss=2.5344988939928887,best:2.353503288215445\n",
      "in attepmt 251: loss=2.6620653654631163,best:2.353503288215445\n",
      "in attepmt 252: loss=2.5658097626220453,best:2.353503288215445\n",
      "in attepmt 253: loss=2.49077463662163,best:2.353503288215445\n",
      "in attepmt 254: loss=2.403479613663322,best:2.353503288215445\n",
      "in attepmt 255: loss=2.3362760069113264,best:2.3362760069113264\n",
      "in attepmt 256: loss=2.6409255863723544,best:2.3362760069113264\n",
      "in attepmt 257: loss=2.558425789188815,best:2.3362760069113264\n",
      "in attepmt 258: loss=2.5797483938409536,best:2.3362760069113264\n",
      "in attepmt 259: loss=2.5503000076216553,best:2.3362760069113264\n",
      "in attepmt 260: loss=2.7188879370027563,best:2.3362760069113264\n",
      "in attepmt 261: loss=2.4605285690077743,best:2.3362760069113264\n",
      "in attepmt 262: loss=2.7113452895518684,best:2.3362760069113264\n",
      "in attepmt 263: loss=2.6650176514499955,best:2.3362760069113264\n",
      "in attepmt 264: loss=2.589301287928727,best:2.3362760069113264\n",
      "in attepmt 265: loss=2.47735749356003,best:2.3362760069113264\n",
      "in attepmt 266: loss=2.5727358559173727,best:2.3362760069113264\n",
      "in attepmt 267: loss=2.6958641247707886,best:2.3362760069113264\n",
      "in attepmt 268: loss=2.5389483611601067,best:2.3362760069113264\n",
      "in attepmt 269: loss=2.5667350994382856,best:2.3362760069113264\n",
      "in attepmt 270: loss=2.4426447854938154,best:2.3362760069113264\n",
      "in attepmt 271: loss=2.6931967969957467,best:2.3362760069113264\n",
      "in attepmt 272: loss=2.4978188553451304,best:2.3362760069113264\n",
      "in attepmt 273: loss=2.542357592675264,best:2.3362760069113264\n",
      "in attepmt 274: loss=2.5033027664236456,best:2.3362760069113264\n",
      "in attepmt 275: loss=2.485561150110467,best:2.3362760069113264\n",
      "in attepmt 276: loss=2.4966744462356805,best:2.3362760069113264\n",
      "in attepmt 277: loss=2.429996479748045,best:2.3362760069113264\n",
      "in attepmt 278: loss=2.4660108685297395,best:2.3362760069113264\n",
      "in attepmt 279: loss=2.5733269592699135,best:2.3362760069113264\n",
      "in attepmt 280: loss=2.4762201662382264,best:2.3362760069113264\n",
      "in attepmt 281: loss=2.4258854652963615,best:2.3362760069113264\n",
      "in attepmt 282: loss=2.593568320807813,best:2.3362760069113264\n",
      "in attepmt 283: loss=2.618202435625938,best:2.3362760069113264\n",
      "in attepmt 284: loss=2.6389690159679633,best:2.3362760069113264\n",
      "in attepmt 285: loss=2.4844616318980477,best:2.3362760069113264\n",
      "in attepmt 286: loss=2.4361624286513477,best:2.3362760069113264\n",
      "in attepmt 287: loss=2.757612962859892,best:2.3362760069113264\n",
      "in attepmt 288: loss=2.540869675788767,best:2.3362760069113264\n",
      "in attepmt 289: loss=2.4937625516964683,best:2.3362760069113264\n",
      "in attepmt 290: loss=2.5234831980819146,best:2.3362760069113264\n",
      "in attepmt 291: loss=2.5045265308038505,best:2.3362760069113264\n",
      "in attepmt 292: loss=2.673286405871431,best:2.3362760069113264\n",
      "in attepmt 293: loss=2.523272895633639,best:2.3362760069113264\n",
      "in attepmt 294: loss=2.612726075766566,best:2.3362760069113264\n",
      "in attepmt 295: loss=2.5158523368395893,best:2.3362760069113264\n",
      "in attepmt 296: loss=2.4805986551302333,best:2.3362760069113264\n",
      "in attepmt 297: loss=2.544191832287124,best:2.3362760069113264\n",
      "in attepmt 298: loss=2.637957945522159,best:2.3362760069113264\n",
      "in attepmt 299: loss=2.6138705144250722,best:2.3362760069113264\n",
      "in attepmt 300: loss=2.7761866749448485,best:2.3362760069113264\n",
      "in attepmt 301: loss=2.42062261719418,best:2.3362760069113264\n",
      "in attepmt 302: loss=2.7351830102556733,best:2.3362760069113264\n",
      "in attepmt 303: loss=2.469083828839917,best:2.3362760069113264\n",
      "in attepmt 304: loss=2.741007606787656,best:2.3362760069113264\n",
      "in attepmt 305: loss=2.5615014651430825,best:2.3362760069113264\n",
      "in attepmt 306: loss=2.461689905293021,best:2.3362760069113264\n",
      "in attepmt 307: loss=2.6336184447320843,best:2.3362760069113264\n",
      "in attepmt 308: loss=2.4702380274903444,best:2.3362760069113264\n",
      "in attepmt 309: loss=2.4901975530544345,best:2.3362760069113264\n",
      "in attepmt 310: loss=2.4909521220424935,best:2.3362760069113264\n",
      "in attepmt 311: loss=2.738624075569457,best:2.3362760069113264\n",
      "in attepmt 312: loss=2.687668386309744,best:2.3362760069113264\n",
      "in attepmt 313: loss=2.4807402574450785,best:2.3362760069113264\n",
      "in attepmt 314: loss=2.4914423778986747,best:2.3362760069113264\n",
      "in attepmt 315: loss=2.4096735350234026,best:2.3362760069113264\n",
      "in attepmt 316: loss=2.5134170928603314,best:2.3362760069113264\n",
      "in attepmt 317: loss=2.64671702140288,best:2.3362760069113264\n",
      "in attepmt 318: loss=2.878150269860702,best:2.3362760069113264\n",
      "in attepmt 319: loss=2.5957356051279623,best:2.3362760069113264\n",
      "in attepmt 320: loss=2.495029113832611,best:2.3362760069113264\n",
      "in attepmt 321: loss=2.604280875839376,best:2.3362760069113264\n",
      "in attepmt 322: loss=2.518955351676892,best:2.3362760069113264\n",
      "in attepmt 323: loss=2.560687727412056,best:2.3362760069113264\n",
      "in attepmt 324: loss=2.43040488756191,best:2.3362760069113264\n",
      "in attepmt 325: loss=2.473649088257017,best:2.3362760069113264\n",
      "in attepmt 326: loss=2.454091632272685,best:2.3362760069113264\n",
      "in attepmt 327: loss=2.420238612942268,best:2.3362760069113264\n",
      "in attepmt 328: loss=2.5899455074469513,best:2.3362760069113264\n",
      "in attepmt 329: loss=2.5673551302383286,best:2.3362760069113264\n",
      "in attepmt 330: loss=2.485485662166159,best:2.3362760069113264\n",
      "in attepmt 331: loss=2.5138203136826385,best:2.3362760069113264\n",
      "in attepmt 332: loss=2.387656077619135,best:2.3362760069113264\n",
      "in attepmt 333: loss=2.5555898645836916,best:2.3362760069113264\n",
      "in attepmt 334: loss=2.4644917770028645,best:2.3362760069113264\n",
      "in attepmt 335: loss=2.458598007393336,best:2.3362760069113264\n",
      "in attepmt 336: loss=2.5433007170711064,best:2.3362760069113264\n",
      "in attepmt 337: loss=2.488758500033327,best:2.3362760069113264\n",
      "in attepmt 338: loss=2.42528642901793,best:2.3362760069113264\n",
      "in attepmt 339: loss=2.507375226769222,best:2.3362760069113264\n",
      "in attepmt 340: loss=2.7954930168219323,best:2.3362760069113264\n",
      "in attepmt 341: loss=2.6837593309571472,best:2.3362760069113264\n",
      "in attepmt 342: loss=2.588549171021357,best:2.3362760069113264\n",
      "in attepmt 343: loss=2.4947659106121773,best:2.3362760069113264\n",
      "in attepmt 344: loss=2.507134422932286,best:2.3362760069113264\n",
      "in attepmt 345: loss=2.4340551793453784,best:2.3362760069113264\n",
      "in attepmt 346: loss=2.4363092813055602,best:2.3362760069113264\n",
      "in attepmt 347: loss=2.4909553919965433,best:2.3362760069113264\n",
      "in attepmt 348: loss=2.5317497062550944,best:2.3362760069113264\n",
      "in attepmt 349: loss=2.5644106715159665,best:2.3362760069113264\n",
      "in attepmt 350: loss=2.427785777084403,best:2.3362760069113264\n",
      "in attepmt 351: loss=2.5850299899294016,best:2.3362760069113264\n",
      "in attepmt 352: loss=2.52818855350315,best:2.3362760069113264\n",
      "in attepmt 353: loss=2.616589665011862,best:2.3362760069113264\n",
      "in attepmt 354: loss=2.4431440600162015,best:2.3362760069113264\n",
      "in attepmt 355: loss=2.4093822317593063,best:2.3362760069113264\n",
      "in attepmt 356: loss=2.5419557481237103,best:2.3362760069113264\n",
      "in attepmt 357: loss=2.502304941120312,best:2.3362760069113264\n",
      "in attepmt 358: loss=2.4763519888499195,best:2.3362760069113264\n",
      "in attepmt 359: loss=2.4095171754404916,best:2.3362760069113264\n",
      "in attepmt 360: loss=2.5360081764493034,best:2.3362760069113264\n",
      "in attepmt 361: loss=2.606348668253116,best:2.3362760069113264\n",
      "in attepmt 362: loss=2.4397412724838645,best:2.3362760069113264\n",
      "in attepmt 363: loss=2.5131883155860377,best:2.3362760069113264\n",
      "in attepmt 364: loss=2.4580418593306224,best:2.3362760069113264\n",
      "in attepmt 365: loss=2.7077027676269134,best:2.3362760069113264\n",
      "in attepmt 366: loss=2.7230084553257097,best:2.3362760069113264\n",
      "in attepmt 367: loss=2.6853779307268617,best:2.3362760069113264\n",
      "in attepmt 368: loss=2.367461701666501,best:2.3362760069113264\n",
      "in attepmt 369: loss=2.649002752341157,best:2.3362760069113264\n",
      "in attepmt 370: loss=2.5336736850698065,best:2.3362760069113264\n",
      "in attepmt 371: loss=2.4884876653142767,best:2.3362760069113264\n",
      "in attepmt 372: loss=2.581416900917643,best:2.3362760069113264\n",
      "in attepmt 373: loss=2.500691603474129,best:2.3362760069113264\n",
      "in attepmt 374: loss=2.5202697513098795,best:2.3362760069113264\n",
      "in attepmt 375: loss=2.505255854063628,best:2.3362760069113264\n",
      "in attepmt 376: loss=2.531894330412374,best:2.3362760069113264\n",
      "in attepmt 377: loss=2.507158857607837,best:2.3362760069113264\n",
      "in attepmt 378: loss=2.563677973181167,best:2.3362760069113264\n",
      "in attepmt 379: loss=2.4003461604502703,best:2.3362760069113264\n",
      "in attepmt 380: loss=2.568365391457049,best:2.3362760069113264\n",
      "in attepmt 381: loss=2.6496212686781173,best:2.3362760069113264\n",
      "in attepmt 382: loss=2.6953658426102565,best:2.3362760069113264\n",
      "in attepmt 383: loss=2.499427798868315,best:2.3362760069113264\n",
      "in attepmt 384: loss=2.694131452927115,best:2.3362760069113264\n",
      "in attepmt 385: loss=2.581957701726825,best:2.3362760069113264\n",
      "in attepmt 386: loss=2.53439238851917,best:2.3362760069113264\n",
      "in attepmt 387: loss=2.785719431334032,best:2.3362760069113264\n",
      "in attepmt 388: loss=2.393199089958045,best:2.3362760069113264\n",
      "in attepmt 389: loss=2.4365268171909804,best:2.3362760069113264\n",
      "in attepmt 390: loss=2.666450582992093,best:2.3362760069113264\n",
      "in attepmt 391: loss=2.592114189041158,best:2.3362760069113264\n",
      "in attepmt 392: loss=2.44067663253941,best:2.3362760069113264\n",
      "in attepmt 393: loss=2.4830176421112484,best:2.3362760069113264\n",
      "in attepmt 394: loss=2.405102457920121,best:2.3362760069113264\n",
      "in attepmt 395: loss=2.591162519107149,best:2.3362760069113264\n",
      "in attepmt 396: loss=2.7201873648509443,best:2.3362760069113264\n",
      "in attepmt 397: loss=2.5202301882269182,best:2.3362760069113264\n",
      "in attepmt 398: loss=2.4919302150215334,best:2.3362760069113264\n",
      "in attepmt 399: loss=2.437288894097198,best:2.3362760069113264\n",
      "in attepmt 400: loss=2.503240413483103,best:2.3362760069113264\n",
      "in attepmt 401: loss=2.4718193515476488,best:2.3362760069113264\n",
      "in attepmt 402: loss=2.629789750000733,best:2.3362760069113264\n",
      "in attepmt 403: loss=2.574786195865424,best:2.3362760069113264\n",
      "in attepmt 404: loss=2.536563738922726,best:2.3362760069113264\n",
      "in attepmt 405: loss=2.5379138291867225,best:2.3362760069113264\n",
      "in attepmt 406: loss=2.517520824830557,best:2.3362760069113264\n",
      "in attepmt 407: loss=2.9666187257778085,best:2.3362760069113264\n",
      "in attepmt 408: loss=2.6768433728124887,best:2.3362760069113264\n",
      "in attepmt 409: loss=2.658807956583483,best:2.3362760069113264\n",
      "in attepmt 410: loss=2.498331231125885,best:2.3362760069113264\n",
      "in attepmt 411: loss=2.6085417590276623,best:2.3362760069113264\n",
      "in attepmt 412: loss=2.5733584315580265,best:2.3362760069113264\n",
      "in attepmt 413: loss=2.684279297691666,best:2.3362760069113264\n",
      "in attepmt 414: loss=2.4757179458321907,best:2.3362760069113264\n",
      "in attepmt 415: loss=2.4510501545429855,best:2.3362760069113264\n",
      "in attepmt 416: loss=2.6798622612676,best:2.3362760069113264\n",
      "in attepmt 417: loss=2.4752178754926297,best:2.3362760069113264\n",
      "in attepmt 418: loss=2.477003796996565,best:2.3362760069113264\n",
      "in attepmt 419: loss=2.487233407301098,best:2.3362760069113264\n",
      "in attepmt 420: loss=2.5584135155130263,best:2.3362760069113264\n",
      "in attepmt 421: loss=2.558615918841528,best:2.3362760069113264\n",
      "in attepmt 422: loss=2.5913462406270757,best:2.3362760069113264\n",
      "in attepmt 423: loss=2.428494852974889,best:2.3362760069113264\n",
      "in attepmt 424: loss=2.558465004903798,best:2.3362760069113264\n",
      "in attepmt 425: loss=2.509865432353697,best:2.3362760069113264\n",
      "in attepmt 426: loss=2.4860961116242244,best:2.3362760069113264\n",
      "in attepmt 427: loss=2.6734345226033183,best:2.3362760069113264\n",
      "in attepmt 428: loss=2.5984910108117862,best:2.3362760069113264\n",
      "in attepmt 429: loss=2.6100062826823534,best:2.3362760069113264\n",
      "in attepmt 430: loss=2.574658081246036,best:2.3362760069113264\n",
      "in attepmt 431: loss=2.412674619600608,best:2.3362760069113264\n",
      "in attepmt 432: loss=2.7982782820891146,best:2.3362760069113264\n",
      "in attepmt 433: loss=2.4544757143534675,best:2.3362760069113264\n",
      "in attepmt 434: loss=2.4382966477083547,best:2.3362760069113264\n",
      "in attepmt 435: loss=2.463923160754894,best:2.3362760069113264\n",
      "in attepmt 436: loss=2.5035917734328366,best:2.3362760069113264\n",
      "in attepmt 437: loss=2.5015570750889964,best:2.3362760069113264\n",
      "in attepmt 438: loss=2.5572740294655674,best:2.3362760069113264\n",
      "in attepmt 439: loss=2.454477154719411,best:2.3362760069113264\n",
      "in attepmt 440: loss=2.6683055390588257,best:2.3362760069113264\n",
      "in attepmt 441: loss=2.618941781669805,best:2.3362760069113264\n",
      "in attepmt 442: loss=2.528781309101492,best:2.3362760069113264\n",
      "in attepmt 443: loss=2.734875467716591,best:2.3362760069113264\n",
      "in attepmt 444: loss=2.5760542803434565,best:2.3362760069113264\n",
      "in attepmt 445: loss=2.6425708696039534,best:2.3362760069113264\n",
      "in attepmt 446: loss=2.7323032261121494,best:2.3362760069113264\n",
      "in attepmt 447: loss=2.7241893502164247,best:2.3362760069113264\n",
      "in attepmt 448: loss=2.4078532572280564,best:2.3362760069113264\n",
      "in attepmt 449: loss=2.5357900540632103,best:2.3362760069113264\n",
      "in attepmt 450: loss=2.5486652625758577,best:2.3362760069113264\n",
      "in attepmt 451: loss=2.5170055516236984,best:2.3362760069113264\n",
      "in attepmt 452: loss=2.574905557558918,best:2.3362760069113264\n",
      "in attepmt 453: loss=2.508017314814218,best:2.3362760069113264\n",
      "in attepmt 454: loss=2.5755803712301275,best:2.3362760069113264\n",
      "in attepmt 455: loss=2.4987656235667948,best:2.3362760069113264\n",
      "in attepmt 456: loss=2.6631950821694197,best:2.3362760069113264\n",
      "in attepmt 457: loss=2.6644658163321977,best:2.3362760069113264\n",
      "in attepmt 458: loss=2.636167307972535,best:2.3362760069113264\n",
      "in attepmt 459: loss=2.67205915504699,best:2.3362760069113264\n",
      "in attepmt 460: loss=2.4845115578669223,best:2.3362760069113264\n",
      "in attepmt 461: loss=2.502650165114442,best:2.3362760069113264\n",
      "in attepmt 462: loss=2.4787868246377465,best:2.3362760069113264\n",
      "in attepmt 463: loss=2.4409567173418054,best:2.3362760069113264\n",
      "in attepmt 464: loss=2.501210131258695,best:2.3362760069113264\n",
      "in attepmt 465: loss=2.557077333276994,best:2.3362760069113264\n",
      "in attepmt 466: loss=2.490472359709087,best:2.3362760069113264\n",
      "in attepmt 467: loss=2.7668882161633133,best:2.3362760069113264\n",
      "in attepmt 468: loss=2.5436276496511905,best:2.3362760069113264\n",
      "in attepmt 469: loss=2.559693076859939,best:2.3362760069113264\n",
      "in attepmt 470: loss=2.457566703633322,best:2.3362760069113264\n",
      "in attepmt 471: loss=2.4812076918524477,best:2.3362760069113264\n",
      "in attepmt 472: loss=2.584507756608873,best:2.3362760069113264\n",
      "in attepmt 473: loss=2.6784926700327807,best:2.3362760069113264\n",
      "in attepmt 474: loss=2.5451203011204706,best:2.3362760069113264\n",
      "in attepmt 475: loss=2.6290139924463793,best:2.3362760069113264\n",
      "in attepmt 476: loss=2.6447174958927913,best:2.3362760069113264\n",
      "in attepmt 477: loss=2.645564673039552,best:2.3362760069113264\n",
      "in attepmt 478: loss=2.5001268384967665,best:2.3362760069113264\n",
      "in attepmt 479: loss=2.454005250913091,best:2.3362760069113264\n",
      "in attepmt 480: loss=2.5667322291571533,best:2.3362760069113264\n",
      "in attepmt 481: loss=2.6708624063369935,best:2.3362760069113264\n",
      "in attepmt 482: loss=2.4448677626307087,best:2.3362760069113264\n",
      "in attepmt 483: loss=2.3935064948652247,best:2.3362760069113264\n",
      "in attepmt 484: loss=2.527214433592032,best:2.3362760069113264\n",
      "in attepmt 485: loss=2.7135110974895187,best:2.3362760069113264\n",
      "in attepmt 486: loss=2.440086517524825,best:2.3362760069113264\n",
      "in attepmt 487: loss=2.5833291189813496,best:2.3362760069113264\n",
      "in attepmt 488: loss=2.548352979859755,best:2.3362760069113264\n",
      "in attepmt 489: loss=2.496844270165908,best:2.3362760069113264\n",
      "in attepmt 490: loss=2.540627722298232,best:2.3362760069113264\n",
      "in attepmt 491: loss=2.516514154099311,best:2.3362760069113264\n",
      "in attepmt 492: loss=2.4998450992894874,best:2.3362760069113264\n",
      "in attepmt 493: loss=2.6146887054943893,best:2.3362760069113264\n",
      "in attepmt 494: loss=2.5103175986628643,best:2.3362760069113264\n",
      "in attepmt 495: loss=2.5450361628401605,best:2.3362760069113264\n",
      "in attepmt 496: loss=2.5319938322551447,best:2.3362760069113264\n",
      "in attepmt 497: loss=2.528751562229757,best:2.3362760069113264\n",
      "in attepmt 498: loss=2.5831791915979108,best:2.3362760069113264\n",
      "in attepmt 499: loss=2.5614499347282043,best:2.3362760069113264\n",
      "in attepmt 500: loss=2.4009938180518744,best:2.3362760069113264\n",
      "in attepmt 501: loss=2.537862491022525,best:2.3362760069113264\n",
      "in attepmt 502: loss=2.5244656608931666,best:2.3362760069113264\n",
      "in attepmt 503: loss=2.5402317903329523,best:2.3362760069113264\n",
      "in attepmt 504: loss=2.522472019384831,best:2.3362760069113264\n",
      "in attepmt 505: loss=2.658010648561236,best:2.3362760069113264\n",
      "in attepmt 506: loss=2.6101195700613378,best:2.3362760069113264\n",
      "in attepmt 507: loss=2.448198913185839,best:2.3362760069113264\n",
      "in attepmt 508: loss=2.5166144204092187,best:2.3362760069113264\n",
      "in attepmt 509: loss=2.627783377697303,best:2.3362760069113264\n",
      "in attepmt 510: loss=2.6238327020318994,best:2.3362760069113264\n",
      "in attepmt 511: loss=2.391331717519759,best:2.3362760069113264\n",
      "in attepmt 512: loss=2.5070282552838936,best:2.3362760069113264\n",
      "in attepmt 513: loss=2.6476370874929342,best:2.3362760069113264\n",
      "in attepmt 514: loss=2.5982921012727624,best:2.3362760069113264\n",
      "in attepmt 515: loss=2.791693777162037,best:2.3362760069113264\n",
      "in attepmt 516: loss=2.5184715605484,best:2.3362760069113264\n",
      "in attepmt 517: loss=2.639427389465321,best:2.3362760069113264\n",
      "in attepmt 518: loss=2.5721143841200385,best:2.3362760069113264\n",
      "in attepmt 519: loss=2.707542189125357,best:2.3362760069113264\n",
      "in attepmt 520: loss=2.5092773271012505,best:2.3362760069113264\n",
      "in attepmt 521: loss=2.4893355418164695,best:2.3362760069113264\n",
      "in attepmt 522: loss=2.555528606247826,best:2.3362760069113264\n",
      "in attepmt 523: loss=2.546463038239527,best:2.3362760069113264\n",
      "in attepmt 524: loss=2.520852467014533,best:2.3362760069113264\n",
      "in attepmt 525: loss=2.542069203313582,best:2.3362760069113264\n",
      "in attepmt 526: loss=2.4814460831157854,best:2.3362760069113264\n",
      "in attepmt 527: loss=2.692145034858206,best:2.3362760069113264\n",
      "in attepmt 528: loss=2.6087344569074027,best:2.3362760069113264\n",
      "in attepmt 529: loss=2.5290554643675347,best:2.3362760069113264\n",
      "in attepmt 530: loss=2.4084928361790725,best:2.3362760069113264\n",
      "in attepmt 531: loss=2.6518692528254904,best:2.3362760069113264\n",
      "in attepmt 532: loss=2.5423856410081003,best:2.3362760069113264\n",
      "in attepmt 533: loss=2.4693199660006124,best:2.3362760069113264\n",
      "in attepmt 534: loss=2.7370282693993526,best:2.3362760069113264\n",
      "in attepmt 535: loss=2.7162824073913288,best:2.3362760069113264\n",
      "in attepmt 536: loss=2.5386990911486813,best:2.3362760069113264\n",
      "in attepmt 537: loss=2.422481749395564,best:2.3362760069113264\n",
      "in attepmt 538: loss=2.4997691651198743,best:2.3362760069113264\n",
      "in attepmt 539: loss=2.580352337454723,best:2.3362760069113264\n",
      "in attepmt 540: loss=2.5887396450138977,best:2.3362760069113264\n",
      "in attepmt 541: loss=2.4441675071274904,best:2.3362760069113264\n",
      "in attepmt 542: loss=2.4688916175554367,best:2.3362760069113264\n",
      "in attepmt 543: loss=2.5775142062857475,best:2.3362760069113264\n",
      "in attepmt 544: loss=2.480919215971497,best:2.3362760069113264\n",
      "in attepmt 545: loss=2.9178963479616633,best:2.3362760069113264\n",
      "in attepmt 546: loss=2.556062656160704,best:2.3362760069113264\n",
      "in attepmt 547: loss=2.3646794071645076,best:2.3362760069113264\n",
      "in attepmt 548: loss=2.5246536638558865,best:2.3362760069113264\n",
      "in attepmt 549: loss=2.4090538681093348,best:2.3362760069113264\n",
      "in attepmt 550: loss=2.6373318416553304,best:2.3362760069113264\n",
      "in attepmt 551: loss=2.4407409484149287,best:2.3362760069113264\n",
      "in attepmt 552: loss=2.5028469447887045,best:2.3362760069113264\n",
      "in attepmt 553: loss=2.5415176845783933,best:2.3362760069113264\n",
      "in attepmt 554: loss=2.5756853460186884,best:2.3362760069113264\n",
      "in attepmt 555: loss=2.599664146336403,best:2.3362760069113264\n",
      "in attepmt 556: loss=2.54491810929549,best:2.3362760069113264\n",
      "in attepmt 557: loss=2.6126307280296936,best:2.3362760069113264\n",
      "in attepmt 558: loss=2.542312523258231,best:2.3362760069113264\n",
      "in attepmt 559: loss=2.438196615935477,best:2.3362760069113264\n",
      "in attepmt 560: loss=2.5959599043996766,best:2.3362760069113264\n",
      "in attepmt 561: loss=2.3988945540501225,best:2.3362760069113264\n",
      "in attepmt 562: loss=2.433436518009296,best:2.3362760069113264\n",
      "in attepmt 563: loss=2.5659554701941993,best:2.3362760069113264\n",
      "in attepmt 564: loss=2.4907953649099364,best:2.3362760069113264\n",
      "in attepmt 565: loss=2.527480280814587,best:2.3362760069113264\n",
      "in attepmt 566: loss=2.430338539470622,best:2.3362760069113264\n",
      "in attepmt 567: loss=2.597250416884426,best:2.3362760069113264\n",
      "in attepmt 568: loss=2.4631500216280293,best:2.3362760069113264\n",
      "in attepmt 569: loss=2.6291802593583586,best:2.3362760069113264\n",
      "in attepmt 570: loss=2.470748121926196,best:2.3362760069113264\n",
      "in attepmt 571: loss=2.5954614783964582,best:2.3362760069113264\n",
      "in attepmt 572: loss=2.52974729119941,best:2.3362760069113264\n",
      "in attepmt 573: loss=2.4384789815620778,best:2.3362760069113264\n",
      "in attepmt 574: loss=2.42122790218912,best:2.3362760069113264\n",
      "in attepmt 575: loss=2.4152878128978856,best:2.3362760069113264\n",
      "in attepmt 576: loss=2.6415691403696897,best:2.3362760069113264\n",
      "in attepmt 577: loss=2.5648156040307275,best:2.3362760069113264\n",
      "in attepmt 578: loss=2.6201622386633137,best:2.3362760069113264\n",
      "in attepmt 579: loss=2.5479911382315192,best:2.3362760069113264\n",
      "in attepmt 580: loss=2.5495169832785076,best:2.3362760069113264\n",
      "in attepmt 581: loss=2.5888626909486105,best:2.3362760069113264\n",
      "in attepmt 582: loss=2.494263642783868,best:2.3362760069113264\n",
      "in attepmt 583: loss=2.519087599401887,best:2.3362760069113264\n",
      "in attepmt 584: loss=2.5130106106531436,best:2.3362760069113264\n",
      "in attepmt 585: loss=2.541784878316038,best:2.3362760069113264\n",
      "in attepmt 586: loss=2.5479261333188665,best:2.3362760069113264\n",
      "in attepmt 587: loss=2.5549064501602574,best:2.3362760069113264\n",
      "in attepmt 588: loss=2.814749444998385,best:2.3362760069113264\n",
      "in attepmt 589: loss=2.536435430580338,best:2.3362760069113264\n",
      "in attepmt 590: loss=2.6582085684445533,best:2.3362760069113264\n",
      "in attepmt 591: loss=2.4635234227710896,best:2.3362760069113264\n",
      "in attepmt 592: loss=2.481430982048375,best:2.3362760069113264\n",
      "in attepmt 593: loss=2.662081680235809,best:2.3362760069113264\n",
      "in attepmt 594: loss=2.6101874371935456,best:2.3362760069113264\n",
      "in attepmt 595: loss=2.463742633793664,best:2.3362760069113264\n",
      "in attepmt 596: loss=2.537960532025404,best:2.3362760069113264\n",
      "in attepmt 597: loss=2.443959017844871,best:2.3362760069113264\n",
      "in attepmt 598: loss=2.5296351608926426,best:2.3362760069113264\n",
      "in attepmt 599: loss=2.62151115629911,best:2.3362760069113264\n",
      "in attepmt 600: loss=2.7498509303214647,best:2.3362760069113264\n",
      "in attepmt 601: loss=2.416829838813624,best:2.3362760069113264\n",
      "in attepmt 602: loss=2.6469481211826347,best:2.3362760069113264\n",
      "in attepmt 603: loss=2.5744108099201486,best:2.3362760069113264\n",
      "in attepmt 604: loss=2.501415714493755,best:2.3362760069113264\n",
      "in attepmt 605: loss=2.6458097749483502,best:2.3362760069113264\n",
      "in attepmt 606: loss=2.865909839264505,best:2.3362760069113264\n",
      "in attepmt 607: loss=2.5636599881150204,best:2.3362760069113264\n",
      "in attepmt 608: loss=2.452475420856398,best:2.3362760069113264\n",
      "in attepmt 609: loss=2.702389425486701,best:2.3362760069113264\n",
      "in attepmt 610: loss=2.469107617078106,best:2.3362760069113264\n",
      "in attepmt 611: loss=2.7933932689569727,best:2.3362760069113264\n",
      "in attepmt 612: loss=2.4824644871704695,best:2.3362760069113264\n",
      "in attepmt 613: loss=2.6189488839271715,best:2.3362760069113264\n",
      "in attepmt 614: loss=2.5149030543235757,best:2.3362760069113264\n",
      "in attepmt 615: loss=2.4292576827364636,best:2.3362760069113264\n",
      "in attepmt 616: loss=2.596257069962958,best:2.3362760069113264\n",
      "in attepmt 617: loss=2.495787402880571,best:2.3362760069113264\n",
      "in attepmt 618: loss=2.7630349896004023,best:2.3362760069113264\n",
      "in attepmt 619: loss=2.7080956181434845,best:2.3362760069113264\n",
      "in attepmt 620: loss=2.8502225324595942,best:2.3362760069113264\n",
      "in attepmt 621: loss=2.5639847192536642,best:2.3362760069113264\n",
      "in attepmt 622: loss=2.494951774976188,best:2.3362760069113264\n",
      "in attepmt 623: loss=2.6388586810517958,best:2.3362760069113264\n",
      "in attepmt 624: loss=2.485918614224742,best:2.3362760069113264\n",
      "in attepmt 625: loss=2.6944033964984158,best:2.3362760069113264\n",
      "in attepmt 626: loss=2.5206415108314495,best:2.3362760069113264\n",
      "in attepmt 627: loss=2.498810460931905,best:2.3362760069113264\n",
      "in attepmt 628: loss=2.430863351472591,best:2.3362760069113264\n",
      "in attepmt 629: loss=2.492082460703789,best:2.3362760069113264\n",
      "in attepmt 630: loss=2.3837451581934896,best:2.3362760069113264\n",
      "in attepmt 631: loss=2.6733440313462213,best:2.3362760069113264\n",
      "in attepmt 632: loss=2.614536457054476,best:2.3362760069113264\n",
      "in attepmt 633: loss=2.6726865713003516,best:2.3362760069113264\n",
      "in attepmt 634: loss=2.6457434627230447,best:2.3362760069113264\n",
      "in attepmt 635: loss=2.412286863863343,best:2.3362760069113264\n",
      "in attepmt 636: loss=2.5409172496420513,best:2.3362760069113264\n",
      "in attepmt 637: loss=2.382895425716793,best:2.3362760069113264\n",
      "in attepmt 638: loss=2.5716834393714714,best:2.3362760069113264\n",
      "in attepmt 639: loss=2.5197476024585814,best:2.3362760069113264\n",
      "in attepmt 640: loss=2.624882488748301,best:2.3362760069113264\n",
      "in attepmt 641: loss=2.578483434647936,best:2.3362760069113264\n",
      "in attepmt 642: loss=2.4535550083512456,best:2.3362760069113264\n",
      "in attepmt 643: loss=2.5568035176829946,best:2.3362760069113264\n",
      "in attepmt 644: loss=2.603588417575139,best:2.3362760069113264\n",
      "in attepmt 645: loss=2.5624630214792474,best:2.3362760069113264\n",
      "in attepmt 646: loss=2.5552162669258482,best:2.3362760069113264\n",
      "in attepmt 647: loss=2.554696830235207,best:2.3362760069113264\n",
      "in attepmt 648: loss=2.444023326805101,best:2.3362760069113264\n",
      "in attepmt 649: loss=2.553382991725979,best:2.3362760069113264\n",
      "in attepmt 650: loss=2.692583413110787,best:2.3362760069113264\n",
      "in attepmt 651: loss=2.4533978470596103,best:2.3362760069113264\n",
      "in attepmt 652: loss=2.499451259311181,best:2.3362760069113264\n",
      "in attepmt 653: loss=2.9792870650626826,best:2.3362760069113264\n",
      "in attepmt 654: loss=2.6670669451397755,best:2.3362760069113264\n",
      "in attepmt 655: loss=2.4568544832018095,best:2.3362760069113264\n",
      "in attepmt 656: loss=2.7218702694931958,best:2.3362760069113264\n",
      "in attepmt 657: loss=2.5991188467723516,best:2.3362760069113264\n",
      "in attepmt 658: loss=2.572691604875752,best:2.3362760069113264\n",
      "in attepmt 659: loss=2.5268168904383743,best:2.3362760069113264\n",
      "in attepmt 660: loss=2.471806205618888,best:2.3362760069113264\n",
      "in attepmt 661: loss=2.738512388916628,best:2.3362760069113264\n",
      "in attepmt 662: loss=2.5641028244687067,best:2.3362760069113264\n",
      "in attepmt 663: loss=2.5421163142023566,best:2.3362760069113264\n",
      "in attepmt 664: loss=2.6178485349047875,best:2.3362760069113264\n",
      "in attepmt 665: loss=2.590154212411336,best:2.3362760069113264\n",
      "in attepmt 666: loss=2.641331689134931,best:2.3362760069113264\n",
      "in attepmt 667: loss=2.636393324289422,best:2.3362760069113264\n",
      "in attepmt 668: loss=2.771484991173916,best:2.3362760069113264\n",
      "in attepmt 669: loss=2.689530401653085,best:2.3362760069113264\n",
      "in attepmt 670: loss=2.475896516394637,best:2.3362760069113264\n",
      "in attepmt 671: loss=2.5345932624251613,best:2.3362760069113264\n",
      "in attepmt 672: loss=2.5100218175845566,best:2.3362760069113264\n",
      "in attepmt 673: loss=2.6482645047592626,best:2.3362760069113264\n",
      "in attepmt 674: loss=2.5552373299766415,best:2.3362760069113264\n",
      "in attepmt 675: loss=2.4031178170956813,best:2.3362760069113264\n",
      "in attepmt 676: loss=2.4368606271469924,best:2.3362760069113264\n",
      "in attepmt 677: loss=2.3757508224890196,best:2.3362760069113264\n",
      "in attepmt 678: loss=2.6187351216201478,best:2.3362760069113264\n",
      "in attepmt 679: loss=2.530493916397902,best:2.3362760069113264\n",
      "in attepmt 680: loss=2.5912385391257935,best:2.3362760069113264\n",
      "in attepmt 681: loss=2.435973171261719,best:2.3362760069113264\n",
      "in attepmt 682: loss=2.6182274857650736,best:2.3362760069113264\n",
      "in attepmt 683: loss=2.617307456298212,best:2.3362760069113264\n",
      "in attepmt 684: loss=2.4061225377380397,best:2.3362760069113264\n",
      "in attepmt 685: loss=2.51585444332615,best:2.3362760069113264\n",
      "in attepmt 686: loss=2.507582551004332,best:2.3362760069113264\n",
      "in attepmt 687: loss=2.745994441560142,best:2.3362760069113264\n",
      "in attepmt 688: loss=2.5894045071882705,best:2.3362760069113264\n",
      "in attepmt 689: loss=2.467615599289187,best:2.3362760069113264\n",
      "in attepmt 690: loss=2.5716511966947118,best:2.3362760069113264\n",
      "in attepmt 691: loss=2.6247387258653414,best:2.3362760069113264\n",
      "in attepmt 692: loss=2.5439677660779516,best:2.3362760069113264\n",
      "in attepmt 693: loss=2.4229846806778914,best:2.3362760069113264\n",
      "in attepmt 694: loss=2.845260813303736,best:2.3362760069113264\n",
      "in attepmt 695: loss=2.5404124425495693,best:2.3362760069113264\n",
      "in attepmt 696: loss=2.4388412471286087,best:2.3362760069113264\n",
      "in attepmt 697: loss=2.7387074907715805,best:2.3362760069113264\n",
      "in attepmt 698: loss=2.6043210478679453,best:2.3362760069113264\n",
      "in attepmt 699: loss=2.4035372350858695,best:2.3362760069113264\n",
      "in attepmt 700: loss=2.5461882727727225,best:2.3362760069113264\n",
      "in attepmt 701: loss=2.6338161759488123,best:2.3362760069113264\n",
      "in attepmt 702: loss=2.5343972056586983,best:2.3362760069113264\n",
      "in attepmt 703: loss=2.470887447573159,best:2.3362760069113264\n",
      "in attepmt 704: loss=2.4517301496887574,best:2.3362760069113264\n",
      "in attepmt 705: loss=2.7700748238990536,best:2.3362760069113264\n",
      "in attepmt 706: loss=2.5890444365500245,best:2.3362760069113264\n",
      "in attepmt 707: loss=2.4887470670247196,best:2.3362760069113264\n",
      "in attepmt 708: loss=2.6825555534721413,best:2.3362760069113264\n",
      "in attepmt 709: loss=2.375896396366607,best:2.3362760069113264\n",
      "in attepmt 710: loss=2.685615880307008,best:2.3362760069113264\n",
      "in attepmt 711: loss=2.5078092147454223,best:2.3362760069113264\n",
      "in attepmt 712: loss=2.49810202940169,best:2.3362760069113264\n",
      "in attepmt 713: loss=2.6179917607202197,best:2.3362760069113264\n",
      "in attepmt 714: loss=2.488010525597679,best:2.3362760069113264\n",
      "in attepmt 715: loss=2.668515950321266,best:2.3362760069113264\n",
      "in attepmt 716: loss=2.626849281956623,best:2.3362760069113264\n",
      "in attepmt 717: loss=2.694215218100633,best:2.3362760069113264\n",
      "in attepmt 718: loss=2.872576008959057,best:2.3362760069113264\n",
      "in attepmt 719: loss=2.701370072718422,best:2.3362760069113264\n",
      "in attepmt 720: loss=2.6134508391763336,best:2.3362760069113264\n",
      "in attepmt 721: loss=2.624959178262497,best:2.3362760069113264\n",
      "in attepmt 722: loss=2.4961316291677242,best:2.3362760069113264\n",
      "in attepmt 723: loss=2.62142787120135,best:2.3362760069113264\n",
      "in attepmt 724: loss=2.5398585825424416,best:2.3362760069113264\n",
      "in attepmt 725: loss=2.494637870295623,best:2.3362760069113264\n",
      "in attepmt 726: loss=2.494827365432981,best:2.3362760069113264\n",
      "in attepmt 727: loss=2.658158910234438,best:2.3362760069113264\n",
      "in attepmt 728: loss=2.680566724419173,best:2.3362760069113264\n",
      "in attepmt 729: loss=2.5829489033983033,best:2.3362760069113264\n",
      "in attepmt 730: loss=2.5126266393180496,best:2.3362760069113264\n",
      "in attepmt 731: loss=2.5754766112093193,best:2.3362760069113264\n",
      "in attepmt 732: loss=2.42950175068277,best:2.3362760069113264\n",
      "in attepmt 733: loss=2.5190789752338962,best:2.3362760069113264\n",
      "in attepmt 734: loss=2.4964445114455,best:2.3362760069113264\n",
      "in attepmt 735: loss=2.6915674652613197,best:2.3362760069113264\n",
      "in attepmt 736: loss=2.510410229257353,best:2.3362760069113264\n",
      "in attepmt 737: loss=2.561587764590417,best:2.3362760069113264\n",
      "in attepmt 738: loss=2.4917723398169405,best:2.3362760069113264\n",
      "in attepmt 739: loss=2.6559695906066336,best:2.3362760069113264\n",
      "in attepmt 740: loss=2.7157935255851546,best:2.3362760069113264\n",
      "in attepmt 741: loss=2.4212066037533195,best:2.3362760069113264\n",
      "in attepmt 742: loss=2.5334102062762804,best:2.3362760069113264\n",
      "in attepmt 743: loss=2.5060747801587793,best:2.3362760069113264\n",
      "in attepmt 744: loss=2.464098507793642,best:2.3362760069113264\n",
      "in attepmt 745: loss=2.414574253215181,best:2.3362760069113264\n",
      "in attepmt 746: loss=2.6811665625921868,best:2.3362760069113264\n",
      "in attepmt 747: loss=2.3937677777738515,best:2.3362760069113264\n",
      "in attepmt 748: loss=2.4412589167744816,best:2.3362760069113264\n",
      "in attepmt 749: loss=2.463834521942836,best:2.3362760069113264\n",
      "in attepmt 750: loss=2.5082459028486315,best:2.3362760069113264\n",
      "in attepmt 751: loss=2.8385820434201765,best:2.3362760069113264\n",
      "in attepmt 752: loss=2.5925081295337167,best:2.3362760069113264\n",
      "in attepmt 753: loss=2.516787017183288,best:2.3362760069113264\n",
      "in attepmt 754: loss=2.5329516471305564,best:2.3362760069113264\n",
      "in attepmt 755: loss=2.4566318712958797,best:2.3362760069113264\n",
      "in attepmt 756: loss=2.6315223093702906,best:2.3362760069113264\n",
      "in attepmt 757: loss=2.6545514813429745,best:2.3362760069113264\n",
      "in attepmt 758: loss=2.513001225908818,best:2.3362760069113264\n",
      "in attepmt 759: loss=2.540459564277872,best:2.3362760069113264\n",
      "in attepmt 760: loss=2.5246987371534226,best:2.3362760069113264\n",
      "in attepmt 761: loss=2.482869960516862,best:2.3362760069113264\n",
      "in attepmt 762: loss=2.5485939146838823,best:2.3362760069113264\n",
      "in attepmt 763: loss=2.408100460025399,best:2.3362760069113264\n",
      "in attepmt 764: loss=2.6737398378797477,best:2.3362760069113264\n",
      "in attepmt 765: loss=2.8409909288191173,best:2.3362760069113264\n",
      "in attepmt 766: loss=2.474896758496901,best:2.3362760069113264\n",
      "in attepmt 767: loss=2.491842473704863,best:2.3362760069113264\n",
      "in attepmt 768: loss=2.462395188936854,best:2.3362760069113264\n",
      "in attepmt 769: loss=2.4284869611360307,best:2.3362760069113264\n",
      "in attepmt 770: loss=2.5167744105993686,best:2.3362760069113264\n",
      "in attepmt 771: loss=2.6176953993294876,best:2.3362760069113264\n",
      "in attepmt 772: loss=2.451697491724135,best:2.3362760069113264\n",
      "in attepmt 773: loss=2.490056913947183,best:2.3362760069113264\n",
      "in attepmt 774: loss=2.534764424229968,best:2.3362760069113264\n",
      "in attepmt 775: loss=2.4370921102966867,best:2.3362760069113264\n",
      "in attepmt 776: loss=2.630391536783595,best:2.3362760069113264\n",
      "in attepmt 777: loss=2.508781929135255,best:2.3362760069113264\n",
      "in attepmt 778: loss=2.6588537097988305,best:2.3362760069113264\n",
      "in attepmt 779: loss=2.543227754778208,best:2.3362760069113264\n",
      "in attepmt 780: loss=2.3815125896165865,best:2.3362760069113264\n",
      "in attepmt 781: loss=2.545017178600248,best:2.3362760069113264\n",
      "in attepmt 782: loss=2.58166602354117,best:2.3362760069113264\n",
      "in attepmt 783: loss=2.5272212888412238,best:2.3362760069113264\n",
      "in attepmt 784: loss=2.570675699622943,best:2.3362760069113264\n",
      "in attepmt 785: loss=2.460558006976704,best:2.3362760069113264\n",
      "in attepmt 786: loss=2.503537162850156,best:2.3362760069113264\n",
      "in attepmt 787: loss=2.645009969628763,best:2.3362760069113264\n",
      "in attepmt 788: loss=2.6014033541458983,best:2.3362760069113264\n",
      "in attepmt 789: loss=2.6243666783854995,best:2.3362760069113264\n",
      "in attepmt 790: loss=2.400514529173171,best:2.3362760069113264\n",
      "in attepmt 791: loss=2.457249216280405,best:2.3362760069113264\n",
      "in attepmt 792: loss=2.4913908606035524,best:2.3362760069113264\n",
      "in attepmt 793: loss=2.8678089602987296,best:2.3362760069113264\n",
      "in attepmt 794: loss=2.431103551980708,best:2.3362760069113264\n",
      "in attepmt 795: loss=2.498635651956782,best:2.3362760069113264\n",
      "in attepmt 796: loss=2.5627787172716867,best:2.3362760069113264\n",
      "in attepmt 797: loss=2.6730713437039557,best:2.3362760069113264\n",
      "in attepmt 798: loss=2.5581802046967868,best:2.3362760069113264\n",
      "in attepmt 799: loss=2.5566546320993306,best:2.3362760069113264\n",
      "in attepmt 800: loss=2.5265722632846974,best:2.3362760069113264\n",
      "in attepmt 801: loss=2.5610820342866303,best:2.3362760069113264\n",
      "in attepmt 802: loss=2.551344576678891,best:2.3362760069113264\n",
      "in attepmt 803: loss=2.7597945755596522,best:2.3362760069113264\n",
      "in attepmt 804: loss=2.660498112179111,best:2.3362760069113264\n",
      "in attepmt 805: loss=2.592097822415059,best:2.3362760069113264\n",
      "in attepmt 806: loss=2.5706655031285943,best:2.3362760069113264\n",
      "in attepmt 807: loss=2.469267048666671,best:2.3362760069113264\n",
      "in attepmt 808: loss=2.6376399793732452,best:2.3362760069113264\n",
      "in attepmt 809: loss=2.6456328908632196,best:2.3362760069113264\n",
      "in attepmt 810: loss=2.57609341843113,best:2.3362760069113264\n",
      "in attepmt 811: loss=2.5234521205669784,best:2.3362760069113264\n",
      "in attepmt 812: loss=2.737522180616426,best:2.3362760069113264\n",
      "in attepmt 813: loss=2.666051706168955,best:2.3362760069113264\n",
      "in attepmt 814: loss=2.5992407272496725,best:2.3362760069113264\n",
      "in attepmt 815: loss=2.439117465650787,best:2.3362760069113264\n",
      "in attepmt 816: loss=2.430532716110954,best:2.3362760069113264\n",
      "in attepmt 817: loss=2.8462757442346565,best:2.3362760069113264\n",
      "in attepmt 818: loss=2.7221509623733438,best:2.3362760069113264\n",
      "in attepmt 819: loss=2.487046574464961,best:2.3362760069113264\n",
      "in attepmt 820: loss=2.5517598011890303,best:2.3362760069113264\n",
      "in attepmt 821: loss=2.8039495472145086,best:2.3362760069113264\n",
      "in attepmt 822: loss=2.5125026329029616,best:2.3362760069113264\n",
      "in attepmt 823: loss=2.5420506450458618,best:2.3362760069113264\n",
      "in attepmt 824: loss=2.492987659595105,best:2.3362760069113264\n",
      "in attepmt 825: loss=2.4048936103012806,best:2.3362760069113264\n",
      "in attepmt 826: loss=2.618625934375532,best:2.3362760069113264\n",
      "in attepmt 827: loss=2.4728362154346355,best:2.3362760069113264\n",
      "in attepmt 828: loss=2.716343845272961,best:2.3362760069113264\n",
      "in attepmt 829: loss=2.461340530590942,best:2.3362760069113264\n",
      "in attepmt 830: loss=2.471437491363615,best:2.3362760069113264\n",
      "in attepmt 831: loss=2.53590516614358,best:2.3362760069113264\n",
      "in attepmt 832: loss=2.6626488645210173,best:2.3362760069113264\n",
      "in attepmt 833: loss=2.4331080102323486,best:2.3362760069113264\n",
      "in attepmt 834: loss=2.449682618311567,best:2.3362760069113264\n",
      "in attepmt 835: loss=2.5271007568795896,best:2.3362760069113264\n",
      "in attepmt 836: loss=2.581855173096904,best:2.3362760069113264\n",
      "in attepmt 837: loss=2.4699983028799375,best:2.3362760069113264\n",
      "in attepmt 838: loss=2.554887511601529,best:2.3362760069113264\n",
      "in attepmt 839: loss=2.6210669863472744,best:2.3362760069113264\n",
      "in attepmt 840: loss=2.6345040433162072,best:2.3362760069113264\n",
      "in attepmt 841: loss=2.5579128399705766,best:2.3362760069113264\n",
      "in attepmt 842: loss=2.6177754466118004,best:2.3362760069113264\n",
      "in attepmt 843: loss=2.550738259097226,best:2.3362760069113264\n",
      "in attepmt 844: loss=2.5534803577304763,best:2.3362760069113264\n",
      "in attepmt 845: loss=2.5051279747887767,best:2.3362760069113264\n",
      "in attepmt 846: loss=2.5002018967607906,best:2.3362760069113264\n",
      "in attepmt 847: loss=2.5362098088872784,best:2.3362760069113264\n",
      "in attepmt 848: loss=2.6271598522455437,best:2.3362760069113264\n",
      "in attepmt 849: loss=2.6028583658995377,best:2.3362760069113264\n",
      "in attepmt 850: loss=2.65059973817177,best:2.3362760069113264\n",
      "in attepmt 851: loss=2.4853283362143,best:2.3362760069113264\n",
      "in attepmt 852: loss=2.5181788244969385,best:2.3362760069113264\n",
      "in attepmt 853: loss=2.440710841047526,best:2.3362760069113264\n",
      "in attepmt 854: loss=2.4764141613465145,best:2.3362760069113264\n",
      "in attepmt 855: loss=2.4054006686077787,best:2.3362760069113264\n",
      "in attepmt 856: loss=2.4741289593736395,best:2.3362760069113264\n",
      "in attepmt 857: loss=2.568902766915843,best:2.3362760069113264\n",
      "in attepmt 858: loss=2.4853615577920998,best:2.3362760069113264\n",
      "in attepmt 859: loss=2.4592466131670943,best:2.3362760069113264\n",
      "in attepmt 860: loss=2.7241725205030742,best:2.3362760069113264\n",
      "in attepmt 861: loss=2.5314238018433337,best:2.3362760069113264\n",
      "in attepmt 862: loss=2.5858222057974305,best:2.3362760069113264\n",
      "in attepmt 863: loss=2.628760545986678,best:2.3362760069113264\n",
      "in attepmt 864: loss=2.466291042436864,best:2.3362760069113264\n",
      "in attepmt 865: loss=2.6133569258481795,best:2.3362760069113264\n",
      "in attepmt 866: loss=2.4751360054940625,best:2.3362760069113264\n",
      "in attepmt 867: loss=2.5605625148337725,best:2.3362760069113264\n",
      "in attepmt 868: loss=2.5784240433205343,best:2.3362760069113264\n",
      "in attepmt 869: loss=3.036754613508447,best:2.3362760069113264\n",
      "in attepmt 870: loss=2.555311471270904,best:2.3362760069113264\n",
      "in attepmt 871: loss=2.7189446618564164,best:2.3362760069113264\n",
      "in attepmt 872: loss=2.6375508479426526,best:2.3362760069113264\n",
      "in attepmt 873: loss=2.729301351926119,best:2.3362760069113264\n",
      "in attepmt 874: loss=2.917096460348887,best:2.3362760069113264\n",
      "in attepmt 875: loss=2.5506629055503054,best:2.3362760069113264\n",
      "in attepmt 876: loss=2.477711950565178,best:2.3362760069113264\n",
      "in attepmt 877: loss=2.431242431562178,best:2.3362760069113264\n",
      "in attepmt 878: loss=2.4297536348827453,best:2.3362760069113264\n",
      "in attepmt 879: loss=2.4738922177721423,best:2.3362760069113264\n",
      "in attepmt 880: loss=2.4873304324934016,best:2.3362760069113264\n",
      "in attepmt 881: loss=2.487941251024366,best:2.3362760069113264\n",
      "in attepmt 882: loss=2.593491874507572,best:2.3362760069113264\n",
      "in attepmt 883: loss=2.718687047814623,best:2.3362760069113264\n",
      "in attepmt 884: loss=2.446322343903294,best:2.3362760069113264\n",
      "in attepmt 885: loss=2.5904734490951475,best:2.3362760069113264\n",
      "in attepmt 886: loss=2.522377288464132,best:2.3362760069113264\n",
      "in attepmt 887: loss=2.491552672268808,best:2.3362760069113264\n",
      "in attepmt 888: loss=2.4883219871904863,best:2.3362760069113264\n",
      "in attepmt 889: loss=2.5828005543952783,best:2.3362760069113264\n",
      "in attepmt 890: loss=2.5058204255771406,best:2.3362760069113264\n",
      "in attepmt 891: loss=2.6750918025297,best:2.3362760069113264\n",
      "in attepmt 892: loss=2.557619345189538,best:2.3362760069113264\n",
      "in attepmt 893: loss=2.5221708205197637,best:2.3362760069113264\n",
      "in attepmt 894: loss=2.4423954217109674,best:2.3362760069113264\n",
      "in attepmt 895: loss=2.580494852045179,best:2.3362760069113264\n",
      "in attepmt 896: loss=2.4240988807968322,best:2.3362760069113264\n",
      "in attepmt 897: loss=2.633536599367327,best:2.3362760069113264\n",
      "in attepmt 898: loss=2.5505806779441724,best:2.3362760069113264\n",
      "in attepmt 899: loss=2.577297569881008,best:2.3362760069113264\n",
      "in attepmt 900: loss=2.6464532656301927,best:2.3362760069113264\n",
      "in attepmt 901: loss=2.4622107873180714,best:2.3362760069113264\n",
      "in attepmt 902: loss=2.608372334797099,best:2.3362760069113264\n",
      "in attepmt 903: loss=2.4655041774070505,best:2.3362760069113264\n",
      "in attepmt 904: loss=2.4999706500392804,best:2.3362760069113264\n",
      "in attepmt 905: loss=2.36885314587016,best:2.3362760069113264\n",
      "in attepmt 906: loss=2.5613639900296548,best:2.3362760069113264\n",
      "in attepmt 907: loss=2.678978402350843,best:2.3362760069113264\n",
      "in attepmt 908: loss=2.513654776381365,best:2.3362760069113264\n",
      "in attepmt 909: loss=2.4802774535023526,best:2.3362760069113264\n",
      "in attepmt 910: loss=2.461091571958094,best:2.3362760069113264\n",
      "in attepmt 911: loss=2.8514643857647584,best:2.3362760069113264\n",
      "in attepmt 912: loss=2.5384596438062688,best:2.3362760069113264\n",
      "in attepmt 913: loss=2.5419729107468867,best:2.3362760069113264\n",
      "in attepmt 914: loss=2.5846634159931567,best:2.3362760069113264\n",
      "in attepmt 915: loss=2.4350868267716876,best:2.3362760069113264\n",
      "in attepmt 916: loss=2.779111730262148,best:2.3362760069113264\n",
      "in attepmt 917: loss=2.501822753845933,best:2.3362760069113264\n",
      "in attepmt 918: loss=2.650765356769957,best:2.3362760069113264\n",
      "in attepmt 919: loss=2.4741635449089823,best:2.3362760069113264\n",
      "in attepmt 920: loss=2.496130095887074,best:2.3362760069113264\n",
      "in attepmt 921: loss=2.6630123544772704,best:2.3362760069113264\n",
      "in attepmt 922: loss=2.653845190003387,best:2.3362760069113264\n",
      "in attepmt 923: loss=2.485038740401567,best:2.3362760069113264\n",
      "in attepmt 924: loss=2.5209943388905134,best:2.3362760069113264\n",
      "in attepmt 925: loss=2.59696758015021,best:2.3362760069113264\n",
      "in attepmt 926: loss=2.546336936487673,best:2.3362760069113264\n",
      "in attepmt 927: loss=2.442139330454844,best:2.3362760069113264\n",
      "in attepmt 928: loss=2.632494176610805,best:2.3362760069113264\n",
      "in attepmt 929: loss=2.596214836140477,best:2.3362760069113264\n",
      "in attepmt 930: loss=2.4002022027637415,best:2.3362760069113264\n",
      "in attepmt 931: loss=2.412682376092292,best:2.3362760069113264\n",
      "in attepmt 932: loss=2.5427068705992335,best:2.3362760069113264\n",
      "in attepmt 933: loss=2.5174690297444076,best:2.3362760069113264\n",
      "in attepmt 934: loss=2.4084489044830213,best:2.3362760069113264\n",
      "in attepmt 935: loss=2.7111286514787847,best:2.3362760069113264\n",
      "in attepmt 936: loss=2.613484890416908,best:2.3362760069113264\n",
      "in attepmt 937: loss=2.4307351739026752,best:2.3362760069113264\n",
      "in attepmt 938: loss=2.544101376203679,best:2.3362760069113264\n",
      "in attepmt 939: loss=2.5087032982553534,best:2.3362760069113264\n",
      "in attepmt 940: loss=2.7478172165484476,best:2.3362760069113264\n",
      "in attepmt 941: loss=2.696169155753877,best:2.3362760069113264\n",
      "in attepmt 942: loss=2.5709715678690555,best:2.3362760069113264\n",
      "in attepmt 943: loss=2.5946569567824995,best:2.3362760069113264\n",
      "in attepmt 944: loss=2.4932326388588133,best:2.3362760069113264\n",
      "in attepmt 945: loss=2.7957428618338764,best:2.3362760069113264\n",
      "in attepmt 946: loss=2.735254110149962,best:2.3362760069113264\n",
      "in attepmt 947: loss=2.5435534487738543,best:2.3362760069113264\n",
      "in attepmt 948: loss=2.4834371909326927,best:2.3362760069113264\n",
      "in attepmt 949: loss=2.6081950860600105,best:2.3362760069113264\n",
      "in attepmt 950: loss=2.55116360094222,best:2.3362760069113264\n",
      "in attepmt 951: loss=2.523674991206795,best:2.3362760069113264\n",
      "in attepmt 952: loss=2.6457161517299954,best:2.3362760069113264\n",
      "in attepmt 953: loss=2.418898588519095,best:2.3362760069113264\n",
      "in attepmt 954: loss=2.380407504353712,best:2.3362760069113264\n",
      "in attepmt 955: loss=2.4721581727658197,best:2.3362760069113264\n",
      "in attepmt 956: loss=2.6097863136454373,best:2.3362760069113264\n",
      "in attepmt 957: loss=2.5204821961049433,best:2.3362760069113264\n",
      "in attepmt 958: loss=2.507910954485814,best:2.3362760069113264\n",
      "in attepmt 959: loss=2.5903700456010745,best:2.3362760069113264\n",
      "in attepmt 960: loss=2.5478448252119317,best:2.3362760069113264\n",
      "in attepmt 961: loss=2.642017840069339,best:2.3362760069113264\n",
      "in attepmt 962: loss=2.4726631685385363,best:2.3362760069113264\n",
      "in attepmt 963: loss=2.6666932286164164,best:2.3362760069113264\n",
      "in attepmt 964: loss=2.564715959570487,best:2.3362760069113264\n",
      "in attepmt 965: loss=2.4875433774333273,best:2.3362760069113264\n",
      "in attepmt 966: loss=2.5842329795213503,best:2.3362760069113264\n",
      "in attepmt 967: loss=2.4592718126309547,best:2.3362760069113264\n",
      "in attepmt 968: loss=2.584220363824767,best:2.3362760069113264\n",
      "in attepmt 969: loss=2.5579894787101094,best:2.3362760069113264\n",
      "in attepmt 970: loss=2.759464954031843,best:2.3362760069113264\n",
      "in attepmt 971: loss=2.441124654601685,best:2.3362760069113264\n",
      "in attepmt 972: loss=2.6733681294378386,best:2.3362760069113264\n",
      "in attepmt 973: loss=2.7314269351874554,best:2.3362760069113264\n",
      "in attepmt 974: loss=2.538940043950739,best:2.3362760069113264\n",
      "in attepmt 975: loss=2.7147714661539823,best:2.3362760069113264\n",
      "in attepmt 976: loss=2.6514719409081846,best:2.3362760069113264\n",
      "in attepmt 977: loss=2.4784305767783876,best:2.3362760069113264\n",
      "in attepmt 978: loss=2.443447989716233,best:2.3362760069113264\n",
      "in attepmt 979: loss=2.5703536988929594,best:2.3362760069113264\n",
      "in attepmt 980: loss=2.462712718671371,best:2.3362760069113264\n",
      "in attepmt 981: loss=2.457501851659467,best:2.3362760069113264\n",
      "in attepmt 982: loss=2.4380988431247457,best:2.3362760069113264\n",
      "in attepmt 983: loss=2.7174916950632393,best:2.3362760069113264\n",
      "in attepmt 984: loss=2.731294665828905,best:2.3362760069113264\n",
      "in attepmt 985: loss=2.4718606514911707,best:2.3362760069113264\n",
      "in attepmt 986: loss=2.520603377065361,best:2.3362760069113264\n",
      "in attepmt 987: loss=2.6121198224606577,best:2.3362760069113264\n",
      "in attepmt 988: loss=2.8198761594765442,best:2.3362760069113264\n",
      "in attepmt 989: loss=2.7904721942570028,best:2.3362760069113264\n",
      "in attepmt 990: loss=2.564102631279255,best:2.3362760069113264\n",
      "in attepmt 991: loss=2.581556982758706,best:2.3362760069113264\n",
      "in attepmt 992: loss=2.457815266917675,best:2.3362760069113264\n",
      "in attepmt 993: loss=2.440687972341833,best:2.3362760069113264\n",
      "in attepmt 994: loss=2.448199131910969,best:2.3362760069113264\n",
      "in attepmt 995: loss=2.427363005864528,best:2.3362760069113264\n",
      "in attepmt 996: loss=2.486317157552123,best:2.3362760069113264\n",
      "in attepmt 997: loss=2.3829487824689966,best:2.3362760069113264\n",
      "in attepmt 998: loss=2.446437956211463,best:2.3362760069113264\n",
      "in attepmt 999: loss=2.828306508556604,best:2.3362760069113264\n",
      "in attepmt 1000: loss=2.7542520215602013,best:2.3362760069113264\n"
     ]
    }
   ],
   "source": [
    "# cross_entropy_loss(X,y,W):\n",
    "best_loss_softmax=float(\"inf\")\n",
    "for i in range(1000):\n",
    "    W_softmax=np.random.randn(num_labels,num_features)*.0001\n",
    "    cost_softmax=cross_entropy_loss(Xtr_rows.T,train_labels,W_softmax)\n",
    "    if cost_softmax<best_loss_softmax:\n",
    "        best_loss_softmax=cost_softmax\n",
    "        best_W_softmax=W_softmax\n",
    "    print(f\"in attepmt {i+1}: loss={cost_softmax},best:{best_loss_softmax}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's calculate accuracy on test set using the optimized W we get from above 1000 iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "softmax accuracy using random optimization : 10.00%\n"
     ]
    }
   ],
   "source": [
    "#calcualte scores\n",
    "scores_softmax=np.dot(best_W_softmax,Xte_rows.T)\n",
    "# find the index with max score in each column (the predicted class) \n",
    "y_pred_softmax=np.argmax(scores_softmax.shape[0])\n",
    "#calculate accuracy (fractions of predictions that are correct)\n",
    "acc_softmax=np.mean(y_pred_softmax==test_labels)\n",
    "print(f\"softmax accuracy using random optimization : {acc_softmax*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This score is higher than the svm score. This is because cross-entropy loss was higher than svm loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Core idea: iterative refinement. \n",
    "- The core idea is that finding the best set of weights W is a very difficult or even impossible problem (especially once W contains weights for entire complex neural networks), \n",
    "- our approach will be to start with a random W and then iteratively refine it, making it slightly better each time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strategy #2: Random Local Search\n",
    "**algorithm**:\n",
    "- start out with a random W\n",
    "- generate random perturbations δW to it and if the loss at the perturbed W+δW is lower, we will perform an update. The code for this procedure is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in attempt 1 loss : 67.67233266844389, best: 67.67233266844389\n",
      "in attempt 2 loss : 79.34509641864469, best: 67.67233266844389\n",
      "in attempt 3 loss : 105.20444064776747, best: 67.67233266844389\n",
      "in attempt 4 loss : 66.6120507330709, best: 66.6120507330709\n",
      "in attempt 5 loss : 64.4807388693133, best: 64.4807388693133\n",
      "in attempt 6 loss : 65.80308708427545, best: 64.4807388693133\n",
      "in attempt 7 loss : 92.35622497479652, best: 64.4807388693133\n",
      "in attempt 8 loss : 73.00425489114286, best: 64.4807388693133\n",
      "in attempt 9 loss : 69.21280603000466, best: 64.4807388693133\n",
      "in attempt 10 loss : 95.68690031966958, best: 64.4807388693133\n",
      "in attempt 11 loss : 76.191449483368, best: 64.4807388693133\n",
      "in attempt 12 loss : 86.24120234658517, best: 64.4807388693133\n",
      "in attempt 13 loss : 83.8406908479443, best: 64.4807388693133\n",
      "in attempt 14 loss : 83.3423902761673, best: 64.4807388693133\n",
      "in attempt 15 loss : 65.44316462430727, best: 64.4807388693133\n",
      "in attempt 16 loss : 57.720921060863034, best: 57.720921060863034\n",
      "in attempt 17 loss : 55.400882940699944, best: 55.400882940699944\n",
      "in attempt 18 loss : 61.46477925231896, best: 55.400882940699944\n",
      "in attempt 19 loss : 82.23446545411801, best: 55.400882940699944\n",
      "in attempt 20 loss : 77.36062003260216, best: 55.400882940699944\n",
      "in attempt 21 loss : 62.34690407168055, best: 55.400882940699944\n",
      "in attempt 22 loss : 64.77415997299963, best: 55.400882940699944\n",
      "in attempt 23 loss : 65.46515584785921, best: 55.400882940699944\n",
      "in attempt 24 loss : 59.79887399233516, best: 55.400882940699944\n",
      "in attempt 25 loss : 55.123772737769876, best: 55.123772737769876\n",
      "in attempt 26 loss : 60.83026963914439, best: 55.123772737769876\n",
      "in attempt 27 loss : 70.78594668249796, best: 55.123772737769876\n",
      "in attempt 28 loss : 57.56569201602849, best: 55.123772737769876\n",
      "in attempt 29 loss : 83.81102919343114, best: 55.123772737769876\n",
      "in attempt 30 loss : 54.56215237755212, best: 54.56215237755212\n",
      "in attempt 31 loss : 72.47433574126359, best: 54.56215237755212\n",
      "in attempt 32 loss : 76.67828187436976, best: 54.56215237755212\n",
      "in attempt 33 loss : 60.861186708541965, best: 54.56215237755212\n",
      "in attempt 34 loss : 73.88104429379024, best: 54.56215237755212\n",
      "in attempt 35 loss : 73.33014273142616, best: 54.56215237755212\n",
      "in attempt 36 loss : 58.389256312250936, best: 54.56215237755212\n",
      "in attempt 37 loss : 71.25096445790052, best: 54.56215237755212\n",
      "in attempt 38 loss : 50.1547172494163, best: 50.1547172494163\n",
      "in attempt 39 loss : 83.55189226386031, best: 50.1547172494163\n",
      "in attempt 40 loss : 60.219771145204334, best: 50.1547172494163\n",
      "in attempt 41 loss : 55.147049799038804, best: 50.1547172494163\n",
      "in attempt 42 loss : 75.95674246095848, best: 50.1547172494163\n",
      "in attempt 43 loss : 63.26489037207044, best: 50.1547172494163\n",
      "in attempt 44 loss : 69.80080932133544, best: 50.1547172494163\n",
      "in attempt 45 loss : 67.82306514755336, best: 50.1547172494163\n",
      "in attempt 46 loss : 57.10496895359724, best: 50.1547172494163\n",
      "in attempt 47 loss : 62.662253538370855, best: 50.1547172494163\n",
      "in attempt 48 loss : 54.28095708566125, best: 50.1547172494163\n",
      "in attempt 49 loss : 58.00317158836929, best: 50.1547172494163\n",
      "in attempt 50 loss : 51.98608968570503, best: 50.1547172494163\n",
      "in attempt 51 loss : 55.19153981890754, best: 50.1547172494163\n",
      "in attempt 52 loss : 56.55897864077262, best: 50.1547172494163\n",
      "in attempt 53 loss : 71.05457584241724, best: 50.1547172494163\n",
      "in attempt 54 loss : 69.08937721198063, best: 50.1547172494163\n",
      "in attempt 55 loss : 60.16976461773837, best: 50.1547172494163\n",
      "in attempt 56 loss : 58.650595600435466, best: 50.1547172494163\n",
      "in attempt 57 loss : 57.002039289275615, best: 50.1547172494163\n",
      "in attempt 58 loss : 63.53766815285594, best: 50.1547172494163\n",
      "in attempt 59 loss : 64.40674160238079, best: 50.1547172494163\n",
      "in attempt 60 loss : 63.56109705861882, best: 50.1547172494163\n",
      "in attempt 61 loss : 64.62999645568293, best: 50.1547172494163\n",
      "in attempt 62 loss : 56.318854551943716, best: 50.1547172494163\n",
      "in attempt 63 loss : 58.50319761542993, best: 50.1547172494163\n",
      "in attempt 64 loss : 58.36319523905295, best: 50.1547172494163\n",
      "in attempt 65 loss : 66.41913851561597, best: 50.1547172494163\n",
      "in attempt 66 loss : 63.59049308549713, best: 50.1547172494163\n",
      "in attempt 67 loss : 69.66011260469654, best: 50.1547172494163\n",
      "in attempt 68 loss : 50.618512058760096, best: 50.1547172494163\n",
      "in attempt 69 loss : 71.555719433092, best: 50.1547172494163\n",
      "in attempt 70 loss : 68.92352896002203, best: 50.1547172494163\n",
      "in attempt 71 loss : 61.366576375387005, best: 50.1547172494163\n",
      "in attempt 72 loss : 55.2336878195801, best: 50.1547172494163\n",
      "in attempt 73 loss : 65.86225489838196, best: 50.1547172494163\n",
      "in attempt 74 loss : 62.93538236134906, best: 50.1547172494163\n",
      "in attempt 75 loss : 53.85978074170891, best: 50.1547172494163\n",
      "in attempt 76 loss : 67.14770242250462, best: 50.1547172494163\n",
      "in attempt 77 loss : 55.361550177134, best: 50.1547172494163\n",
      "in attempt 78 loss : 58.34847291710697, best: 50.1547172494163\n",
      "in attempt 79 loss : 68.71099332467878, best: 50.1547172494163\n",
      "in attempt 80 loss : 55.66506634407831, best: 50.1547172494163\n",
      "in attempt 81 loss : 54.616247467286634, best: 50.1547172494163\n",
      "in attempt 82 loss : 67.6909485684952, best: 50.1547172494163\n",
      "in attempt 83 loss : 60.94932516100617, best: 50.1547172494163\n",
      "in attempt 84 loss : 67.36016627756479, best: 50.1547172494163\n",
      "in attempt 85 loss : 63.21505991707961, best: 50.1547172494163\n",
      "in attempt 86 loss : 81.81177849921927, best: 50.1547172494163\n",
      "in attempt 87 loss : 54.83059748730376, best: 50.1547172494163\n",
      "in attempt 88 loss : 74.38003487993224, best: 50.1547172494163\n",
      "in attempt 89 loss : 67.17545917096643, best: 50.1547172494163\n",
      "in attempt 90 loss : 68.35030292095935, best: 50.1547172494163\n",
      "in attempt 91 loss : 57.86937695596141, best: 50.1547172494163\n",
      "in attempt 92 loss : 71.29132864072874, best: 50.1547172494163\n",
      "in attempt 93 loss : 69.79872530352182, best: 50.1547172494163\n",
      "in attempt 94 loss : 57.4433334294104, best: 50.1547172494163\n",
      "in attempt 95 loss : 54.352876888391485, best: 50.1547172494163\n",
      "in attempt 96 loss : 54.38740036140989, best: 50.1547172494163\n",
      "in attempt 97 loss : 70.9507080915323, best: 50.1547172494163\n",
      "in attempt 98 loss : 44.83717936539696, best: 44.83717936539696\n",
      "in attempt 99 loss : 57.21734628252919, best: 44.83717936539696\n",
      "in attempt 100 loss : 70.92674350525414, best: 44.83717936539696\n",
      "in attempt 101 loss : 55.88584018390734, best: 44.83717936539696\n",
      "in attempt 102 loss : 60.06798894672924, best: 44.83717936539696\n",
      "in attempt 103 loss : 50.20656053041828, best: 44.83717936539696\n",
      "in attempt 104 loss : 50.20811551452091, best: 44.83717936539696\n",
      "in attempt 105 loss : 60.19340739880502, best: 44.83717936539696\n",
      "in attempt 106 loss : 57.882481585755116, best: 44.83717936539696\n",
      "in attempt 107 loss : 55.751288829951896, best: 44.83717936539696\n",
      "in attempt 108 loss : 61.67192286036035, best: 44.83717936539696\n",
      "in attempt 109 loss : 53.470612170169986, best: 44.83717936539696\n",
      "in attempt 110 loss : 58.205233890439864, best: 44.83717936539696\n",
      "in attempt 111 loss : 51.230080719936055, best: 44.83717936539696\n",
      "in attempt 112 loss : 53.845118406593684, best: 44.83717936539696\n",
      "in attempt 113 loss : 52.21125793020566, best: 44.83717936539696\n",
      "in attempt 114 loss : 53.120864874396624, best: 44.83717936539696\n",
      "in attempt 115 loss : 63.047889603782636, best: 44.83717936539696\n",
      "in attempt 116 loss : 66.85563878686128, best: 44.83717936539696\n",
      "in attempt 117 loss : 48.59925193004144, best: 44.83717936539696\n",
      "in attempt 118 loss : 50.82403613968201, best: 44.83717936539696\n",
      "in attempt 119 loss : 58.929423976088636, best: 44.83717936539696\n",
      "in attempt 120 loss : 62.70728189203688, best: 44.83717936539696\n",
      "in attempt 121 loss : 60.073187006292024, best: 44.83717936539696\n",
      "in attempt 122 loss : 48.97611211528377, best: 44.83717936539696\n",
      "in attempt 123 loss : 58.75745372886861, best: 44.83717936539696\n",
      "in attempt 124 loss : 65.00119819574711, best: 44.83717936539696\n",
      "in attempt 125 loss : 54.81467900657516, best: 44.83717936539696\n",
      "in attempt 126 loss : 50.26120228965958, best: 44.83717936539696\n",
      "in attempt 127 loss : 57.17666242900927, best: 44.83717936539696\n",
      "in attempt 128 loss : 51.99145903349227, best: 44.83717936539696\n",
      "in attempt 129 loss : 53.38779328222757, best: 44.83717936539696\n",
      "in attempt 130 loss : 63.39545194951542, best: 44.83717936539696\n",
      "in attempt 131 loss : 59.36135674776405, best: 44.83717936539696\n",
      "in attempt 132 loss : 58.129572255903355, best: 44.83717936539696\n",
      "in attempt 133 loss : 56.7024690653379, best: 44.83717936539696\n",
      "in attempt 134 loss : 48.205332688127505, best: 44.83717936539696\n",
      "in attempt 135 loss : 65.67935110119443, best: 44.83717936539696\n",
      "in attempt 136 loss : 57.09285706135246, best: 44.83717936539696\n",
      "in attempt 137 loss : 63.609308754505804, best: 44.83717936539696\n",
      "in attempt 138 loss : 58.07762447197074, best: 44.83717936539696\n",
      "in attempt 139 loss : 57.68098336172606, best: 44.83717936539696\n",
      "in attempt 140 loss : 54.125724577297916, best: 44.83717936539696\n",
      "in attempt 141 loss : 59.97634075989118, best: 44.83717936539696\n",
      "in attempt 142 loss : 58.57285827554168, best: 44.83717936539696\n",
      "in attempt 143 loss : 54.65937910291195, best: 44.83717936539696\n",
      "in attempt 144 loss : 62.57792882428323, best: 44.83717936539696\n",
      "in attempt 145 loss : 63.71730181647572, best: 44.83717936539696\n",
      "in attempt 146 loss : 49.92325464653941, best: 44.83717936539696\n",
      "in attempt 147 loss : 50.12871616957961, best: 44.83717936539696\n",
      "in attempt 148 loss : 65.27108700745043, best: 44.83717936539696\n",
      "in attempt 149 loss : 50.51243217224714, best: 44.83717936539696\n",
      "in attempt 150 loss : 51.391053317115556, best: 44.83717936539696\n",
      "in attempt 151 loss : 55.41647170667302, best: 44.83717936539696\n",
      "in attempt 152 loss : 56.1728884267722, best: 44.83717936539696\n",
      "in attempt 153 loss : 52.0098300147896, best: 44.83717936539696\n",
      "in attempt 154 loss : 53.65100472879093, best: 44.83717936539696\n",
      "in attempt 155 loss : 65.41501819800384, best: 44.83717936539696\n",
      "in attempt 156 loss : 65.08815585066651, best: 44.83717936539696\n",
      "in attempt 157 loss : 51.324818289843876, best: 44.83717936539696\n",
      "in attempt 158 loss : 56.01745875831181, best: 44.83717936539696\n",
      "in attempt 159 loss : 60.52278749560857, best: 44.83717936539696\n",
      "in attempt 160 loss : 57.723338501929, best: 44.83717936539696\n",
      "in attempt 161 loss : 48.660192710785886, best: 44.83717936539696\n",
      "in attempt 162 loss : 56.942601611788554, best: 44.83717936539696\n",
      "in attempt 163 loss : 54.83557083419681, best: 44.83717936539696\n",
      "in attempt 164 loss : 52.79254769121637, best: 44.83717936539696\n",
      "in attempt 165 loss : 69.39296254390463, best: 44.83717936539696\n",
      "in attempt 166 loss : 61.50688269117967, best: 44.83717936539696\n",
      "in attempt 167 loss : 66.6102440352819, best: 44.83717936539696\n",
      "in attempt 168 loss : 54.98192395642708, best: 44.83717936539696\n",
      "in attempt 169 loss : 53.29940707943201, best: 44.83717936539696\n",
      "in attempt 170 loss : 68.6418460013891, best: 44.83717936539696\n",
      "in attempt 171 loss : 67.52080800203747, best: 44.83717936539696\n",
      "in attempt 172 loss : 54.18823974454542, best: 44.83717936539696\n",
      "in attempt 173 loss : 55.920684781729, best: 44.83717936539696\n",
      "in attempt 174 loss : 61.005280915551474, best: 44.83717936539696\n",
      "in attempt 175 loss : 55.614243872624755, best: 44.83717936539696\n",
      "in attempt 176 loss : 69.63142044587049, best: 44.83717936539696\n",
      "in attempt 177 loss : 60.89191942306592, best: 44.83717936539696\n",
      "in attempt 178 loss : 50.310516527540756, best: 44.83717936539696\n",
      "in attempt 179 loss : 62.14817076410401, best: 44.83717936539696\n",
      "in attempt 180 loss : 68.86893035227129, best: 44.83717936539696\n",
      "in attempt 181 loss : 67.79635444507511, best: 44.83717936539696\n",
      "in attempt 182 loss : 56.45174691355491, best: 44.83717936539696\n",
      "in attempt 183 loss : 65.40347420368401, best: 44.83717936539696\n",
      "in attempt 184 loss : 54.79798617190409, best: 44.83717936539696\n",
      "in attempt 185 loss : 52.17615403688862, best: 44.83717936539696\n",
      "in attempt 186 loss : 55.77619662448365, best: 44.83717936539696\n",
      "in attempt 187 loss : 51.242684142952186, best: 44.83717936539696\n",
      "in attempt 188 loss : 61.026273329195895, best: 44.83717936539696\n",
      "in attempt 189 loss : 58.43194722768524, best: 44.83717936539696\n",
      "in attempt 190 loss : 66.50218558615575, best: 44.83717936539696\n",
      "in attempt 191 loss : 54.68204917895677, best: 44.83717936539696\n",
      "in attempt 192 loss : 65.71715709429137, best: 44.83717936539696\n",
      "in attempt 193 loss : 59.51563185720743, best: 44.83717936539696\n",
      "in attempt 194 loss : 50.319657297290036, best: 44.83717936539696\n",
      "in attempt 195 loss : 56.72369739241568, best: 44.83717936539696\n",
      "in attempt 196 loss : 51.229605832843426, best: 44.83717936539696\n",
      "in attempt 197 loss : 59.78699468183328, best: 44.83717936539696\n",
      "in attempt 198 loss : 62.43853179449417, best: 44.83717936539696\n",
      "in attempt 199 loss : 63.65435751672375, best: 44.83717936539696\n",
      "in attempt 200 loss : 57.22444117050355, best: 44.83717936539696\n",
      "in attempt 201 loss : 54.7300629570089, best: 44.83717936539696\n",
      "in attempt 202 loss : 58.889711019997485, best: 44.83717936539696\n",
      "in attempt 203 loss : 57.7292477732942, best: 44.83717936539696\n",
      "in attempt 204 loss : 58.24009989630989, best: 44.83717936539696\n",
      "in attempt 205 loss : 56.64267589403008, best: 44.83717936539696\n",
      "in attempt 206 loss : 54.6212381373851, best: 44.83717936539696\n",
      "in attempt 207 loss : 49.73724084203494, best: 44.83717936539696\n",
      "in attempt 208 loss : 53.820675538375816, best: 44.83717936539696\n",
      "in attempt 209 loss : 49.2690283984782, best: 44.83717936539696\n",
      "in attempt 210 loss : 53.25223429730226, best: 44.83717936539696\n",
      "in attempt 211 loss : 58.53067235596323, best: 44.83717936539696\n",
      "in attempt 212 loss : 63.85297723770998, best: 44.83717936539696\n",
      "in attempt 213 loss : 49.4816793175234, best: 44.83717936539696\n",
      "in attempt 214 loss : 52.771886888839155, best: 44.83717936539696\n",
      "in attempt 215 loss : 61.11864606627456, best: 44.83717936539696\n",
      "in attempt 216 loss : 65.73605249737105, best: 44.83717936539696\n",
      "in attempt 217 loss : 53.171471876933396, best: 44.83717936539696\n",
      "in attempt 218 loss : 53.224294214275, best: 44.83717936539696\n",
      "in attempt 219 loss : 56.31139813341106, best: 44.83717936539696\n",
      "in attempt 220 loss : 49.44931159649059, best: 44.83717936539696\n",
      "in attempt 221 loss : 59.36954316215658, best: 44.83717936539696\n",
      "in attempt 222 loss : 62.39334599066174, best: 44.83717936539696\n",
      "in attempt 223 loss : 54.186466133055745, best: 44.83717936539696\n",
      "in attempt 224 loss : 54.40872797045914, best: 44.83717936539696\n",
      "in attempt 225 loss : 49.28597177937633, best: 44.83717936539696\n",
      "in attempt 226 loss : 52.61145762814771, best: 44.83717936539696\n",
      "in attempt 227 loss : 61.97384197593723, best: 44.83717936539696\n",
      "in attempt 228 loss : 50.337554536592755, best: 44.83717936539696\n",
      "in attempt 229 loss : 48.44263122864354, best: 44.83717936539696\n",
      "in attempt 230 loss : 61.80749104684005, best: 44.83717936539696\n",
      "in attempt 231 loss : 64.6696359984055, best: 44.83717936539696\n",
      "in attempt 232 loss : 48.21309258864519, best: 44.83717936539696\n",
      "in attempt 233 loss : 56.40049106540873, best: 44.83717936539696\n",
      "in attempt 234 loss : 66.58335141820513, best: 44.83717936539696\n",
      "in attempt 235 loss : 60.980927314785085, best: 44.83717936539696\n",
      "in attempt 236 loss : 50.98790226705155, best: 44.83717936539696\n",
      "in attempt 237 loss : 55.7922846126731, best: 44.83717936539696\n",
      "in attempt 238 loss : 61.52367943321788, best: 44.83717936539696\n",
      "in attempt 239 loss : 65.42693062965986, best: 44.83717936539696\n",
      "in attempt 240 loss : 69.0750834873669, best: 44.83717936539696\n",
      "in attempt 241 loss : 63.45249422819307, best: 44.83717936539696\n",
      "in attempt 242 loss : 57.1606541037411, best: 44.83717936539696\n",
      "in attempt 243 loss : 56.81847519186795, best: 44.83717936539696\n",
      "in attempt 244 loss : 60.22808945440875, best: 44.83717936539696\n",
      "in attempt 245 loss : 57.56236760016227, best: 44.83717936539696\n",
      "in attempt 246 loss : 69.2253992714134, best: 44.83717936539696\n",
      "in attempt 247 loss : 55.40607362339191, best: 44.83717936539696\n",
      "in attempt 248 loss : 84.63933774243264, best: 44.83717936539696\n",
      "in attempt 249 loss : 56.50160789463761, best: 44.83717936539696\n",
      "in attempt 250 loss : 63.744542855698434, best: 44.83717936539696\n",
      "in attempt 251 loss : 58.81124107737167, best: 44.83717936539696\n",
      "in attempt 252 loss : 63.95158061672514, best: 44.83717936539696\n",
      "in attempt 253 loss : 58.75968813468959, best: 44.83717936539696\n",
      "in attempt 254 loss : 52.49873702759919, best: 44.83717936539696\n",
      "in attempt 255 loss : 59.673447806409136, best: 44.83717936539696\n",
      "in attempt 256 loss : 67.00847002344767, best: 44.83717936539696\n",
      "in attempt 257 loss : 58.61704074529863, best: 44.83717936539696\n",
      "in attempt 258 loss : 78.820513798695, best: 44.83717936539696\n",
      "in attempt 259 loss : 58.17393764440522, best: 44.83717936539696\n",
      "in attempt 260 loss : 68.97449834123474, best: 44.83717936539696\n",
      "in attempt 261 loss : 58.68968034960677, best: 44.83717936539696\n",
      "in attempt 262 loss : 56.629425478093296, best: 44.83717936539696\n",
      "in attempt 263 loss : 62.96049384085425, best: 44.83717936539696\n",
      "in attempt 264 loss : 52.6461146500092, best: 44.83717936539696\n",
      "in attempt 265 loss : 54.2104997160778, best: 44.83717936539696\n",
      "in attempt 266 loss : 65.60497344091722, best: 44.83717936539696\n",
      "in attempt 267 loss : 67.14285693264661, best: 44.83717936539696\n",
      "in attempt 268 loss : 50.84508310753022, best: 44.83717936539696\n",
      "in attempt 269 loss : 53.121771919375405, best: 44.83717936539696\n",
      "in attempt 270 loss : 50.43987502946435, best: 44.83717936539696\n",
      "in attempt 271 loss : 73.49383191550653, best: 44.83717936539696\n",
      "in attempt 272 loss : 54.404057233633786, best: 44.83717936539696\n",
      "in attempt 273 loss : 50.638499812805065, best: 44.83717936539696\n",
      "in attempt 274 loss : 52.74030651024371, best: 44.83717936539696\n",
      "in attempt 275 loss : 58.98979741281446, best: 44.83717936539696\n",
      "in attempt 276 loss : 52.027041501496896, best: 44.83717936539696\n",
      "in attempt 277 loss : 48.853023980153786, best: 44.83717936539696\n",
      "in attempt 278 loss : 49.379578953375564, best: 44.83717936539696\n",
      "in attempt 279 loss : 58.7494735743903, best: 44.83717936539696\n",
      "in attempt 280 loss : 52.60651696464472, best: 44.83717936539696\n",
      "in attempt 281 loss : 62.781173167888944, best: 44.83717936539696\n",
      "in attempt 282 loss : 50.613588169515786, best: 44.83717936539696\n",
      "in attempt 283 loss : 60.642252314349214, best: 44.83717936539696\n",
      "in attempt 284 loss : 57.09283991668517, best: 44.83717936539696\n",
      "in attempt 285 loss : 54.22970083072845, best: 44.83717936539696\n",
      "in attempt 286 loss : 66.70492953890063, best: 44.83717936539696\n",
      "in attempt 287 loss : 47.38269470936832, best: 44.83717936539696\n",
      "in attempt 288 loss : 54.234895627042825, best: 44.83717936539696\n",
      "in attempt 289 loss : 53.112075453612384, best: 44.83717936539696\n",
      "in attempt 290 loss : 61.142158203806595, best: 44.83717936539696\n",
      "in attempt 291 loss : 63.103859190764375, best: 44.83717936539696\n",
      "in attempt 292 loss : 66.90213772683914, best: 44.83717936539696\n",
      "in attempt 293 loss : 60.58511141595638, best: 44.83717936539696\n",
      "in attempt 294 loss : 55.23510304356514, best: 44.83717936539696\n",
      "in attempt 295 loss : 56.04291556586522, best: 44.83717936539696\n",
      "in attempt 296 loss : 62.42667064653652, best: 44.83717936539696\n",
      "in attempt 297 loss : 47.85457493041653, best: 44.83717936539696\n",
      "in attempt 298 loss : 57.466565670253566, best: 44.83717936539696\n",
      "in attempt 299 loss : 48.342406932965254, best: 44.83717936539696\n",
      "in attempt 300 loss : 60.07326258779743, best: 44.83717936539696\n",
      "in attempt 301 loss : 60.101514372182564, best: 44.83717936539696\n",
      "in attempt 302 loss : 47.651750602202334, best: 44.83717936539696\n",
      "in attempt 303 loss : 55.81139216377798, best: 44.83717936539696\n",
      "in attempt 304 loss : 61.117430237008634, best: 44.83717936539696\n",
      "in attempt 305 loss : 55.695712568606474, best: 44.83717936539696\n",
      "in attempt 306 loss : 60.312399871937394, best: 44.83717936539696\n",
      "in attempt 307 loss : 71.94184320204397, best: 44.83717936539696\n",
      "in attempt 308 loss : 50.75554409832151, best: 44.83717936539696\n",
      "in attempt 309 loss : 59.08004547415589, best: 44.83717936539696\n",
      "in attempt 310 loss : 62.11245986644976, best: 44.83717936539696\n",
      "in attempt 311 loss : 59.80998866819246, best: 44.83717936539696\n",
      "in attempt 312 loss : 58.23665556786428, best: 44.83717936539696\n",
      "in attempt 313 loss : 57.99755398761306, best: 44.83717936539696\n",
      "in attempt 314 loss : 69.38569738449455, best: 44.83717936539696\n",
      "in attempt 315 loss : 57.06326298473667, best: 44.83717936539696\n",
      "in attempt 316 loss : 56.83783492112204, best: 44.83717936539696\n",
      "in attempt 317 loss : 57.23845047791973, best: 44.83717936539696\n",
      "in attempt 318 loss : 62.9371563813353, best: 44.83717936539696\n",
      "in attempt 319 loss : 56.30552142042552, best: 44.83717936539696\n",
      "in attempt 320 loss : 56.229994736100565, best: 44.83717936539696\n",
      "in attempt 321 loss : 51.62596932227412, best: 44.83717936539696\n",
      "in attempt 322 loss : 53.210185522842146, best: 44.83717936539696\n",
      "in attempt 323 loss : 59.71566257702314, best: 44.83717936539696\n",
      "in attempt 324 loss : 55.579702429202946, best: 44.83717936539696\n",
      "in attempt 325 loss : 53.11217748950148, best: 44.83717936539696\n",
      "in attempt 326 loss : 58.67421506044416, best: 44.83717936539696\n",
      "in attempt 327 loss : 50.731389992746195, best: 44.83717936539696\n",
      "in attempt 328 loss : 73.01932103716963, best: 44.83717936539696\n",
      "in attempt 329 loss : 75.46045991754568, best: 44.83717936539696\n",
      "in attempt 330 loss : 46.308430655879405, best: 44.83717936539696\n",
      "in attempt 331 loss : 59.87671351119426, best: 44.83717936539696\n",
      "in attempt 332 loss : 54.30431323501316, best: 44.83717936539696\n",
      "in attempt 333 loss : 53.212950981679604, best: 44.83717936539696\n",
      "in attempt 334 loss : 65.54603991934029, best: 44.83717936539696\n",
      "in attempt 335 loss : 54.92491405434602, best: 44.83717936539696\n",
      "in attempt 336 loss : 65.47208739466805, best: 44.83717936539696\n",
      "in attempt 337 loss : 53.26159420103969, best: 44.83717936539696\n",
      "in attempt 338 loss : 57.75634129902268, best: 44.83717936539696\n",
      "in attempt 339 loss : 59.66856707683522, best: 44.83717936539696\n",
      "in attempt 340 loss : 49.543755856550035, best: 44.83717936539696\n",
      "in attempt 341 loss : 51.23129581085512, best: 44.83717936539696\n",
      "in attempt 342 loss : 56.4510489254262, best: 44.83717936539696\n",
      "in attempt 343 loss : 52.17177130166604, best: 44.83717936539696\n",
      "in attempt 344 loss : 60.47452011548344, best: 44.83717936539696\n",
      "in attempt 345 loss : 64.50384715473125, best: 44.83717936539696\n",
      "in attempt 346 loss : 62.727022538319495, best: 44.83717936539696\n",
      "in attempt 347 loss : 58.90543672915481, best: 44.83717936539696\n",
      "in attempt 348 loss : 53.507021389418895, best: 44.83717936539696\n",
      "in attempt 349 loss : 54.346902800992865, best: 44.83717936539696\n",
      "in attempt 350 loss : 82.12338605151318, best: 44.83717936539696\n",
      "in attempt 351 loss : 54.80297437616451, best: 44.83717936539696\n",
      "in attempt 352 loss : 57.03535529789603, best: 44.83717936539696\n",
      "in attempt 353 loss : 55.72244836427905, best: 44.83717936539696\n",
      "in attempt 354 loss : 80.06210264371892, best: 44.83717936539696\n",
      "in attempt 355 loss : 67.57396515056163, best: 44.83717936539696\n",
      "in attempt 356 loss : 59.973185361593266, best: 44.83717936539696\n",
      "in attempt 357 loss : 56.164606798976976, best: 44.83717936539696\n",
      "in attempt 358 loss : 56.85759093553504, best: 44.83717936539696\n",
      "in attempt 359 loss : 61.8712408741479, best: 44.83717936539696\n",
      "in attempt 360 loss : 57.0425527694864, best: 44.83717936539696\n",
      "in attempt 361 loss : 51.80390640156696, best: 44.83717936539696\n",
      "in attempt 362 loss : 55.08897313935421, best: 44.83717936539696\n",
      "in attempt 363 loss : 60.37762270693428, best: 44.83717936539696\n",
      "in attempt 364 loss : 53.81992538711032, best: 44.83717936539696\n",
      "in attempt 365 loss : 66.40202954025008, best: 44.83717936539696\n",
      "in attempt 366 loss : 53.177026433068335, best: 44.83717936539696\n",
      "in attempt 367 loss : 58.88686890752109, best: 44.83717936539696\n",
      "in attempt 368 loss : 53.1131419841563, best: 44.83717936539696\n",
      "in attempt 369 loss : 50.8980522207109, best: 44.83717936539696\n",
      "in attempt 370 loss : 68.78687113359958, best: 44.83717936539696\n",
      "in attempt 371 loss : 55.149353615986506, best: 44.83717936539696\n",
      "in attempt 372 loss : 60.724000226297676, best: 44.83717936539696\n",
      "in attempt 373 loss : 68.08616032225189, best: 44.83717936539696\n",
      "in attempt 374 loss : 53.03780572031546, best: 44.83717936539696\n",
      "in attempt 375 loss : 63.75940601277009, best: 44.83717936539696\n",
      "in attempt 376 loss : 61.58383363552542, best: 44.83717936539696\n",
      "in attempt 377 loss : 52.96595294251858, best: 44.83717936539696\n",
      "in attempt 378 loss : 72.42085372448517, best: 44.83717936539696\n",
      "in attempt 379 loss : 59.51371287010342, best: 44.83717936539696\n",
      "in attempt 380 loss : 58.686112867792325, best: 44.83717936539696\n",
      "in attempt 381 loss : 52.91784712377684, best: 44.83717936539696\n",
      "in attempt 382 loss : 53.45819593253396, best: 44.83717936539696\n",
      "in attempt 383 loss : 60.48788485667327, best: 44.83717936539696\n",
      "in attempt 384 loss : 69.88922419836764, best: 44.83717936539696\n",
      "in attempt 385 loss : 52.11705583777649, best: 44.83717936539696\n",
      "in attempt 386 loss : 50.69566324710891, best: 44.83717936539696\n",
      "in attempt 387 loss : 58.330002119719225, best: 44.83717936539696\n",
      "in attempt 388 loss : 59.99962911089328, best: 44.83717936539696\n",
      "in attempt 389 loss : 60.79282212185423, best: 44.83717936539696\n",
      "in attempt 390 loss : 62.72998480852213, best: 44.83717936539696\n",
      "in attempt 391 loss : 68.24647655281025, best: 44.83717936539696\n",
      "in attempt 392 loss : 46.08609504356181, best: 44.83717936539696\n",
      "in attempt 393 loss : 57.643865310336736, best: 44.83717936539696\n",
      "in attempt 394 loss : 60.769186036615935, best: 44.83717936539696\n",
      "in attempt 395 loss : 58.78048281565991, best: 44.83717936539696\n",
      "in attempt 396 loss : 57.08684807753226, best: 44.83717936539696\n",
      "in attempt 397 loss : 54.730974888671135, best: 44.83717936539696\n",
      "in attempt 398 loss : 55.35721860587126, best: 44.83717936539696\n",
      "in attempt 399 loss : 51.80943556750662, best: 44.83717936539696\n",
      "in attempt 400 loss : 67.08746480163629, best: 44.83717936539696\n",
      "in attempt 401 loss : 51.70068216811024, best: 44.83717936539696\n",
      "in attempt 402 loss : 58.41417795621266, best: 44.83717936539696\n",
      "in attempt 403 loss : 57.83072272421539, best: 44.83717936539696\n",
      "in attempt 404 loss : 53.73956703786909, best: 44.83717936539696\n",
      "in attempt 405 loss : 67.38590742927522, best: 44.83717936539696\n",
      "in attempt 406 loss : 58.06457063081719, best: 44.83717936539696\n",
      "in attempt 407 loss : 66.04546860975134, best: 44.83717936539696\n",
      "in attempt 408 loss : 53.98230488264471, best: 44.83717936539696\n",
      "in attempt 409 loss : 63.33735568896745, best: 44.83717936539696\n",
      "in attempt 410 loss : 64.49507867614133, best: 44.83717936539696\n",
      "in attempt 411 loss : 53.59040194929457, best: 44.83717936539696\n",
      "in attempt 412 loss : 64.33986944622333, best: 44.83717936539696\n",
      "in attempt 413 loss : 55.925371184416996, best: 44.83717936539696\n",
      "in attempt 414 loss : 57.33462752700499, best: 44.83717936539696\n",
      "in attempt 415 loss : 62.21834341458194, best: 44.83717936539696\n",
      "in attempt 416 loss : 58.913329199163535, best: 44.83717936539696\n",
      "in attempt 417 loss : 70.43768742861835, best: 44.83717936539696\n",
      "in attempt 418 loss : 56.62861436577531, best: 44.83717936539696\n",
      "in attempt 419 loss : 55.56928878258697, best: 44.83717936539696\n",
      "in attempt 420 loss : 59.76797349207876, best: 44.83717936539696\n",
      "in attempt 421 loss : 62.447845401369406, best: 44.83717936539696\n",
      "in attempt 422 loss : 59.57571800451225, best: 44.83717936539696\n",
      "in attempt 423 loss : 56.45880126328934, best: 44.83717936539696\n",
      "in attempt 424 loss : 52.88988177356421, best: 44.83717936539696\n",
      "in attempt 425 loss : 64.7522592068365, best: 44.83717936539696\n",
      "in attempt 426 loss : 55.826299363800516, best: 44.83717936539696\n",
      "in attempt 427 loss : 46.90726445832929, best: 44.83717936539696\n",
      "in attempt 428 loss : 58.98615127221528, best: 44.83717936539696\n",
      "in attempt 429 loss : 51.66834534252828, best: 44.83717936539696\n",
      "in attempt 430 loss : 64.97420903776064, best: 44.83717936539696\n",
      "in attempt 431 loss : 57.08348853891407, best: 44.83717936539696\n",
      "in attempt 432 loss : 55.04347081617306, best: 44.83717936539696\n",
      "in attempt 433 loss : 52.72463772127187, best: 44.83717936539696\n",
      "in attempt 434 loss : 61.7562438623868, best: 44.83717936539696\n",
      "in attempt 435 loss : 58.25498923204321, best: 44.83717936539696\n",
      "in attempt 436 loss : 57.23691547993699, best: 44.83717936539696\n",
      "in attempt 437 loss : 55.16604798381517, best: 44.83717936539696\n",
      "in attempt 438 loss : 61.60068706806108, best: 44.83717936539696\n",
      "in attempt 439 loss : 61.830773418054896, best: 44.83717936539696\n",
      "in attempt 440 loss : 60.100812351295275, best: 44.83717936539696\n",
      "in attempt 441 loss : 61.2134169719336, best: 44.83717936539696\n",
      "in attempt 442 loss : 62.83708571338792, best: 44.83717936539696\n",
      "in attempt 443 loss : 61.29922068267159, best: 44.83717936539696\n",
      "in attempt 444 loss : 64.44650360954618, best: 44.83717936539696\n",
      "in attempt 445 loss : 62.67275309045176, best: 44.83717936539696\n",
      "in attempt 446 loss : 63.08871991238617, best: 44.83717936539696\n",
      "in attempt 447 loss : 62.083849663486724, best: 44.83717936539696\n",
      "in attempt 448 loss : 59.7561954561369, best: 44.83717936539696\n",
      "in attempt 449 loss : 65.00535622136613, best: 44.83717936539696\n",
      "in attempt 450 loss : 53.64444397410741, best: 44.83717936539696\n",
      "in attempt 451 loss : 55.51477158336161, best: 44.83717936539696\n",
      "in attempt 452 loss : 52.974319306605594, best: 44.83717936539696\n",
      "in attempt 453 loss : 61.214477334146025, best: 44.83717936539696\n",
      "in attempt 454 loss : 60.58643937130447, best: 44.83717936539696\n",
      "in attempt 455 loss : 58.12621353730011, best: 44.83717936539696\n",
      "in attempt 456 loss : 58.557658871910945, best: 44.83717936539696\n",
      "in attempt 457 loss : 60.69428200563236, best: 44.83717936539696\n",
      "in attempt 458 loss : 58.64239623625234, best: 44.83717936539696\n",
      "in attempt 459 loss : 66.06865840306841, best: 44.83717936539696\n",
      "in attempt 460 loss : 54.720592827648716, best: 44.83717936539696\n",
      "in attempt 461 loss : 54.22761592917397, best: 44.83717936539696\n",
      "in attempt 462 loss : 54.22902076708158, best: 44.83717936539696\n",
      "in attempt 463 loss : 54.22588363752464, best: 44.83717936539696\n",
      "in attempt 464 loss : 51.84583978677148, best: 44.83717936539696\n",
      "in attempt 465 loss : 51.60240064719197, best: 44.83717936539696\n",
      "in attempt 466 loss : 57.61749303639361, best: 44.83717936539696\n",
      "in attempt 467 loss : 60.59345714810299, best: 44.83717936539696\n",
      "in attempt 468 loss : 66.15389557150095, best: 44.83717936539696\n",
      "in attempt 469 loss : 58.497081996801214, best: 44.83717936539696\n",
      "in attempt 470 loss : 59.37711508435767, best: 44.83717936539696\n",
      "in attempt 471 loss : 59.348902997886626, best: 44.83717936539696\n",
      "in attempt 472 loss : 62.576290018432566, best: 44.83717936539696\n",
      "in attempt 473 loss : 57.20201174674103, best: 44.83717936539696\n",
      "in attempt 474 loss : 54.948512895552724, best: 44.83717936539696\n",
      "in attempt 475 loss : 63.05788533273, best: 44.83717936539696\n",
      "in attempt 476 loss : 53.806223045039694, best: 44.83717936539696\n",
      "in attempt 477 loss : 72.13378097656125, best: 44.83717936539696\n",
      "in attempt 478 loss : 65.48177473297461, best: 44.83717936539696\n",
      "in attempt 479 loss : 57.71317656898138, best: 44.83717936539696\n",
      "in attempt 480 loss : 56.212877024385456, best: 44.83717936539696\n",
      "in attempt 481 loss : 62.21580798458626, best: 44.83717936539696\n",
      "in attempt 482 loss : 65.5324080542081, best: 44.83717936539696\n",
      "in attempt 483 loss : 53.74624652977249, best: 44.83717936539696\n",
      "in attempt 484 loss : 55.17169630630686, best: 44.83717936539696\n",
      "in attempt 485 loss : 56.24988440396354, best: 44.83717936539696\n",
      "in attempt 486 loss : 63.52672582931071, best: 44.83717936539696\n",
      "in attempt 487 loss : 49.71444334549773, best: 44.83717936539696\n",
      "in attempt 488 loss : 75.0548742015055, best: 44.83717936539696\n",
      "in attempt 489 loss : 58.79979631097942, best: 44.83717936539696\n",
      "in attempt 490 loss : 60.44796816740239, best: 44.83717936539696\n",
      "in attempt 491 loss : 60.45284916876221, best: 44.83717936539696\n",
      "in attempt 492 loss : 57.47596962152232, best: 44.83717936539696\n",
      "in attempt 493 loss : 67.4038683791813, best: 44.83717936539696\n",
      "in attempt 494 loss : 56.09428351883554, best: 44.83717936539696\n",
      "in attempt 495 loss : 63.146166407138324, best: 44.83717936539696\n",
      "in attempt 496 loss : 49.20516399788384, best: 44.83717936539696\n",
      "in attempt 497 loss : 63.792319833489124, best: 44.83717936539696\n",
      "in attempt 498 loss : 51.93992891712948, best: 44.83717936539696\n",
      "in attempt 499 loss : 60.7204235180117, best: 44.83717936539696\n",
      "in attempt 500 loss : 56.45047596082942, best: 44.83717936539696\n",
      "in attempt 501 loss : 49.78912678062406, best: 44.83717936539696\n",
      "in attempt 502 loss : 57.583957749847535, best: 44.83717936539696\n",
      "in attempt 503 loss : 61.427979349051725, best: 44.83717936539696\n",
      "in attempt 504 loss : 60.33552837526472, best: 44.83717936539696\n",
      "in attempt 505 loss : 56.74957308237816, best: 44.83717936539696\n",
      "in attempt 506 loss : 49.96861307207652, best: 44.83717936539696\n",
      "in attempt 507 loss : 60.40164866180593, best: 44.83717936539696\n",
      "in attempt 508 loss : 58.943028113602466, best: 44.83717936539696\n",
      "in attempt 509 loss : 49.185469918554794, best: 44.83717936539696\n",
      "in attempt 510 loss : 64.58223850932966, best: 44.83717936539696\n",
      "in attempt 511 loss : 65.80536336466128, best: 44.83717936539696\n",
      "in attempt 512 loss : 47.27337796460536, best: 44.83717936539696\n",
      "in attempt 513 loss : 50.603612710273445, best: 44.83717936539696\n",
      "in attempt 514 loss : 58.143011091224786, best: 44.83717936539696\n",
      "in attempt 515 loss : 59.289310525568304, best: 44.83717936539696\n",
      "in attempt 516 loss : 57.7029731279796, best: 44.83717936539696\n",
      "in attempt 517 loss : 54.48041480440538, best: 44.83717936539696\n",
      "in attempt 518 loss : 60.687557507068675, best: 44.83717936539696\n",
      "in attempt 519 loss : 48.90036580043714, best: 44.83717936539696\n",
      "in attempt 520 loss : 68.31054873421986, best: 44.83717936539696\n",
      "in attempt 521 loss : 59.81295798483896, best: 44.83717936539696\n",
      "in attempt 522 loss : 55.18038482566063, best: 44.83717936539696\n",
      "in attempt 523 loss : 55.75161734583242, best: 44.83717936539696\n",
      "in attempt 524 loss : 67.55070737160304, best: 44.83717936539696\n",
      "in attempt 525 loss : 68.99027423872359, best: 44.83717936539696\n",
      "in attempt 526 loss : 60.700647149124286, best: 44.83717936539696\n",
      "in attempt 527 loss : 60.23304777575902, best: 44.83717936539696\n",
      "in attempt 528 loss : 61.865430425420186, best: 44.83717936539696\n",
      "in attempt 529 loss : 55.37116418565282, best: 44.83717936539696\n",
      "in attempt 530 loss : 62.69539681661215, best: 44.83717936539696\n",
      "in attempt 531 loss : 62.85276557543387, best: 44.83717936539696\n",
      "in attempt 532 loss : 56.38388455618306, best: 44.83717936539696\n",
      "in attempt 533 loss : 63.127384519651315, best: 44.83717936539696\n",
      "in attempt 534 loss : 51.23783346986355, best: 44.83717936539696\n",
      "in attempt 535 loss : 58.86217125024416, best: 44.83717936539696\n",
      "in attempt 536 loss : 60.73151424559744, best: 44.83717936539696\n",
      "in attempt 537 loss : 52.65866816987783, best: 44.83717936539696\n",
      "in attempt 538 loss : 51.90516876125026, best: 44.83717936539696\n",
      "in attempt 539 loss : 52.753622411819784, best: 44.83717936539696\n",
      "in attempt 540 loss : 67.49005900593762, best: 44.83717936539696\n",
      "in attempt 541 loss : 60.654674610155844, best: 44.83717936539696\n",
      "in attempt 542 loss : 54.274676358771615, best: 44.83717936539696\n",
      "in attempt 543 loss : 55.752360612465154, best: 44.83717936539696\n",
      "in attempt 544 loss : 50.41306998979067, best: 44.83717936539696\n",
      "in attempt 545 loss : 59.216582995686544, best: 44.83717936539696\n",
      "in attempt 546 loss : 54.85842370582267, best: 44.83717936539696\n",
      "in attempt 547 loss : 54.17815721340776, best: 44.83717936539696\n",
      "in attempt 548 loss : 51.05842766232945, best: 44.83717936539696\n",
      "in attempt 549 loss : 58.44497198514279, best: 44.83717936539696\n",
      "in attempt 550 loss : 60.112116759680355, best: 44.83717936539696\n",
      "in attempt 551 loss : 58.57197921095803, best: 44.83717936539696\n",
      "in attempt 552 loss : 49.32377610276643, best: 44.83717936539696\n",
      "in attempt 553 loss : 62.79133649529289, best: 44.83717936539696\n",
      "in attempt 554 loss : 50.920948873556966, best: 44.83717936539696\n",
      "in attempt 555 loss : 54.07094442853985, best: 44.83717936539696\n",
      "in attempt 556 loss : 54.81140663577357, best: 44.83717936539696\n",
      "in attempt 557 loss : 57.97219426355693, best: 44.83717936539696\n",
      "in attempt 558 loss : 63.408316286864356, best: 44.83717936539696\n",
      "in attempt 559 loss : 55.17568488290662, best: 44.83717936539696\n",
      "in attempt 560 loss : 56.414659530175, best: 44.83717936539696\n",
      "in attempt 561 loss : 59.730127005119584, best: 44.83717936539696\n",
      "in attempt 562 loss : 59.019604158570104, best: 44.83717936539696\n",
      "in attempt 563 loss : 61.61347408623403, best: 44.83717936539696\n",
      "in attempt 564 loss : 70.51590355696962, best: 44.83717936539696\n",
      "in attempt 565 loss : 54.184060359452104, best: 44.83717936539696\n",
      "in attempt 566 loss : 58.41435803873624, best: 44.83717936539696\n",
      "in attempt 567 loss : 63.637293028854224, best: 44.83717936539696\n",
      "in attempt 568 loss : 51.7166871477006, best: 44.83717936539696\n",
      "in attempt 569 loss : 53.039753563056216, best: 44.83717936539696\n",
      "in attempt 570 loss : 59.294198819161124, best: 44.83717936539696\n",
      "in attempt 571 loss : 64.26437374298827, best: 44.83717936539696\n",
      "in attempt 572 loss : 70.68429282745849, best: 44.83717936539696\n",
      "in attempt 573 loss : 56.86813669550712, best: 44.83717936539696\n",
      "in attempt 574 loss : 53.9651481884504, best: 44.83717936539696\n",
      "in attempt 575 loss : 60.21345242960566, best: 44.83717936539696\n",
      "in attempt 576 loss : 52.14799341155379, best: 44.83717936539696\n",
      "in attempt 577 loss : 61.172576000782165, best: 44.83717936539696\n",
      "in attempt 578 loss : 54.32734424520915, best: 44.83717936539696\n",
      "in attempt 579 loss : 61.50523331198491, best: 44.83717936539696\n",
      "in attempt 580 loss : 62.340023566081534, best: 44.83717936539696\n",
      "in attempt 581 loss : 56.98204670335239, best: 44.83717936539696\n",
      "in attempt 582 loss : 54.488425031422096, best: 44.83717936539696\n",
      "in attempt 583 loss : 56.331942636514626, best: 44.83717936539696\n",
      "in attempt 584 loss : 62.345339293589724, best: 44.83717936539696\n",
      "in attempt 585 loss : 60.821095823066784, best: 44.83717936539696\n",
      "in attempt 586 loss : 57.63946568695658, best: 44.83717936539696\n",
      "in attempt 587 loss : 56.45893237965496, best: 44.83717936539696\n",
      "in attempt 588 loss : 54.88951405322935, best: 44.83717936539696\n",
      "in attempt 589 loss : 56.341697124331674, best: 44.83717936539696\n",
      "in attempt 590 loss : 64.73967142986339, best: 44.83717936539696\n",
      "in attempt 591 loss : 50.501294965144055, best: 44.83717936539696\n",
      "in attempt 592 loss : 68.02458686716228, best: 44.83717936539696\n",
      "in attempt 593 loss : 67.4069637793366, best: 44.83717936539696\n",
      "in attempt 594 loss : 57.43423581897451, best: 44.83717936539696\n",
      "in attempt 595 loss : 54.70186141706812, best: 44.83717936539696\n",
      "in attempt 596 loss : 57.28170350131421, best: 44.83717936539696\n",
      "in attempt 597 loss : 55.19315241486188, best: 44.83717936539696\n",
      "in attempt 598 loss : 49.8725842516013, best: 44.83717936539696\n",
      "in attempt 599 loss : 54.54555690139741, best: 44.83717936539696\n",
      "in attempt 600 loss : 60.35346317266133, best: 44.83717936539696\n",
      "in attempt 601 loss : 57.499703901836504, best: 44.83717936539696\n",
      "in attempt 602 loss : 53.9938520797359, best: 44.83717936539696\n",
      "in attempt 603 loss : 49.05565591608182, best: 44.83717936539696\n",
      "in attempt 604 loss : 54.61531771708983, best: 44.83717936539696\n",
      "in attempt 605 loss : 56.6509762095633, best: 44.83717936539696\n",
      "in attempt 606 loss : 57.90051574985089, best: 44.83717936539696\n",
      "in attempt 607 loss : 57.80435106444141, best: 44.83717936539696\n",
      "in attempt 608 loss : 63.77571525917659, best: 44.83717936539696\n",
      "in attempt 609 loss : 65.24033817997288, best: 44.83717936539696\n",
      "in attempt 610 loss : 59.76172810682126, best: 44.83717936539696\n",
      "in attempt 611 loss : 73.81315049411177, best: 44.83717936539696\n",
      "in attempt 612 loss : 54.30903794027444, best: 44.83717936539696\n",
      "in attempt 613 loss : 70.15933974482974, best: 44.83717936539696\n",
      "in attempt 614 loss : 57.56036744393743, best: 44.83717936539696\n",
      "in attempt 615 loss : 49.230271900197344, best: 44.83717936539696\n",
      "in attempt 616 loss : 54.062505354000685, best: 44.83717936539696\n",
      "in attempt 617 loss : 53.882547160896884, best: 44.83717936539696\n",
      "in attempt 618 loss : 65.93689259463818, best: 44.83717936539696\n",
      "in attempt 619 loss : 51.38930572568039, best: 44.83717936539696\n",
      "in attempt 620 loss : 57.03515577265886, best: 44.83717936539696\n",
      "in attempt 621 loss : 55.27602464943178, best: 44.83717936539696\n",
      "in attempt 622 loss : 49.139372378001404, best: 44.83717936539696\n",
      "in attempt 623 loss : 63.7121091581712, best: 44.83717936539696\n",
      "in attempt 624 loss : 70.18894748561108, best: 44.83717936539696\n",
      "in attempt 625 loss : 52.299731227040446, best: 44.83717936539696\n",
      "in attempt 626 loss : 57.959662486091325, best: 44.83717936539696\n",
      "in attempt 627 loss : 57.44751061782328, best: 44.83717936539696\n",
      "in attempt 628 loss : 52.176386731573196, best: 44.83717936539696\n",
      "in attempt 629 loss : 65.34092636637325, best: 44.83717936539696\n",
      "in attempt 630 loss : 50.003237832381345, best: 44.83717936539696\n",
      "in attempt 631 loss : 67.21128245704719, best: 44.83717936539696\n",
      "in attempt 632 loss : 52.121904226601515, best: 44.83717936539696\n",
      "in attempt 633 loss : 53.86888519588905, best: 44.83717936539696\n",
      "in attempt 634 loss : 57.39835457889489, best: 44.83717936539696\n",
      "in attempt 635 loss : 50.272338935541185, best: 44.83717936539696\n",
      "in attempt 636 loss : 54.142178876038685, best: 44.83717936539696\n",
      "in attempt 637 loss : 68.69802519694578, best: 44.83717936539696\n",
      "in attempt 638 loss : 58.309828182018315, best: 44.83717936539696\n",
      "in attempt 639 loss : 55.96387392225466, best: 44.83717936539696\n",
      "in attempt 640 loss : 54.1190733831925, best: 44.83717936539696\n",
      "in attempt 641 loss : 47.5924811634359, best: 44.83717936539696\n",
      "in attempt 642 loss : 53.66948665254182, best: 44.83717936539696\n",
      "in attempt 643 loss : 60.055995056078146, best: 44.83717936539696\n",
      "in attempt 644 loss : 56.08179571833696, best: 44.83717936539696\n",
      "in attempt 645 loss : 57.4242819067045, best: 44.83717936539696\n",
      "in attempt 646 loss : 56.532679451479325, best: 44.83717936539696\n",
      "in attempt 647 loss : 60.91582708416506, best: 44.83717936539696\n",
      "in attempt 648 loss : 58.93801826100135, best: 44.83717936539696\n",
      "in attempt 649 loss : 56.81336096466332, best: 44.83717936539696\n",
      "in attempt 650 loss : 65.55339253598505, best: 44.83717936539696\n",
      "in attempt 651 loss : 57.79374144993111, best: 44.83717936539696\n",
      "in attempt 652 loss : 51.114305943340284, best: 44.83717936539696\n",
      "in attempt 653 loss : 46.97989874576538, best: 44.83717936539696\n",
      "in attempt 654 loss : 59.724380097408236, best: 44.83717936539696\n",
      "in attempt 655 loss : 60.30600527789861, best: 44.83717936539696\n",
      "in attempt 656 loss : 53.36712513998875, best: 44.83717936539696\n",
      "in attempt 657 loss : 54.043421009237264, best: 44.83717936539696\n",
      "in attempt 658 loss : 66.61103965505409, best: 44.83717936539696\n",
      "in attempt 659 loss : 75.86703837324073, best: 44.83717936539696\n",
      "in attempt 660 loss : 79.01351972791443, best: 44.83717936539696\n",
      "in attempt 661 loss : 57.93095427939635, best: 44.83717936539696\n",
      "in attempt 662 loss : 57.99100501723296, best: 44.83717936539696\n",
      "in attempt 663 loss : 54.70666941991721, best: 44.83717936539696\n",
      "in attempt 664 loss : 59.30358870791348, best: 44.83717936539696\n",
      "in attempt 665 loss : 46.35849893028665, best: 44.83717936539696\n",
      "in attempt 666 loss : 63.191639904135116, best: 44.83717936539696\n",
      "in attempt 667 loss : 69.98706491970948, best: 44.83717936539696\n",
      "in attempt 668 loss : 58.26261271554157, best: 44.83717936539696\n",
      "in attempt 669 loss : 65.33951123168414, best: 44.83717936539696\n",
      "in attempt 670 loss : 62.214964749378126, best: 44.83717936539696\n",
      "in attempt 671 loss : 66.47680157830797, best: 44.83717936539696\n",
      "in attempt 672 loss : 57.15024725476527, best: 44.83717936539696\n",
      "in attempt 673 loss : 59.81515286587176, best: 44.83717936539696\n",
      "in attempt 674 loss : 66.41552192487924, best: 44.83717936539696\n",
      "in attempt 675 loss : 53.58530534692238, best: 44.83717936539696\n",
      "in attempt 676 loss : 51.962615043454505, best: 44.83717936539696\n",
      "in attempt 677 loss : 53.727257918017436, best: 44.83717936539696\n",
      "in attempt 678 loss : 68.25316525709964, best: 44.83717936539696\n",
      "in attempt 679 loss : 68.07338046964978, best: 44.83717936539696\n",
      "in attempt 680 loss : 61.052631056824644, best: 44.83717936539696\n",
      "in attempt 681 loss : 70.02489745499936, best: 44.83717936539696\n",
      "in attempt 682 loss : 56.998000473486854, best: 44.83717936539696\n",
      "in attempt 683 loss : 61.90746805431572, best: 44.83717936539696\n",
      "in attempt 684 loss : 60.802492111785185, best: 44.83717936539696\n",
      "in attempt 685 loss : 69.71968945337989, best: 44.83717936539696\n",
      "in attempt 686 loss : 53.854962403775794, best: 44.83717936539696\n",
      "in attempt 687 loss : 55.73947715838139, best: 44.83717936539696\n",
      "in attempt 688 loss : 60.2446376452918, best: 44.83717936539696\n",
      "in attempt 689 loss : 55.69981174293041, best: 44.83717936539696\n",
      "in attempt 690 loss : 56.5002235768819, best: 44.83717936539696\n",
      "in attempt 691 loss : 62.30919239396712, best: 44.83717936539696\n",
      "in attempt 692 loss : 57.4581983007934, best: 44.83717936539696\n",
      "in attempt 693 loss : 58.50615422893497, best: 44.83717936539696\n",
      "in attempt 694 loss : 68.171225575677, best: 44.83717936539696\n",
      "in attempt 695 loss : 55.424451313811836, best: 44.83717936539696\n",
      "in attempt 696 loss : 53.596603708026414, best: 44.83717936539696\n",
      "in attempt 697 loss : 70.54314438424387, best: 44.83717936539696\n",
      "in attempt 698 loss : 65.05215234191174, best: 44.83717936539696\n",
      "in attempt 699 loss : 50.55290102960762, best: 44.83717936539696\n",
      "in attempt 700 loss : 50.77036162277007, best: 44.83717936539696\n",
      "in attempt 701 loss : 60.80066796553459, best: 44.83717936539696\n",
      "in attempt 702 loss : 62.56363094646058, best: 44.83717936539696\n",
      "in attempt 703 loss : 59.307687099555466, best: 44.83717936539696\n",
      "in attempt 704 loss : 56.989859475264346, best: 44.83717936539696\n",
      "in attempt 705 loss : 53.38515219683138, best: 44.83717936539696\n",
      "in attempt 706 loss : 59.807882517229544, best: 44.83717936539696\n",
      "in attempt 707 loss : 60.78639913253007, best: 44.83717936539696\n",
      "in attempt 708 loss : 64.02061007268472, best: 44.83717936539696\n",
      "in attempt 709 loss : 70.28815478908272, best: 44.83717936539696\n",
      "in attempt 710 loss : 60.16652522079958, best: 44.83717936539696\n",
      "in attempt 711 loss : 56.99670068659891, best: 44.83717936539696\n",
      "in attempt 712 loss : 64.18013297061403, best: 44.83717936539696\n",
      "in attempt 713 loss : 63.82359468911869, best: 44.83717936539696\n",
      "in attempt 714 loss : 57.10919879061559, best: 44.83717936539696\n",
      "in attempt 715 loss : 50.22598632059717, best: 44.83717936539696\n",
      "in attempt 716 loss : 71.0153942775481, best: 44.83717936539696\n",
      "in attempt 717 loss : 59.740943218300494, best: 44.83717936539696\n",
      "in attempt 718 loss : 49.3099083848228, best: 44.83717936539696\n",
      "in attempt 719 loss : 49.628307870063296, best: 44.83717936539696\n",
      "in attempt 720 loss : 64.15971762970146, best: 44.83717936539696\n",
      "in attempt 721 loss : 57.3257898492908, best: 44.83717936539696\n",
      "in attempt 722 loss : 47.920413328034115, best: 44.83717936539696\n",
      "in attempt 723 loss : 51.996839445026076, best: 44.83717936539696\n",
      "in attempt 724 loss : 56.34535190033834, best: 44.83717936539696\n",
      "in attempt 725 loss : 54.48114461972532, best: 44.83717936539696\n",
      "in attempt 726 loss : 50.33107107176794, best: 44.83717936539696\n",
      "in attempt 727 loss : 75.95420804572603, best: 44.83717936539696\n",
      "in attempt 728 loss : 54.06453374234038, best: 44.83717936539696\n",
      "in attempt 729 loss : 67.7049132373682, best: 44.83717936539696\n",
      "in attempt 730 loss : 71.83273142511565, best: 44.83717936539696\n",
      "in attempt 731 loss : 63.870545515106734, best: 44.83717936539696\n",
      "in attempt 732 loss : 63.60336735740587, best: 44.83717936539696\n",
      "in attempt 733 loss : 59.437248236485374, best: 44.83717936539696\n",
      "in attempt 734 loss : 56.662570200887565, best: 44.83717936539696\n",
      "in attempt 735 loss : 51.275817614957866, best: 44.83717936539696\n",
      "in attempt 736 loss : 54.47772979872878, best: 44.83717936539696\n",
      "in attempt 737 loss : 60.24129426725203, best: 44.83717936539696\n",
      "in attempt 738 loss : 52.209930146868174, best: 44.83717936539696\n",
      "in attempt 739 loss : 55.44633539081021, best: 44.83717936539696\n",
      "in attempt 740 loss : 64.76365571303816, best: 44.83717936539696\n",
      "in attempt 741 loss : 65.29748805197745, best: 44.83717936539696\n",
      "in attempt 742 loss : 56.122745859926475, best: 44.83717936539696\n",
      "in attempt 743 loss : 64.31295082331239, best: 44.83717936539696\n",
      "in attempt 744 loss : 56.594089371846174, best: 44.83717936539696\n",
      "in attempt 745 loss : 63.80515511062831, best: 44.83717936539696\n",
      "in attempt 746 loss : 63.6135212607447, best: 44.83717936539696\n",
      "in attempt 747 loss : 51.715649132124945, best: 44.83717936539696\n",
      "in attempt 748 loss : 63.70225350536771, best: 44.83717936539696\n",
      "in attempt 749 loss : 65.51469767238198, best: 44.83717936539696\n",
      "in attempt 750 loss : 57.623999773485075, best: 44.83717936539696\n",
      "in attempt 751 loss : 58.646237239194654, best: 44.83717936539696\n",
      "in attempt 752 loss : 55.12969825225697, best: 44.83717936539696\n",
      "in attempt 753 loss : 62.284086438369854, best: 44.83717936539696\n",
      "in attempt 754 loss : 75.4803633656871, best: 44.83717936539696\n",
      "in attempt 755 loss : 66.9765830563182, best: 44.83717936539696\n",
      "in attempt 756 loss : 67.7030164410022, best: 44.83717936539696\n",
      "in attempt 757 loss : 60.1417808447611, best: 44.83717936539696\n",
      "in attempt 758 loss : 57.551660331998036, best: 44.83717936539696\n",
      "in attempt 759 loss : 49.477242803146595, best: 44.83717936539696\n",
      "in attempt 760 loss : 57.84694990466971, best: 44.83717936539696\n",
      "in attempt 761 loss : 56.83172995806339, best: 44.83717936539696\n",
      "in attempt 762 loss : 65.80060784889952, best: 44.83717936539696\n",
      "in attempt 763 loss : 64.27954323706486, best: 44.83717936539696\n",
      "in attempt 764 loss : 60.22876427969568, best: 44.83717936539696\n",
      "in attempt 765 loss : 57.08989344756905, best: 44.83717936539696\n",
      "in attempt 766 loss : 52.75519413535966, best: 44.83717936539696\n",
      "in attempt 767 loss : 58.40537534850937, best: 44.83717936539696\n",
      "in attempt 768 loss : 52.022447314787385, best: 44.83717936539696\n",
      "in attempt 769 loss : 49.46610490358777, best: 44.83717936539696\n",
      "in attempt 770 loss : 63.07072345064382, best: 44.83717936539696\n",
      "in attempt 771 loss : 57.08974024927986, best: 44.83717936539696\n",
      "in attempt 772 loss : 63.01930260951593, best: 44.83717936539696\n",
      "in attempt 773 loss : 61.21989160463685, best: 44.83717936539696\n",
      "in attempt 774 loss : 52.18676681070954, best: 44.83717936539696\n",
      "in attempt 775 loss : 62.45694522321855, best: 44.83717936539696\n",
      "in attempt 776 loss : 65.20172895588634, best: 44.83717936539696\n",
      "in attempt 777 loss : 54.39601586270843, best: 44.83717936539696\n",
      "in attempt 778 loss : 45.78498291870304, best: 44.83717936539696\n",
      "in attempt 779 loss : 52.39763467588118, best: 44.83717936539696\n",
      "in attempt 780 loss : 49.83997643890066, best: 44.83717936539696\n",
      "in attempt 781 loss : 52.70209243768306, best: 44.83717936539696\n",
      "in attempt 782 loss : 57.93180784284063, best: 44.83717936539696\n",
      "in attempt 783 loss : 57.77221710079252, best: 44.83717936539696\n",
      "in attempt 784 loss : 64.70110412478836, best: 44.83717936539696\n",
      "in attempt 785 loss : 57.75779954244945, best: 44.83717936539696\n",
      "in attempt 786 loss : 57.342882873462216, best: 44.83717936539696\n",
      "in attempt 787 loss : 55.489677383389655, best: 44.83717936539696\n",
      "in attempt 788 loss : 47.533183135326894, best: 44.83717936539696\n",
      "in attempt 789 loss : 68.90529188511854, best: 44.83717936539696\n",
      "in attempt 790 loss : 61.507675946284905, best: 44.83717936539696\n",
      "in attempt 791 loss : 57.04962880872457, best: 44.83717936539696\n",
      "in attempt 792 loss : 50.646031509658506, best: 44.83717936539696\n",
      "in attempt 793 loss : 69.63099193480795, best: 44.83717936539696\n",
      "in attempt 794 loss : 49.64587774546337, best: 44.83717936539696\n",
      "in attempt 795 loss : 53.09128881642334, best: 44.83717936539696\n",
      "in attempt 796 loss : 61.342651195002475, best: 44.83717936539696\n",
      "in attempt 797 loss : 54.84713264679278, best: 44.83717936539696\n",
      "in attempt 798 loss : 54.10951679650427, best: 44.83717936539696\n",
      "in attempt 799 loss : 66.93784422740704, best: 44.83717936539696\n",
      "in attempt 800 loss : 59.62105943661599, best: 44.83717936539696\n",
      "in attempt 801 loss : 55.12425638162847, best: 44.83717936539696\n",
      "in attempt 802 loss : 69.78849620098424, best: 44.83717936539696\n",
      "in attempt 803 loss : 54.14337185209307, best: 44.83717936539696\n",
      "in attempt 804 loss : 59.375778638112855, best: 44.83717936539696\n",
      "in attempt 805 loss : 66.49037077049773, best: 44.83717936539696\n",
      "in attempt 806 loss : 59.12072370208252, best: 44.83717936539696\n",
      "in attempt 807 loss : 56.60995092195799, best: 44.83717936539696\n",
      "in attempt 808 loss : 61.7057219814191, best: 44.83717936539696\n",
      "in attempt 809 loss : 52.21652604771211, best: 44.83717936539696\n",
      "in attempt 810 loss : 63.75251372467391, best: 44.83717936539696\n",
      "in attempt 811 loss : 68.96631833717983, best: 44.83717936539696\n",
      "in attempt 812 loss : 57.858845414340394, best: 44.83717936539696\n",
      "in attempt 813 loss : 69.2424533390976, best: 44.83717936539696\n",
      "in attempt 814 loss : 64.63463491416125, best: 44.83717936539696\n",
      "in attempt 815 loss : 59.890900216790136, best: 44.83717936539696\n",
      "in attempt 816 loss : 56.683808908163556, best: 44.83717936539696\n",
      "in attempt 817 loss : 64.55282434847537, best: 44.83717936539696\n",
      "in attempt 818 loss : 61.48358709055076, best: 44.83717936539696\n",
      "in attempt 819 loss : 46.50989185841465, best: 44.83717936539696\n",
      "in attempt 820 loss : 65.73021816725998, best: 44.83717936539696\n",
      "in attempt 821 loss : 56.87513804381661, best: 44.83717936539696\n",
      "in attempt 822 loss : 58.14097437375284, best: 44.83717936539696\n",
      "in attempt 823 loss : 62.471220697047734, best: 44.83717936539696\n",
      "in attempt 824 loss : 64.5104817772756, best: 44.83717936539696\n",
      "in attempt 825 loss : 54.697658788574024, best: 44.83717936539696\n",
      "in attempt 826 loss : 74.30421495721747, best: 44.83717936539696\n",
      "in attempt 827 loss : 58.57437712414956, best: 44.83717936539696\n",
      "in attempt 828 loss : 59.059839170489525, best: 44.83717936539696\n",
      "in attempt 829 loss : 55.952798685959536, best: 44.83717936539696\n",
      "in attempt 830 loss : 56.05402041049464, best: 44.83717936539696\n",
      "in attempt 831 loss : 50.34554856336619, best: 44.83717936539696\n",
      "in attempt 832 loss : 53.98054578440832, best: 44.83717936539696\n",
      "in attempt 833 loss : 61.66461091501102, best: 44.83717936539696\n",
      "in attempt 834 loss : 50.10434725951773, best: 44.83717936539696\n",
      "in attempt 835 loss : 58.096342175977114, best: 44.83717936539696\n",
      "in attempt 836 loss : 71.00822938602069, best: 44.83717936539696\n",
      "in attempt 837 loss : 61.03163698390624, best: 44.83717936539696\n",
      "in attempt 838 loss : 65.61920537168494, best: 44.83717936539696\n",
      "in attempt 839 loss : 56.18515563779865, best: 44.83717936539696\n",
      "in attempt 840 loss : 65.81246281667694, best: 44.83717936539696\n",
      "in attempt 841 loss : 53.58332301538026, best: 44.83717936539696\n",
      "in attempt 842 loss : 58.200936343519096, best: 44.83717936539696\n",
      "in attempt 843 loss : 52.605862304150406, best: 44.83717936539696\n",
      "in attempt 844 loss : 58.94750717979236, best: 44.83717936539696\n",
      "in attempt 845 loss : 64.47716576252918, best: 44.83717936539696\n",
      "in attempt 846 loss : 60.67776713179729, best: 44.83717936539696\n",
      "in attempt 847 loss : 57.88231341764582, best: 44.83717936539696\n",
      "in attempt 848 loss : 60.03212282770818, best: 44.83717936539696\n",
      "in attempt 849 loss : 64.87343227878074, best: 44.83717936539696\n",
      "in attempt 850 loss : 56.09854378127468, best: 44.83717936539696\n",
      "in attempt 851 loss : 71.82578531656272, best: 44.83717936539696\n",
      "in attempt 852 loss : 56.557620346225555, best: 44.83717936539696\n",
      "in attempt 853 loss : 57.53472022820119, best: 44.83717936539696\n",
      "in attempt 854 loss : 61.32879052497926, best: 44.83717936539696\n",
      "in attempt 855 loss : 52.61064909591355, best: 44.83717936539696\n",
      "in attempt 856 loss : 55.50195213365092, best: 44.83717936539696\n",
      "in attempt 857 loss : 54.71155261237452, best: 44.83717936539696\n",
      "in attempt 858 loss : 61.984329261706854, best: 44.83717936539696\n",
      "in attempt 859 loss : 65.7966999051935, best: 44.83717936539696\n",
      "in attempt 860 loss : 52.97545149484365, best: 44.83717936539696\n",
      "in attempt 861 loss : 53.08084077731722, best: 44.83717936539696\n",
      "in attempt 862 loss : 52.57341949685883, best: 44.83717936539696\n",
      "in attempt 863 loss : 57.47841670671591, best: 44.83717936539696\n",
      "in attempt 864 loss : 55.41996605297552, best: 44.83717936539696\n",
      "in attempt 865 loss : 68.65194922174713, best: 44.83717936539696\n",
      "in attempt 866 loss : 57.528858299555694, best: 44.83717936539696\n",
      "in attempt 867 loss : 73.8435125224142, best: 44.83717936539696\n",
      "in attempt 868 loss : 60.740616761013854, best: 44.83717936539696\n",
      "in attempt 869 loss : 63.28073533698691, best: 44.83717936539696\n",
      "in attempt 870 loss : 54.56680461640043, best: 44.83717936539696\n",
      "in attempt 871 loss : 52.30802857286482, best: 44.83717936539696\n",
      "in attempt 872 loss : 53.57301717505422, best: 44.83717936539696\n",
      "in attempt 873 loss : 61.17418464417606, best: 44.83717936539696\n",
      "in attempt 874 loss : 54.60973618159067, best: 44.83717936539696\n",
      "in attempt 875 loss : 71.00261978739402, best: 44.83717936539696\n",
      "in attempt 876 loss : 54.412761135838046, best: 44.83717936539696\n",
      "in attempt 877 loss : 74.65430472921516, best: 44.83717936539696\n",
      "in attempt 878 loss : 62.73251867950922, best: 44.83717936539696\n",
      "in attempt 879 loss : 56.52964976840028, best: 44.83717936539696\n",
      "in attempt 880 loss : 52.35913551577153, best: 44.83717936539696\n",
      "in attempt 881 loss : 53.57349905221058, best: 44.83717936539696\n",
      "in attempt 882 loss : 57.345147400021055, best: 44.83717936539696\n",
      "in attempt 883 loss : 48.03136044339895, best: 44.83717936539696\n",
      "in attempt 884 loss : 55.1119275358453, best: 44.83717936539696\n",
      "in attempt 885 loss : 62.336480101319964, best: 44.83717936539696\n",
      "in attempt 886 loss : 53.1616545450267, best: 44.83717936539696\n",
      "in attempt 887 loss : 58.913758483020864, best: 44.83717936539696\n",
      "in attempt 888 loss : 68.70409057093288, best: 44.83717936539696\n",
      "in attempt 889 loss : 47.28545243732164, best: 44.83717936539696\n",
      "in attempt 890 loss : 64.09856185616955, best: 44.83717936539696\n",
      "in attempt 891 loss : 57.813670396426474, best: 44.83717936539696\n",
      "in attempt 892 loss : 65.25074595392319, best: 44.83717936539696\n",
      "in attempt 893 loss : 49.561649375402126, best: 44.83717936539696\n",
      "in attempt 894 loss : 62.88942827904678, best: 44.83717936539696\n",
      "in attempt 895 loss : 49.96574510070196, best: 44.83717936539696\n",
      "in attempt 896 loss : 69.23166154574253, best: 44.83717936539696\n",
      "in attempt 897 loss : 47.0235828667243, best: 44.83717936539696\n",
      "in attempt 898 loss : 57.29056236104304, best: 44.83717936539696\n",
      "in attempt 899 loss : 51.92594183036861, best: 44.83717936539696\n",
      "in attempt 900 loss : 66.41410574004844, best: 44.83717936539696\n",
      "in attempt 901 loss : 55.641207170548334, best: 44.83717936539696\n",
      "in attempt 902 loss : 57.46390650760625, best: 44.83717936539696\n",
      "in attempt 903 loss : 55.46901572735064, best: 44.83717936539696\n",
      "in attempt 904 loss : 60.181024085618695, best: 44.83717936539696\n",
      "in attempt 905 loss : 59.17606573384478, best: 44.83717936539696\n",
      "in attempt 906 loss : 54.019034613830485, best: 44.83717936539696\n",
      "in attempt 907 loss : 62.26633010513542, best: 44.83717936539696\n",
      "in attempt 908 loss : 53.55863185673044, best: 44.83717936539696\n",
      "in attempt 909 loss : 52.200537635612555, best: 44.83717936539696\n",
      "in attempt 910 loss : 56.58938664123986, best: 44.83717936539696\n",
      "in attempt 911 loss : 65.78555360270593, best: 44.83717936539696\n",
      "in attempt 912 loss : 58.608055983431946, best: 44.83717936539696\n",
      "in attempt 913 loss : 54.421368981508294, best: 44.83717936539696\n",
      "in attempt 914 loss : 62.98916363948151, best: 44.83717936539696\n",
      "in attempt 915 loss : 64.60731005000072, best: 44.83717936539696\n",
      "in attempt 916 loss : 56.704725482642715, best: 44.83717936539696\n",
      "in attempt 917 loss : 61.429404947190804, best: 44.83717936539696\n",
      "in attempt 918 loss : 57.8954799878856, best: 44.83717936539696\n",
      "in attempt 919 loss : 50.10980418309324, best: 44.83717936539696\n",
      "in attempt 920 loss : 60.33548172523977, best: 44.83717936539696\n",
      "in attempt 921 loss : 69.15398975612699, best: 44.83717936539696\n",
      "in attempt 922 loss : 58.43847935828986, best: 44.83717936539696\n",
      "in attempt 923 loss : 78.74332455625603, best: 44.83717936539696\n",
      "in attempt 924 loss : 62.51562761583272, best: 44.83717936539696\n",
      "in attempt 925 loss : 48.54030602463728, best: 44.83717936539696\n",
      "in attempt 926 loss : 54.875463621006645, best: 44.83717936539696\n",
      "in attempt 927 loss : 68.91545198990347, best: 44.83717936539696\n",
      "in attempt 928 loss : 57.44621089111929, best: 44.83717936539696\n",
      "in attempt 929 loss : 51.93448427114235, best: 44.83717936539696\n",
      "in attempt 930 loss : 56.892554264467215, best: 44.83717936539696\n",
      "in attempt 931 loss : 68.05076536563381, best: 44.83717936539696\n",
      "in attempt 932 loss : 57.028502763846376, best: 44.83717936539696\n",
      "in attempt 933 loss : 53.27860871863185, best: 44.83717936539696\n",
      "in attempt 934 loss : 64.07896307638644, best: 44.83717936539696\n",
      "in attempt 935 loss : 59.385881328686594, best: 44.83717936539696\n",
      "in attempt 936 loss : 68.85430986369944, best: 44.83717936539696\n",
      "in attempt 937 loss : 53.99956876025575, best: 44.83717936539696\n",
      "in attempt 938 loss : 54.30237923940335, best: 44.83717936539696\n",
      "in attempt 939 loss : 58.639335568322686, best: 44.83717936539696\n",
      "in attempt 940 loss : 59.96700092680864, best: 44.83717936539696\n",
      "in attempt 941 loss : 53.55315565171606, best: 44.83717936539696\n",
      "in attempt 942 loss : 65.60956647461379, best: 44.83717936539696\n",
      "in attempt 943 loss : 58.11705731367775, best: 44.83717936539696\n",
      "in attempt 944 loss : 64.30376049930227, best: 44.83717936539696\n",
      "in attempt 945 loss : 66.54272764081597, best: 44.83717936539696\n",
      "in attempt 946 loss : 54.491258600491406, best: 44.83717936539696\n",
      "in attempt 947 loss : 56.55565829426529, best: 44.83717936539696\n",
      "in attempt 948 loss : 56.08347695615525, best: 44.83717936539696\n",
      "in attempt 949 loss : 52.09476610484025, best: 44.83717936539696\n",
      "in attempt 950 loss : 52.97264613497542, best: 44.83717936539696\n",
      "in attempt 951 loss : 62.0947130946613, best: 44.83717936539696\n",
      "in attempt 952 loss : 64.21439721626564, best: 44.83717936539696\n",
      "in attempt 953 loss : 51.8883830303729, best: 44.83717936539696\n",
      "in attempt 954 loss : 56.743857719436704, best: 44.83717936539696\n",
      "in attempt 955 loss : 80.28455606260505, best: 44.83717936539696\n",
      "in attempt 956 loss : 61.55811483433034, best: 44.83717936539696\n",
      "in attempt 957 loss : 63.966086172230426, best: 44.83717936539696\n",
      "in attempt 958 loss : 56.245343110260585, best: 44.83717936539696\n",
      "in attempt 959 loss : 71.1308964899563, best: 44.83717936539696\n",
      "in attempt 960 loss : 58.08451743637775, best: 44.83717936539696\n",
      "in attempt 961 loss : 67.86472847626115, best: 44.83717936539696\n",
      "in attempt 962 loss : 57.14988838896677, best: 44.83717936539696\n",
      "in attempt 963 loss : 60.014682948738376, best: 44.83717936539696\n",
      "in attempt 964 loss : 54.85691694724611, best: 44.83717936539696\n",
      "in attempt 965 loss : 63.44248506372873, best: 44.83717936539696\n",
      "in attempt 966 loss : 52.36519181768319, best: 44.83717936539696\n",
      "in attempt 967 loss : 52.35124826099279, best: 44.83717936539696\n",
      "in attempt 968 loss : 57.1476946448712, best: 44.83717936539696\n",
      "in attempt 969 loss : 58.42134955066677, best: 44.83717936539696\n",
      "in attempt 970 loss : 52.773251926874885, best: 44.83717936539696\n",
      "in attempt 971 loss : 65.70700879741734, best: 44.83717936539696\n",
      "in attempt 972 loss : 45.89332339796883, best: 44.83717936539696\n",
      "in attempt 973 loss : 67.95925082857536, best: 44.83717936539696\n",
      "in attempt 974 loss : 50.000228300499586, best: 44.83717936539696\n",
      "in attempt 975 loss : 55.281860228311395, best: 44.83717936539696\n",
      "in attempt 976 loss : 64.76138734775579, best: 44.83717936539696\n",
      "in attempt 977 loss : 48.43595980179515, best: 44.83717936539696\n",
      "in attempt 978 loss : 71.83344115543402, best: 44.83717936539696\n",
      "in attempt 979 loss : 58.27432157548285, best: 44.83717936539696\n",
      "in attempt 980 loss : 58.812266077173476, best: 44.83717936539696\n",
      "in attempt 981 loss : 58.418620181413715, best: 44.83717936539696\n",
      "in attempt 982 loss : 59.982446317798455, best: 44.83717936539696\n",
      "in attempt 983 loss : 54.097821704079934, best: 44.83717936539696\n",
      "in attempt 984 loss : 63.96981963189009, best: 44.83717936539696\n",
      "in attempt 985 loss : 57.36793093923411, best: 44.83717936539696\n",
      "in attempt 986 loss : 62.30195160615863, best: 44.83717936539696\n",
      "in attempt 987 loss : 53.91572888484628, best: 44.83717936539696\n",
      "in attempt 988 loss : 66.29733737326136, best: 44.83717936539696\n",
      "in attempt 989 loss : 53.827126025866995, best: 44.83717936539696\n",
      "in attempt 990 loss : 58.282553202094775, best: 44.83717936539696\n",
      "in attempt 991 loss : 56.58311962705618, best: 44.83717936539696\n",
      "in attempt 992 loss : 58.180127425782, best: 44.83717936539696\n",
      "in attempt 993 loss : 58.447325662258514, best: 44.83717936539696\n",
      "in attempt 994 loss : 52.56929079839386, best: 44.83717936539696\n",
      "in attempt 995 loss : 57.2814795839437, best: 44.83717936539696\n",
      "in attempt 996 loss : 50.90068215155565, best: 44.83717936539696\n",
      "in attempt 997 loss : 62.460132077142575, best: 44.83717936539696\n",
      "in attempt 998 loss : 60.92879243682292, best: 44.83717936539696\n",
      "in attempt 999 loss : 53.903941874068906, best: 44.83717936539696\n",
      "in attempt 1000 loss : 61.54858916402258, best: 44.83717936539696\n"
     ]
    }
   ],
   "source": [
    "# rl = random local in the following code\n",
    "W_rl=np.random.randn(num_labels,num_features)*.01\n",
    "best_loss_rl=float(\"inf\")\n",
    "for i in range(1000):\n",
    "    W_try_rl=W_rl+np.random.randn(num_labels,num_features)*.01\n",
    "    cost_rl=svm_loss(Xtr_rows.T,train_labels,W_try_rl,Lambda)\n",
    "    if cost_rl<best_loss_rl:\n",
    "        best_loss_rl=cost_rl\n",
    "        W_rl=W_try_rl   #updating w\n",
    "    print(f\"in attempt {i+1} loss : {cost_rl}, best: {best_loss_rl}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy using random local search is 10.00\n"
     ]
    }
   ],
   "source": [
    "# accuracy on test scores\n",
    "#score\n",
    "scores_rl=np.dot(W_rl,Xte_rows.T)\n",
    "# find the index with max scores with each column\n",
    "y_pred_rl=np.argmax(scores_rl.shape[0])\n",
    "# find the accuracy\n",
    "acc_rl=np.mean(y_pred_rl==test_labels)\n",
    "print(f\"accuracy using random local search is {acc_rl*100:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A little bit accuracy score is increased. If we increase the number of iteratin we would get W that would give more accuracy score on test data.Try 1000 iterations instead of 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strategy #3: Following the Gradient\n",
    "**Concept** In the previous section we tried to find a direction in the weight-space that would improve our weight vector. We can use gradient to choose the best direction along which we would get W that minimizes the loss mathematically. This direction is related to the gradient of the loss function.\n",
    "\n",
    "The gradient is a generalization of slope for functions that don’t take a single number but a vector of numbers. Mathematical expression for the derivative of a 1-D function,\n",
    " $\\frac{df(x)}{dx} = \\lim_{h\\ \\to 0} \\frac{f(x + h) - f(x)}{h}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Computing the gradient\n",
    "Two ways-\n",
    " - **Numerical Gradient**: fast,easy,approximate\n",
    " - **Analytic Gradient**:fast,exact,most error prone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Computing the gradient numerically with **finite** differences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def eval_numerical_gradient(f, x):\n",
    "#   \"\"\"\n",
    "#   a naive implementation of numerical gradient of f at x\n",
    "#   - f should be a function that takes a single argument\n",
    "#   - x is the point (numpy array) to evaluate the gradient at\n",
    "#   \"\"\"\n",
    "\n",
    "#   fx = f(x) # evaluate function value at original point\n",
    "#   grad = np.zeros(x.shape)\n",
    "#   h = 0.00001\n",
    "\n",
    "#   # iterate over all indexes in x\n",
    "#   it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
    "#   while not it.finished:\n",
    "\n",
    "#     # evaluate function at x+h\n",
    "#     ix = it.multi_index\n",
    "#     old_value = x[ix]\n",
    "#     x[ix] = old_value + h # increment by h\n",
    "#     fxh = f(x) # evalute f(x + h)\n",
    "#     x[ix] = old_value # restore to previous value (very important!)\n",
    "\n",
    "#     # compute the partial derivative\n",
    "#     grad[ix] = (fxh - fx) / h # the slope\n",
    "#     it.iternext() # step to next dimension\n",
    "\n",
    "#   return grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**practical consideration**\n",
    "- In formula h tnds to zero, Practically a sufficient smaller value works fine. In fact very smaaller value results time consuming in the gradient descent algorithm which we will see later\n",
    "- it often works better to compute the numeric gradient using the centered difference formula:\n",
    "$[f(x+h) - f(x-h)] / 2 h$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # to use the generic code above we want a function that takes a single argument\n",
    "# # (the weights in our case) so we close over X_train and Y_train\n",
    "# def CIFAR10_loss_fun(W):\n",
    "#   return L(Xtr_rows.T,train_labels, W)\n",
    "\n",
    "# W = np.random.rand(10, 3073) * 0.001 # random weight vector\n",
    "# df = eval_numerical_gradient(CIFAR10_loss_fun, W) # get the gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #update\n",
    "# loss_original = CIFAR10_loss_fun(W) # the original loss\n",
    "# print 'original loss: %f' % (loss_original, )\n",
    "\n",
    "# # lets see the effect of multiple step sizes\n",
    "# for step_size_log in [-10, -9, -8, -7, -6, -5,-4,-3,-2,-1]:\n",
    "#   step_size = 10 ** step_size_log\n",
    "#   W_new = W - step_size * df # new position in the weight space\n",
    "#   loss_new = CIFAR10_loss_fun(W_new)\n",
    "#   print 'for step size %f new loss: %f' % (step_size, loss_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***NB***: I am leaving executing the above three block of code considering numerical gradient. Because this will require tremendous calculations :( . Later I will try"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**update in negative direction**: We update W in the negative direction of the gradient since we wish our loss functions to decrease,not increase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Effect of step size(learning rate):\n",
    "- if step_size/learning_rate/alpha is too small it takes long time to train\n",
    "- if too large algorithm may diverge rather than converge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem of efficiency**: \n",
    "- complexity of numerical gradient is the number of parameters.\n",
    "-  In our example we had 30730 parameters in total and therefore had to perform 30,731 evaluations of the loss function to evaluate the gradient and to perform only a single parameter update\n",
    "\n",
    "In real life we work with data having millions of parameters. In that case its good to avoid this algorithm look for new way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Computing the gradient analytically with Calculus\n",
    "**properties**\n",
    "- fast, no approximation, error prone\n",
    "\n",
    "**Gradient Check**: Since analytic approach with calculas is more error prone , in practice it is good to use both approach and see which is better.This is called gradient check.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall SVM loss for a single data point: $L_i = \\sum_{j\\neq y_i} \\left[ \\max(0, w_j^Tx_i - w_{y_i}^Tx_i + \\Delta) \\right]$\n",
    "\n",
    "We can differentiate the function with respect to the weights. For example, taking the gradient with respect to wyi\n",
    " we obtain: $\\nabla_{w_{y_i}} L_i = - \\left( \\sum_{j\\neq y_i} \\mathbb{1}(w_j^Tx_i - w_{y_i}^Tx_i + \\Delta > 0) \\right) x_i$\n",
    " where 1\n",
    " is the indicator function that is one if the condition inside is true or zero otherwise. While the expression may look scary when it is written out, when you’re implementing this in code you’d simply **count the number of classes that didn’t meet the desired margin** (and hence contributed to the loss function) and then the data vector xi\n",
    " scaled by this number is the gradient. Notice that this is the gradient only with respect to the row of W\n",
    " that corresponds to the correct class. For the other rows where j≠yi\n",
    " the gradient is:\n",
    "$\\nabla_{w_j} L_i = \\mathbb{1}(w_j^Tx_i - w_{y_i}^Tx_i + \\Delta > 0) x_i$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Implementation**:\n",
    " **algorithm**: \n",
    " - count the #classes that don't meet the margin then multiply by that training example ($x_i$). -  - Iterate over all the training example through $x_i  \\text{to}  x_n$ \n",
    " - sum the all classes that don't meet margin from $x_i \\text{to} x_n$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4.865684e+06 5.063405e+06 4.890451e+06 ... 4.705953e+06 4.226599e+06\n",
      "  3.796900e+04]\n",
      " [7.333060e+05 7.542810e+05 7.233800e+05 ... 9.088710e+05 7.975300e+05\n",
      "  7.101000e+03]\n",
      " [4.183433e+06 4.345228e+06 4.252617e+06 ... 4.032097e+06 3.636106e+06\n",
      "  3.193100e+04]\n",
      " ...\n",
      " [2.286263e+06 2.361888e+06 2.308817e+06 ... 2.172732e+06 2.000192e+06\n",
      "  1.707700e+04]\n",
      " [3.419451e+06 3.523091e+06 3.386327e+06 ... 3.415164e+06 3.021496e+06\n",
      "  2.627800e+04]\n",
      " [5.534420e+05 5.657700e+05 5.518290e+05 ... 4.317370e+05 3.971320e+05\n",
      "  3.693000e+03]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def calculate_gradient(X, y, W, delta):\n",
    "    num_classes = W.shape[0]\n",
    "    num_samples = X.shape[1]\n",
    "\n",
    "    scores = np.dot(W, X)\n",
    "\n",
    "    margins = np.maximum(0, scores - scores[y, np.arange(num_samples)] + delta)\n",
    "    margins[y, np.arange(num_samples)] = 0  # Ignore the correct class\n",
    "\n",
    "    incorrect_mask = margins > 0\n",
    "\n",
    "    gradient = np.zeros_like(W)\n",
    "\n",
    "    # Compute the gradient for the correct class weight\n",
    "    num_incorrect = np.sum(incorrect_mask, axis=0)\n",
    "    gradient[y, :] = -num_incorrect[:, np.newaxis] * X.T\n",
    "\n",
    "    # Compute the gradient for the incorrect class weights\n",
    "    gradient += np.dot(incorrect_mask, X.T)\n",
    "\n",
    "    return gradient\n",
    "\n",
    "#Example usage:\n",
    "# X = np.array([[1, 2], [2, 3], [4, 5], [5, 6], [7, 8], [9, 10]])  # Example input matrix with shape (num_features, num_samples)\n",
    "# y = np.array([0, 1, 2, 0, 1, 2])  # Example target classes\n",
    "# W = np.random.randn(3, 2) * 0.001  # Example weight matrix\n",
    "W_gd=np.random.randn(10,3073)*.001\n",
    "delta = .1  # Example margin value\n",
    "gradient = calculate_gradient(Xtr_rows.T, train_labels, W_gd, delta)\n",
    "print(gradient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 3073)\n"
     ]
    }
   ],
   "source": [
    "print(gradient.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent: \n",
    " the procedure of repeatedly evaluating the gradient and then performing a parameter update is called Gradient Descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initializing parameters\n",
    "def gradient_descent(X,y,learning_rate,num_iters):\n",
    "    #initializing\n",
    "    Lamda=.4\n",
    "    delta=.1\n",
    "    W = np.random.randn(10, 3073) * 0.001\n",
    "    for i in range(num_iters):\n",
    "        grad=calculate_gradient(X,y,W,delta)\n",
    "        #update\n",
    "        W-=learning_rate*grad\n",
    "        # print loss and accuracy after certain periods\n",
    "        if (i+1)%25==0:\n",
    "\n",
    "            loss=svm_loss(X,y,W,Lambda)\n",
    "            scores=np.dot(W,X)\n",
    "            y_pred=np.argmax(scores.shape[0])\n",
    "            accuracy=np.mean(y_pred==y)\n",
    "            print(f\"iteration {i+1}, loss={loss:.2f}, accuracy={accuracy*100:.2f}\")\n",
    "    return W\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 25, loss=13801092198788.98, accuracy=10.00\n",
      "iteration 50, loss=53989984404179.52, accuracy=10.00\n",
      "iteration 75, loss=118530384972456.03, accuracy=10.00\n",
      "iteration 100, loss=209000555105055.50, accuracy=10.00\n",
      "iteration 125, loss=324142878751814.25, accuracy=10.00\n",
      "iteration 150, loss=461961483910127.75, accuracy=10.00\n",
      "iteration 175, loss=625132659902101.88, accuracy=10.00\n",
      "iteration 200, loss=812630889927441.62, accuracy=10.00\n",
      "iteration 225, loss=1025912810030875.88, accuracy=10.00\n",
      "iteration 250, loss=1262053288729738.75, accuracy=10.00\n",
      "iteration 275, loss=1522457540541227.75, accuracy=10.00\n",
      "iteration 300, loss=1806741478237227.50, accuracy=10.00\n",
      "iteration 325, loss=2112662560771676.25, accuracy=10.00\n",
      "iteration 350, loss=2441638021681355.50, accuracy=10.00\n",
      "iteration 375, loss=2790676688972976.00, accuracy=10.00\n",
      "iteration 400, loss=3159270009080922.00, accuracy=10.00\n"
     ]
    }
   ],
   "source": [
    "#perform gradient descent\n",
    "num_iters=400\n",
    "learning_rate=.01\n",
    "final_weights=gradient_descent(Xtr_rows.T,train_labels,learning_rate,num_iters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "performance on the test data using the final weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1\n"
     ]
    }
   ],
   "source": [
    "sc=np.dot(final_weights,Xte_rows.T)\n",
    "y_pred=np.argmax(sc.shape[0])\n",
    "accuracy=np.mean(y_pred==test_labels)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we need to work further to increase accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **mini-batch gradient descent:\n",
    "we can split the dataset inot sets. and can apply gradinet descent algorithm on each batch. The extereme case when batch size =1. This condition is known as stochastic gradient descent(SGD).\n",
    "This is relatively less common to see because in practice due to vectorized code optimizations it can be computationally much more efficient to evaluate the gradient for 100 examples, than the gradient for one example 100 times. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
