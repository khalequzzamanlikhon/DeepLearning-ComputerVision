{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Classification\n",
    "In the last section we introduced image classification using K-NN algorithm. K-NN has disadvantages.\n",
    " - The classifier must remember all of the training data and store it for future comparisons with the test data. This is space inefficient because datasets may easily be gigabytes in size.\n",
    " - Classifying a test image is expensive since it requires a comparison to all training images.\n",
    "\n",
    "\n",
    " **Overview**:\n",
    "  We are now going to develop a more powerful approach to image classification that we will eventually naturally extend to entire Neural Networks and Convolutional Neural Networks. The approach will have two major components: a score function that maps the raw data to class scores, and a loss function that quantifies the agreement between the predicted scores and the ground truth labels. We will then cast this as an optimization problem in which we will minimize the loss function with respect to the parameters of the score function.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameterized mapping from images to label scores\n",
    "- **score** function\n",
    "- **loss** function\n",
    "\n",
    "let’s assume a training dataset of images $x_i \\in R^D$\n",
    ", each associated with a label yi\n",
    ". Here $ i=1…N $\n",
    " and $ yi∈1…K $\n",
    ". That is, we have N examples (each with a dimensionality D) and K distinct categories. For example, in CIFAR-10 we have a training set of N = 50,000 images, each with D = 32 x 32 x 3 = 3072 pixels, and K = 10, since there are 10 distinct classes (dog, cat, car, etc). We will now define the score function $ f:R^D\\mapsto R^K$\n",
    " that maps the raw image pixels to class scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Classifier\n",
    " $ f(x_i, W, b) =  W x_i + b$\n",
    "\n",
    "\n",
    " In the above equation, we are assuming that the image xi\n",
    " has all of its pixels flattened out to a single column vector of shape [D x 1]. The matrix W (of size [K x D]), and the vector b (of size [K x 1]) are the parameters of the function. In CIFAR-10, xi\n",
    " contains all pixels in the i-th image flattened into a single [3072 x 1] column, W is [10 x 3072] and b is [10 x 1], so 3072 numbers come into the function (the raw pixel values) and 10 numbers come out (the class scores). The parameters in W are often called the weights, and b is called the bias vector because it influences the output scores, but without interacting with the actual data xi\n",
    ". However, you will often hear people use the terms weights and parameters interchangeably."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpreting a linear classifier as template matching\n",
    "- each row of weight vector is responsible for a specified class. Therefore each row can be said to be a template.\n",
    "\n",
    "- To label an image we need to comparee the image with all the template and see which template matches with the image.\n",
    "\n",
    "**Problem**\n",
    "- ex1: suppose a dataset has different color of image and having maximum colors with red. Then the template of car class would mostly recognize red car but not the cars of other color.\n",
    "\n",
    "**solution**\n",
    "- introducing neural network.a neural network will be able to develop intermediate neurons in its hidden layers that could detect specific car types (e.g. green car facing left, blue car facing front, etc.), and neurons on the next layer could combine these into a more accurate car score through a weighted sum of the individual car detectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bias trick\n",
    " it is cumbersome to keep track of two sets of parameters. Therefore we combine these two into one. For this-\n",
    "\n",
    " -by extending the vector xi\n",
    " with one additional dimension that always holds the constant 1 and a default bias dimension.\n",
    "\n",
    " Now the score function looks like:  $f(x_i,W)=W x_i$\n",
    "\n",
    " **New Dimensions**\n",
    " $x_i$ =[3073 X 1] , $W$=[10 X 3073]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image data processing\n",
    "Its always recommended to normalize data before training.\n",
    "\n",
    "**Concept**: Centering the dataset. subtracting the mean from every feature. \n",
    "- range of the values of dataset:\n",
    "\n",
    "        - [-1,1] :zero mean centering\n",
    "        - [-127,127] we will do in each image processing\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss and Optimization\n",
    "## Loss\n",
    "In the previous lecture we learned to work with data having a set of hyperpaarameters. For linear classifier weights are hyperparameters. For different set of hyperparameters we get different predictions. The questions is , which hyperparameters should we use in our model. In other words, which parameter set is best to use in our model. To get intuition about this question \"loss\" comes to the picture. In machine learning we want our model to predict as much as perfect. Say we want our a model to classify an image as cat. We want our model that every time it encounters an image it correctly predict as cat or not a cat. When model fails to predict correctly we consider it as loss. Therefore we can define loss as the difference between the predicted value and the actual value for a single example. Now the answer is, we will select that hyperparameter set that results the least loss.\n",
    "\n",
    "##### Types of loss function\n",
    "There are different types of loss function. Depending on the problem set and algorithm we use. In image classification for multiclass problem we can use two types of loss function.\n",
    "  - Multiclass Support Vector Machine Loss\n",
    "  - Cross Entropy Loss\n",
    "**Multiclass SVM loss(hinge loss)**:\n",
    "    - **Goal**:  The SVM loss is set up so that the SVM “wants” the correct class for each image to a have a score higher than the incorrect classes by some fixed margin Δ\n",
    "\n",
    "    $L_i = \\sum_{j\\neq y_i} \\max(0, s_j - s_{y_i} + \\Delta)$\n",
    "    \n",
    "  Since we are working with linear score function we can write it as: $L_i = \\sum_{j\\neq y_i} \\max(0, w_j^T x_i - w_{y_i}^T x_i + \\Delta)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation of loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_i_unvectorized(x, y, W):\n",
    "  \"\"\"\n",
    "  unvectorized version. Compute the multiclass svm loss for a single example (x,y)\n",
    "  - x is a column vector representing an image (e.g. 3073 x 1 in CIFAR-10)\n",
    "    with an appended bias dimension in the 3073-rd position (i.e. bias trick)\n",
    "  - y is an integer giving index of correct class (e.g. between 0 and 9 in CIFAR-10)\n",
    "  - W is the weight matrix (e.g. 10 x 3073 in CIFAR-10)\n",
    "  \"\"\"\n",
    "  delta = 1.0 # see notes about delta later in this section\n",
    "  scores = W.dot(x) # scores becomes of size 10 x 1, the scores for each class\n",
    "  correct_class_score = scores[y]\n",
    "  D = W.shape[0] # number of classes, e.g. 10\n",
    "  loss_i = 0.0\n",
    "  for j in range(D): # iterate over all wrong classes\n",
    "    if j == y:\n",
    "      # skip for the true class to only loop over incorrect classes\n",
    "      continue\n",
    "    # accumulate loss for the i-th example\n",
    "    loss_i += max(0, scores[j] - correct_class_score + delta)\n",
    "  return loss_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_i_vectorized(x, y, W):\n",
    "  \"\"\"\n",
    "  A faster half-vectorized implementation. half-vectorized\n",
    "  refers to the fact that for a single example the implementation contains\n",
    "  no for loops, but there is still one loop over the examples (outside this function)\n",
    "  \"\"\"\n",
    "  delta = 1.0\n",
    "  scores = W.dot(x)\n",
    "  # compute the margins for all classes in one vector operation\n",
    "  margins = np.maximum(0, scores - scores[y] + delta)\n",
    "  # on y-th position scores[y] - scores[y] canceled and gave delta. We want\n",
    "  # to ignore the y-th position and only consider margin on max wrong class\n",
    "  margins[y] = 0\n",
    "  loss_i = np.sum(margins)\n",
    "  return loss_i\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "## Fully vectorized implementation\n",
    "def L(X,y,W):\n",
    "     \n",
    "     \"\"\"\n",
    "    Fully-vectorized implementation of the loss function.\n",
    "    - X: Holds all the training examples as columns (e.g., 3073 x 50,000 in CIFAR-10)\n",
    "    - y: Array of integers specifying the correct class (e.g., 50,000-D array)\n",
    "    - W: Weights (e.g., 10 x 3073)\n",
    "    \"\"\"\n",
    "     #compute scores for all classes\n",
    "     scores=np.dot(W,X)\n",
    "\n",
    "     #select only the scores for the correct class for each example\n",
    "     correct_class_scores=scores[y,np.arange(X.shape[1])] \n",
    "     #compute the hinge loss for all classes\n",
    "     margins=np.maximum(0,scores-correct_class_scores+1)\n",
    "     margins[y,np.arange(X.shape[1])]=0  #set loss for correct class 0\n",
    "\n",
    "     #compute the overall loss as the average of all hinge losses\n",
    "     loss=np.mean(margins)\n",
    "     return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q&A(Multiclass SVM loss function)\n",
    "\n",
    "**Q1**: Suppose we get L_i=0 for a specified class. What happens if we change a littele bit score corresponding to this class?\n",
    "\n",
    " - **Answer**: The loss will still be same(0 in this case). Because its score value is already greter than the other scores of other classes.\n",
    "\n",
    "**Q2**:Whata is the min max possibe loss?\n",
    "\n",
    " - **Answer**: Min=0 and max = infinity. \n",
    "      - Min: Because if the differences between the scores is negative then the function returns max value zero.\n",
    "      - Max: Because from the equation we can say that if the correct scores gets very very negative score. Then we accumulate infinity loss.\n",
    "\n",
    "**Q3**: At initialization Wis small so all s ≈ 0.What is the loss? \n",
    " - **Answer**: Number of classes minus one (C-1). The predicted score and the original class score will be same .therfore we will get error 1 for the delta term. And remember we don't calculate for when  $j=y_i$. Thats why minus one.\n",
    "\n",
    "**Q4**:What if the sumwas over all classes?(including $j = y_i$)?\n",
    " - **Answer**: Loss increases by 1. Because for this correct class loss=0 but we get the delta value 1.\n",
    "\n",
    "**Q5**:What if we used mean instead ofsum? ?\n",
    " - **Answer**: Doesn't change. because the number of classes remains constant all the time. It only rescale the loss.\n",
    "\n",
    "**Q6**:What if we use suared of the loss function ?\n",
    " - **Answer**: would be a different loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization\n",
    "\n",
    "Why do we use regularization?\n",
    "- To avoid overfitting \n",
    "- To generalize\n",
    "- model should be simple.So that works better on test data\n",
    "\n",
    "**Concept**:\n",
    "When the number of features is high then the training data fits the model perfectly. But doesn't show a good result in test case. This phenomenon is known as overfitting(High Variance). We know that the trainig examples ($x_i,y_i$) is fixed. We can not change them. Thus some features may have values much more higher than other features. So these exteme features can have great impact on the score function. To overcome this situation we can only penalize thsese values only by penalizing the wieghts. Because by penalizing weights we can control features. There are many regularized loss term availabe. But we will use squared L2 regularized term. $R(W) = \\sum_k\\sum_l W_{k,l}^2$. Byadding this term to loss function we have now two parts in loss function. One is **data loss** and the **regularization loss**\n",
    "\n",
    "$L =  \\underbrace{ \\frac{1}{N} \\sum_i L_i }_\\text{data loss} + \\underbrace{ \\lambda R(W) }_\\text{regularization loss} \\\\\\\\$\n",
    "\n",
    "Expanding this out in its full form:\n",
    "$L = \\frac{1}{N} \\sum_i \\sum_{j\\neq y_i} \\left[ \\max(0, f(x_i; W)_j - f(x_i; W)_{y_i} + \\Delta) \\right] + \\lambda \\sum_k\\sum_l W_{k,l}^2$\n",
    "\n",
    "**Example** Suppose we have $x=[1,1,1,1]$ ,$w1=[1,0,0,0]$, $w2=[0.25,0.25,0.25,0.25]$. \n",
    "Here,$w_1^Tx = w_2^Tx = 1$ But according to L2 regularization w2 is preferable. Because the sum of the suqred values of w2(.5)< w1(1). This demonstrates that regularized loss for w2 is less than w1.\n",
    "\n",
    "**Lambda**: Hyperparameters which plays role for trade off between data loss and regualrization loss.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implemetation of loss with regularized term\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#unvectorized\n",
    "#same as before only regularized term will be added to the loss\n",
    "Lambda=.4\n",
    "\n",
    "def L_regularized(x, y, W,Lambda):\n",
    "  \"\"\"\n",
    "  unvectorized version. Compute the multiclass svm loss for a single example (x,y)\n",
    "  - x is a column vector representing an image (e.g. 3073 x 1 in CIFAR-10)\n",
    "    with an appended bias dimension in the 3073-rd position (i.e. bias trick)\n",
    "  - y is an integer giving index of correct class (e.g. between 0 and 9 in CIFAR-10)\n",
    "  - W is the weight matrix (e.g. 10 x 3073 in CIFAR-10)\n",
    "  \"\"\"\n",
    "  delta = 1.0 # see notes about delta later in this section\n",
    "  scores = np.dot(np.transpose(W),x) # scores becomes of size 10 x 1, the scores for each class\n",
    "  correct_class_score = scores[y]\n",
    "  D = W.shape[0] # number of classes, e.g. 10\n",
    "  loss_i = 0.0\n",
    "  for j in range(D): # iterate over all wrong classes\n",
    "    if j == y:\n",
    "      # skip for the true class to only loop over incorrect classes\n",
    "      continue\n",
    "    # accumulate loss for the i-th example\n",
    "    loss_i += max(0, scores[j] - correct_class_score + delta)\n",
    "  regularized_loss=loss_i+ .05*Lambda*np.sum(W*W) #.5 is multiplied due to convenience of calculation later\n",
    "  return regularized_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**other Multiclass SVM formulations:**\n",
    "- OneVsAll\n",
    "  -  trains an independent binary SVM for each class vs. all other classes.\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax Classifier (Multinomial Logistic Rregression)\n",
    "the Softmax classifier is generalization of binary logistic regression classifier to multiple classes.\n",
    "scores=unnormalized log probabilities of the classse  ; $s=f(x_i,W)$\n",
    "**Steps**\n",
    "  - we take the score from $x=f(x_i,W)$\n",
    "  - we exponentiate them so that they become positive\n",
    "  - we normalize them by sum of the exponents\n",
    "\n",
    "These three steps is done by a function named softmax function. This function ends up to a vector of probabilities corresponding to each classes. The sum of all the elements is 1.\n",
    "\n",
    "$P(Y=k\\ X=x_i)= \\frac{e^{s_k}}{\\sum_k e^{s_j}}$ ; outputs the pobability of class k (k=1 to K)\n",
    "\n",
    "Here, $f_j(z) = \\frac{e^{z_j}}{\\sum_k e^{z_k}}$ is the softmax function\n",
    "\n",
    "\n",
    "##### Softmax loss Function(Cross Entropy Loss):\n",
    "  **Target** : Want to maximize the log likelihood, or (for a loss function) to minimize the negative log likelihood of the correct class. In other words the loss fucntion tries to make the correct class probability near to 1 and the other class probability near to 0. Thus the loss functin is Negative log of the probability of the correct class.\n",
    "  $L_i = -\\log\\left(\\frac{e^{f_{y_i}}}{ \\sum_j e^{f_j} }\\right) \\hspace{0.5in} \\text{or equivalently} \\hspace{0.5in} L_i = -f_{y_i} + \\log\\sum_j e^{f_j}$\n",
    "  \n",
    "  The equation says, minimize the negative log likelihood of the correct class which can be interpreted as performing Maximum Likelihood Estimation (MLE).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Practical Issues:\n",
    "The intermediate terms $e^{f_{y_i}}$ and $ \\sum_j e^{f_j}$  may be very large due to the exponentials. Dividing large numbers can be numerically unstable. for stability we multiply neumeratkor and denominator by a constant C and we get,\n",
    "$\\frac{e^{f_{y_i}}}{\\sum_j e^{f_j}}\n",
    "= \\frac{Ce^{f_{y_i}}}{C\\sum_j e^{f_j}}\n",
    "= \\frac{e^{f_{y_i} + \\log C}}{\\sum_j e^{f_j + \\log C}}$\n",
    "\n",
    "**Value of C**: we can take any value. A common choice is to set $logC= -max_j f_j$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\klikh\\AppData\\Local\\Temp\\ipykernel_7192\\866872901.py:2: RuntimeWarning: overflow encountered in exp\n",
      "  p = np.exp(f) / np.sum(np.exp(f)) # Bad: Numeric problem, potential blowup\n",
      "C:\\Users\\klikh\\AppData\\Local\\Temp\\ipykernel_7192\\866872901.py:2: RuntimeWarning: invalid value encountered in divide\n",
      "  p = np.exp(f) / np.sum(np.exp(f)) # Bad: Numeric problem, potential blowup\n"
     ]
    }
   ],
   "source": [
    "f = np.array([123, 456, 789]) # example with 3 classes and each having large scores\n",
    "p = np.exp(f) / np.sum(np.exp(f)) # Bad: Numeric problem, potential blowup\n",
    "\n",
    "# instead: first shift the values of f so that the highest number is 0:\n",
    "f -= np.max(f) # f becomes [-666, -333, 0]\n",
    "p = np.exp(f) / np.sum(np.exp(f)) # safe to do, gives the correct answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naming convention:\n",
    " Multiclass SVM loss/ hinge loss is sometimes called max-margin loss. Technically softmax function is a squashing function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM vs Softmax:\n",
    "**SVM**\n",
    " - through score function wants that the correct class score value is higher than the other class scores by a margin.\n",
    " -The SVM does not care about the details of the individual scores\n",
    "**Softmax**\n",
    " - The Softmax classifier instead interprets the scores as (unnormalized) log probabilities for each class and then encourages the (normalized) log probability of the correct class to be high (equivalently the negative of it to be low)\n",
    " - the Softmax classifier allows us to compute “probabilities” for all labels. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization\n",
    "\n",
    "### Introduction:\n",
    "\n",
    "**linear score function** : $f(x_i, W) =  W x_i$\n",
    "\n",
    "**SVM loss**: $L=1N∑i∑j≠yi[max(0,f(xi;W)j−f(xi;W)yi+1)]+αR(W)$\n",
    "\n",
    "We saw that a setting of the parameters Wthat produced predictions for examples xiconsistent with their ground truth labels yiwould also have a very low loss L. We are now going to introduce the third and last key component: optimization. Optimization is the process of finding the set of parameters W that minimize the loss function.\n",
    "\n",
    "**Goal**: understanding the interaction among **score** function, **loss** fucntion and **optimization** these three components."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualising the loss function\n",
    "\n",
    "$L_i = \\sum_{j\\neq y_i} \\left[ \\max(0, w_j^Tx_i - w_{y_i}^Tx_i + 1) \\right]$\n",
    "\n",
    "It is clear from the equation that the data loss for each example is a sum of (zero-thresholded due to the max(0,−) function) linear functions of W\n",
    "\n",
    "**Sign in front of W**:\n",
    "- +ve sign: it means that the value of weight is positively corelated with the likelihood of the corresponding class. In other words increasing the value of w will increase the chances of being classified as the correct class.\n",
    "- -ve sign: weights are negatively correlated.\n",
    "\n",
    "The above equation involves data loss and thresholding, the sign of the weight determines whether the contribution of that weight is considered in the calculation of the loss. Positive weights associated with incorrect class labels (where the model predicted the wrong class) contribute positively to the data loss. On the other hand, negative weights associated with correct class labels contribute negatively to the data loss, reducing it.\n",
    "\n",
    "**Shape of loss function**:\n",
    "\n",
    "SVM loss is convex function. Once we extend our score functions f to Neural Networks our objective functions will become non-convex, and the visualizations of those non-convex function will not feature bowls but complex, bumpy terrains.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization\n",
    "\n",
    "**Goal** : The goal of optimization is to find W that minimizes the loss function. We will now motivate and slowly develop an approach to optimizing the loss function. There are different optimization function based on the type of loss function. For example, for convex function we can use gradient optimization. Our final goal is to optimize neural networks where we can't easily find any of the tools developed in the convex optimization literature.\n",
    "\n",
    "First we will talk for convex function. In this case for hinge loss.\n",
    "Approaches we will try-\n",
    " - Random Search\n",
    " - Random Local Search\n",
    " - Following the gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Strategy #1: A first very bad idea solution: Random search\n",
    "Since it is so simple to check how good a given set of parameters W is, the first (very bad) idea that may come to mind is to simply try out many different random weights and keep track of what works best. This procedure might look as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we need to import our data and split it into train test\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpickle(file):\n",
    "    \"\"\"load the cifar-10 data\"\"\"\n",
    "\n",
    "    with open(file, 'rb') as fo:\n",
    "        data = pickle.load(fo, encoding='bytes')\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_cifar_10_data(data_dir, negatives=False):\n",
    "    \"\"\"\n",
    "    Return train_data, train_filenames, train_labels, test_data, test_filenames, test_labels\n",
    "    \"\"\"\n",
    "\n",
    "    # get the meta_data_dict\n",
    "    # num_cases_per_batch: 1000\n",
    "    # label_names: ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "    # num_vis: :3072\n",
    "\n",
    "    meta_data_dict = unpickle(data_dir + \"/batches.meta\")\n",
    "    cifar_label_names = meta_data_dict[b'label_names']\n",
    "    cifar_label_names = np.array(cifar_label_names)\n",
    "\n",
    "    # training data\n",
    "    cifar_train_data = None\n",
    "    cifar_train_filenames = []\n",
    "    cifar_train_labels = []\n",
    "\n",
    "    # cifar_train_data_dict\n",
    "    # 'batch_label': 'training batch 5 of 5'\n",
    "    # 'data': ndarray\n",
    "    # 'filenames': list\n",
    "    # 'labels': list\n",
    "\n",
    "    for i in range(1, 6):\n",
    "        cifar_train_data_dict = unpickle(data_dir + \"/data_batch_{}\".format(i))\n",
    "        if i == 1:\n",
    "            cifar_train_data = cifar_train_data_dict[b'data']\n",
    "        else:\n",
    "            cifar_train_data = np.vstack((cifar_train_data, cifar_train_data_dict[b'data']))\n",
    "        cifar_train_filenames += cifar_train_data_dict[b'filenames']\n",
    "        cifar_train_labels += cifar_train_data_dict[b'labels']\n",
    "\n",
    "    cifar_train_data = cifar_train_data.reshape((len(cifar_train_data), 3, 32, 32))\n",
    "    if negatives:\n",
    "        cifar_train_data = cifar_train_data.transpose(0, 2, 3, 1).astype(np.float32)\n",
    "    else:\n",
    "        cifar_train_data = np.rollaxis(cifar_train_data, 1, 4)\n",
    "    cifar_train_filenames = np.array(cifar_train_filenames)\n",
    "    cifar_train_labels = np.array(cifar_train_labels)\n",
    "\n",
    "    # test data\n",
    "    # cifar_test_data_dict\n",
    "    # 'batch_label': 'testing batch 1 of 1'\n",
    "    # 'data': ndarray\n",
    "    # 'filenames': list\n",
    "    # 'labels': list\n",
    "\n",
    "    cifar_test_data_dict = unpickle(data_dir + \"/test_batch\")\n",
    "    cifar_test_data = cifar_test_data_dict[b'data']\n",
    "    cifar_test_filenames = cifar_test_data_dict[b'filenames']\n",
    "    cifar_test_labels = cifar_test_data_dict[b'labels']\n",
    "\n",
    "    cifar_test_data = cifar_test_data.reshape((len(cifar_test_data), 3, 32, 32))\n",
    "    if negatives:\n",
    "        cifar_test_data = cifar_test_data.transpose(0, 2, 3, 1).astype(np.float32)\n",
    "    else:\n",
    "        cifar_test_data = np.rollaxis(cifar_test_data, 1, 4)\n",
    "    cifar_test_filenames = np.array(cifar_test_filenames)\n",
    "    cifar_test_labels = np.array(cifar_test_labels)\n",
    "\n",
    "    return cifar_train_data, cifar_train_filenames, cifar_train_labels, \\\n",
    "        cifar_test_data, cifar_test_filenames, cifar_test_labels, cifar_label_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data:  (50000, 32, 32, 3)\n",
      "Train filenames:  (50000,)\n",
      "Train labels:  (50000,)\n",
      "Test data:  (10000, 32, 32, 3)\n",
      "Test filenames:  (10000,)\n",
      "Test labels:  (10000,)\n",
      "Label names:  (10,)\n"
     ]
    }
   ],
   "source": [
    "cifar_10_dir = 'dataset\\cifar10'\n",
    "train_data, train_filenames, train_labels, test_data, test_filenames, test_labels, label_names = load_cifar_10_data(cifar_10_dir)\n",
    "print(\"Train data: \", train_data.shape)\n",
    "print(\"Train filenames: \", train_filenames.shape)\n",
    "print(\"Train labels: \", train_labels.shape)\n",
    "print(\"Test data: \", test_data.shape)\n",
    "print(\"Test filenames: \", test_filenames.shape)\n",
    "print(\"Test labels: \", test_labels.shape)\n",
    "print(\"Label names: \", label_names.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Flatten out all the images to be one dimensional\n",
    "Xtr_rows = train_data.reshape(train_data.shape[0], 32 * 32 * 3) # Xtr_rows becomes 50000 x 3072\n",
    "Xte_rows = test_data.reshape(test_data.shape[0], 32 * 32 * 3) # Xte_rows becomes 10000 x 3072"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 3072)\n",
      "(10000, 3072)\n"
     ]
    }
   ],
   "source": [
    "print(Xtr_rows.shape)\n",
    "print(Xte_rows.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding bias term\n",
    "bias_term=np.ones((50000,1))\n",
    "#np.expand_dims(bias_term,axis=0)\n",
    "Xtr_rows=np.hstack((Xtr_rows,bias_term))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 3073)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtr_rows.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hinge_loss(x, y, W, reg_strength):\n",
    "    scores = np.dot(W, x)  # Compute scores\n",
    "    correct_class_score = scores[y]\n",
    "    margins = np.maximum(0, scores - correct_class_score + 1)  # Compute margins\n",
    "    margins[y] = 0  # Ignore the margin for the correct class\n",
    "    loss = np.sum(margins)  # Compute the loss\n",
    "\n",
    "    # Compute the gradient\n",
    "    num_classes = W.shape[0]\n",
    "    num_features = x.shape[0]\n",
    "    # dW = np.zeros_like(W)\n",
    "    # for c in range(num_classes):\n",
    "    #     if c == y:\n",
    "    #         continue\n",
    "    #     margin = margins[c]\n",
    "    #     if margin > 0:\n",
    "    #         dW[c] += x\n",
    "    #         dW[y] -= x\n",
    "\n",
    "    # Average loss and gradient over all examples\n",
    "    loss /= num_classes\n",
    "    # dW /= num_classes\n",
    "\n",
    "    # Add regularization to the loss and gradient\n",
    "    loss += 0.5 * reg_strength * np.sum(W * W)\n",
    "    # dW += reg_strength * W\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In attempt 0, the loss was 1.000024, best 1.000024\n",
      "In attempt 1, the loss was 0.983465, best 0.983465\n",
      "In attempt 2, the loss was 1.074962, best 0.983465\n",
      "In attempt 3, the loss was 1.004174, best 0.983465\n",
      "In attempt 4, the loss was 0.985748, best 0.983465\n",
      "In attempt 5, the loss was 0.929322, best 0.929322\n",
      "In attempt 6, the loss was 1.012900, best 0.929322\n",
      "In attempt 7, the loss was 0.989676, best 0.929322\n",
      "In attempt 8, the loss was 1.007881, best 0.929322\n",
      "In attempt 9, the loss was 0.987415, best 0.929322\n"
     ]
    }
   ],
   "source": [
    "Lambda = 0.4\n",
    "bestloss = float(\"inf\")  # Python assigns the highest possible float value\n",
    "num_classes = 10\n",
    "num_features = Xtr_rows.shape[1]\n",
    "\n",
    "for num in range(10):\n",
    "    W = np.random.randn(num_classes, num_features) * 0.0001  # Generate random parameters\n",
    "    loss = 0.0\n",
    "    # dW = np.zeros_like(W)\n",
    "\n",
    "    for i in range(Xtr_rows.shape[0]):\n",
    "        # loss_i, dW_i = hinge_loss(Xtr_rows[i], train_labels[i], W, Lambda)\n",
    "        loss_i= hinge_loss(Xtr_rows[i], train_labels[i], W, Lambda)\n",
    "        loss += loss_i\n",
    "        # dW += dW_i\n",
    "\n",
    "    loss /= Xtr_rows.shape[0]  # Compute average loss over the training set\n",
    "    # dW /= Xtr_rows.shape[0]  # Compute average gradient over the training set\n",
    "\n",
    "    if loss < bestloss:  # Keep track of the best solution\n",
    "        bestloss = loss\n",
    "        bestW = W\n",
    "\n",
    "    print('In attempt %d, the loss was %f, best %f' % (num, loss, bestloss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For time saving we only work for 10 weight matrix. we should try much more. But lets test our test data on the best weights and measure accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_accuracy(X, y, W):\n",
    "    \"\"\"\n",
    "    Predict the labels for the given data and calculate the accuracy.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input data, shape (N, D)\n",
    "    y -- true labels, shape (N, )\n",
    "    W -- weight matrix, shape (C, D)\n",
    "    Returns:\n",
    "    accuracy -- prediction accuracy\n",
    "    \"\"\"\n",
    "    scores = np.dot(W, X.T)  # Compute scores\n",
    "    predicted_labels = np.argmax(scores, axis=0)  # Predict labels\n",
    "    accuracy = np.mean(predicted_labels == y) * 100.0  # Calculate accuracy\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction accuracy: 10.05%\n"
     ]
    }
   ],
   "source": [
    "# Flatten out the test images and add bias term\n",
    "bias_term = np.ones((Xte_rows.shape[0], 1))\n",
    "Xtest_rows = np.hstack((Xte_rows, bias_term))\n",
    "\n",
    "# Predict accuracy\n",
    "accuracy = predict_accuracy(Xtest_rows, test_labels, bestW)\n",
    "print(\"Prediction accuracy: %.2f%%\" % accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this score is really very poor .We could perform better by iterating more to get w. e.g. if we iterated over 1000 times may be we would get a W that would give accuracy more than 10.56"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Core idea: iterative refinement. \n",
    "- The core idea is that finding the best set of weights W is a very difficult or even impossible problem (especially once W contains weights for entire complex neural networks), \n",
    "- our approach will be to start with a random W and then iteratively refine it, making it slightly better each time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strategy #2: Random Local Search\n",
    "**algorithm**:\n",
    "- start out with a random W\n",
    "- generate random perturbations δW to it and if the loss at the perturbed W+δW is lower, we will perform an update. The code for this procedure is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (10,) (50000,) ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 12\u001b[0m\n\u001b[0;32m     10\u001b[0m Wtry \u001b[39m=\u001b[39m W_rls \u001b[39m+\u001b[39m np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mrandn(\u001b[39m10\u001b[39m, \u001b[39m3073\u001b[39m) \u001b[39m*\u001b[39m step_size\n\u001b[0;32m     11\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(Xtr_rows\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]):\n\u001b[1;32m---> 12\u001b[0m    loss_rls_i\u001b[39m=\u001b[39m hinge_loss(Xtr_rows[i],train_labels, Wtry,Lambda)\n\u001b[0;32m     13\u001b[0m    loss_rls\u001b[39m+\u001b[39m\u001b[39m=\u001b[39mloss_rls_i\n\u001b[0;32m     14\u001b[0m loss_rls\u001b[39m/\u001b[39m\u001b[39m=\u001b[39mXte_rows\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n",
      "Cell \u001b[1;32mIn[14], line 4\u001b[0m, in \u001b[0;36mhinge_loss\u001b[1;34m(x, y, W, reg_strength)\u001b[0m\n\u001b[0;32m      2\u001b[0m scores \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mdot(W, x)  \u001b[39m# Compute scores\u001b[39;00m\n\u001b[0;32m      3\u001b[0m correct_class_score \u001b[39m=\u001b[39m scores[y]\n\u001b[1;32m----> 4\u001b[0m margins \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mmaximum(\u001b[39m0\u001b[39m, scores \u001b[39m-\u001b[39;49m correct_class_score \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m)  \u001b[39m# Compute margins\u001b[39;00m\n\u001b[0;32m      5\u001b[0m margins[y] \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m  \u001b[39m# Ignore the margin for the correct class\u001b[39;00m\n\u001b[0;32m      6\u001b[0m loss \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39msum(margins)  \u001b[39m# Compute the loss\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: operands could not be broadcast together with shapes (10,) (50000,) "
     ]
    }
   ],
   "source": [
    "Lambda = 0.4\n",
    "bestloss_rls = float(\"inf\")  # Python assigns the highest possible float value\n",
    "num_classes = 10\n",
    "num_features = Xtr_rows.shape[1]\n",
    "W_rls = np.random.randn(num_classes,num_features) * 0.001 # generate random starting W\n",
    "\n",
    "for i in range(10):\n",
    "  loss_rls=0.0\n",
    "  step_size = 0.0001\n",
    "  Wtry = W_rls + np.random.randn(10, 3073) * step_size\n",
    "  for i in range(Xtr_rows.shape[0]):\n",
    "     loss_rls_i= hinge_loss(Xtr_rows[i],train_labels, Wtry,Lambda)\n",
    "     loss_rls+=loss_rls_i\n",
    "  loss_rls/=Xte_rows.shape[0]\n",
    "  if loss_rls < bestloss_rls:\n",
    "    W_rls = Wtry\n",
    "    bestloss_rls = loss_rls\n",
    "  print('iter %d loss is %f' % (i, bestloss_rls))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
