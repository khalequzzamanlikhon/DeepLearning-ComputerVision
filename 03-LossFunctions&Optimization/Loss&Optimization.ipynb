{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Classification\n",
    "In the last section we introduced image classification using K-NN algorithm. K-NN has disadvantages.\n",
    " - The classifier must remember all of the training data and store it for future comparisons with the test data. This is space inefficient because datasets may easily be gigabytes in size.\n",
    " - Classifying a test image is expensive since it requires a comparison to all training images.\n",
    "\n",
    "\n",
    " **Overview**:\n",
    "  We are now going to develop a more powerful approach to image classification that we will eventually naturally extend to entire Neural Networks and Convolutional Neural Networks. The approach will have two major components: a score function that maps the raw data to class scores, and a loss function that quantifies the agreement between the predicted scores and the ground truth labels. We will then cast this as an optimization problem in which we will minimize the loss function with respect to the parameters of the score function.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameterized mapping from images to label scores\n",
    "- **score** function\n",
    "- **loss** function\n",
    "\n",
    "let’s assume a training dataset of images $x_i \\in R^D$\n",
    ", each associated with a label yi\n",
    ". Here $ i=1…N $\n",
    " and $ yi∈1…K $\n",
    ". That is, we have N examples (each with a dimensionality D) and K distinct categories. For example, in CIFAR-10 we have a training set of N = 50,000 images, each with D = 32 x 32 x 3 = 3072 pixels, and K = 10, since there are 10 distinct classes (dog, cat, car, etc). We will now define the score function $ f:R^D\\mapsto R^K$\n",
    " that maps the raw image pixels to class scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss and Optimization\n",
    "## Loss\n",
    "In the previous lecture we learned to work with data having a set of hyperpaarameters. For linear classifier weights are hyperparameters. For different set of hyperparameters we get different predictions. The questions is , which hyperparameters should we use in our model. In other words, which parameter set is best to use in our model. To get intuition about this question \"loss\" comes to the picture. In machine learning we want our model to predict as much as perfect. Say we want our a model to classify an image as cat. We want our model that every time it encounters an image it correctly predict as cat or not a cat. When model fails to predict correctly we consider it as loss. Therefore we can define loss as the difference between the predicted value and the actual value for a single example. Now the answer is, we will select that hyperparameter set that results the least loss.\n",
    "\n",
    "##### Types of loss function\n",
    "There are different types of loss function. Depending on the problem set and algorithm we use. For linera classifier we use Mean Square Error(MSE).\n",
    "In general, Given a dataset of examples  {(xi,yi)}i=1 to N ,xi is image and yi is label(integer). Loss over the dataset is a sum of loss over examples.\n",
    "\n",
    "Loss = $L[i] =(f(X[i],W),Y[i])$\n",
    "\n",
    "Loss_for_all = $1/N * Sum(Li(f(X[i],W),Y[i])) $\n",
    "\n",
    "\n",
    "Now we will discuss Multiclass SVM loss.\n",
    "##### Multiclass SVM loss:\n",
    "![alt text](lossfunc.png \"loss function\")\n",
    "\n",
    "for a single training example for a specified class level , if the value of specified class level is greater than other values then we consider loss as 0 and a amount of loss otherwise. We compare the difference between specified class value with another value. if the specified class value is greater then we return 0 and the difference+1 otherwise. This type of loss is called **hinge** loss\n",
    "\n",
    "Suppose 3 training example, 3 classes with some W the scoeres f(x,W)=Wx are\n",
    "for demonstration we consider an 2d array. each value of the inner array contains value for cat,car and frog. for each array first,second and third arrays are\n",
    "\n",
    "cat,car, frog=[[3.2,5.1,-1.7],[1.3,4.9,2.0],[2.2,2.5,-3.1]]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_i(x, y, W):\n",
    "  \"\"\"\n",
    "  unvectorized version. Compute the multiclass svm loss for a single example (x,y)\n",
    "  - x is a column vector representing an image (e.g. 3073 x 1 in CIFAR-10)\n",
    "    with an appended bias dimension in the 3073-rd position (i.e. bias trick)\n",
    "  - y is an integer giving index of correct class (e.g. between 0 and 9 in CIFAR-10)\n",
    "  - W is the weight matrix (e.g. 10 x 3073 in CIFAR-10)\n",
    "  \"\"\"\n",
    "  delta = 1.0 # see notes about delta later in this section\n",
    "  scores = W.dot(x) # scores becomes of size 10 x 1, the scores for each class\n",
    "  correct_class_score = scores[y]\n",
    "  D = W.shape[0] # number of classes, e.g. 10\n",
    "  loss_i = 0.0\n",
    "  for j in range(D): # iterate over all wrong classes\n",
    "    if j == y:\n",
    "      # skip for the true class to only loop over incorrect classes\n",
    "      continue\n",
    "    # accumulate loss for the i-th example\n",
    "    loss_i += max(0, scores[j] - correct_class_score + delta)\n",
    "  return loss_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_i_vectorized(x, y, W):\n",
    "  \"\"\"\n",
    "  A faster half-vectorized implementation. half-vectorized\n",
    "  refers to the fact that for a single example the implementation contains\n",
    "  no for loops, but there is still one loop over the examples (outside this function)\n",
    "  \"\"\"\n",
    "  delta = 1.0\n",
    "  scores = W.dot(x)\n",
    "  # compute the margins for all classes in one vector operation\n",
    "  margins = np.maximum(0, scores - scores[y] + delta)\n",
    "  # on y-th position scores[y] - scores[y] canceled and gave delta. We want\n",
    "  # to ignore the y-th position and only consider margin on max wrong class\n",
    "  margins[y] = 0\n",
    "  loss_i = np.sum(margins)\n",
    "  return loss_i\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "## Fully vectorized implementation\n",
    "def L(X,y,W):\n",
    "     \n",
    "     \"\"\"\n",
    "    Fully-vectorized implementation of the loss function.\n",
    "    - X: Holds all the training examples as columns (e.g., 3073 x 50,000 in CIFAR-10)\n",
    "    - y: Array of integers specifying the correct class (e.g., 50,000-D array)\n",
    "    - W: Weights (e.g., 10 x 3073)\n",
    "    \"\"\"\n",
    "     #compute scores for all classes\n",
    "     scores=np.dot(W,X)\n",
    "\n",
    "     #select only the scores for the correct class for each example\n",
    "     correct_class_scores=scores[y,np.arange(X.shape[1])] \n",
    "     #compute the hinge loss for all classes\n",
    "     margins=np.maximum(0,scores-correct_class_scores+1)\n",
    "     margins[y,np.arange(X.shape[1])]=0  #set loss for correct class 0\n",
    "\n",
    "     #compute the overall loss as the average of all hinge losses\n",
    "     loss=np.mean(margins)\n",
    "     return loss\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
