{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principal Component Analysis(PCA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**prerequisite**: \n",
    "- covariance matrix\n",
    "- eigen values eigen vectors\n",
    "\n",
    "**Principal Components**:Principal components are new variables that are constructed as linear combinations or mixtures of the initial variables. These combinations are done in such a way that the new variables (i.e., principal components) are uncorrelated and most of the information within the initial variables is squeezed or compressed into the first components. So, the idea is 10-dimensional data gives you 10 principal components, but PCA tries to put maximum possible information in the first component, then maximum remaining information in the second and so on, until having something like shown in the scree plot below.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## introduction\n",
    "PCA is a dimensionality reducion procedure. It reduces higher dimensions of a given dataset into lower dimension to visualize and to work efficiently. PCA does not reduces number of features manually, rather it finds the principal components of the datasets and then condider these components as new datasets. This is done with help of covariance matrix and eigen vectors. Although for this we have to consider a little bit more loss but efficient model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm\n",
    "- Standardize the range of continuous initial variables\n",
    "- Compute the covariance matrix to identify correlations\n",
    "- Compute the eigenvectors and eigenvalues of the covariance matrix to identify the principal components\n",
    "- Create a feature vector to decide which principal components to keep\n",
    "- Recast the data along the principal components axes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1:\n",
    "**standardization**: Before applying PCA it is important to standardize the data. Because PCA is sensitive to the variance of the data. For example, if one value ranges from (1-100) will have greater impact than the value having range (0-1),which lead to biased result.\n",
    "Formula:  x=(x-mean)/std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 11.5  -1. ]\n",
      " [  3.5  -4. ]\n",
      " [  1.5   0. ]\n",
      " [-16.5   5. ]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# step 1 standardization\n",
    "# initializing data\n",
    "X=np.array([68,60,58,40],dtype=float)\n",
    "Y=np.array([29,26,30,35],dtype=float)\n",
    "data=np.vstack((X,Y))\n",
    "data=data.T # now each column represents a example\n",
    "data-=np.mean(data,axis=0)\n",
    "print(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### step 2: \n",
    "**Covarinace matrix**:\n",
    "The goal of covariance matrix is to find how data points are dispersed from the mean. Moreover, we can see correlation of data points and truncate redundant features.\n",
    "\n",
    "We have discussed covariance matrix earlier in another section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[104.75 -27.  ]\n",
      " [-27.    10.5 ]]\n"
     ]
    }
   ],
   "source": [
    "# covariance matrix\n",
    "cov_matrix=np.dot(data.T,data)/data.shape[0]\n",
    "print(cov_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Computing the eigen vectiors and eigen values of the covariance matrix to identify principal components\n",
    "principal components represent the directions of the data that explain a maximal amount of variance, that is to say, the lines that capture most information of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "eigen vectors and eigen values in python . first eigen values $A-\\lambda I$ to determine lamda. then with lamda values we will determine eigen vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eigen values : [111.93674482   3.31325518]\n",
      "eigen vectors \n",
      " [[ 0.96635295  0.25721971]\n",
      " [-0.25721971  0.96635295]]\n"
     ]
    }
   ],
   "source": [
    "# eigen values\n",
    "#I=np.eye(*cov_matrix.shape)\n",
    "eigen_val, eigen_vec=np.linalg.eig(cov_matrix)\n",
    "print(f\"eigen values : {eigen_val}\")\n",
    "print(f\"eigen vectors \\n {eigen_vec}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for eigen value 1 the eigen vector is [ 0.96635295 -0.25721971] \n",
      "for eigen value 2 the eigen vector is [0.25721971 0.96635295] \n"
     ]
    }
   ],
   "source": [
    "for i in range(len(eigen_val)):\n",
    "    print(f\"for eigen value {i+1} the eigen vector is {eigen_vec[:,i]} \")\n",
    "                     \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **Summary**: The dataset has two principal components. The first principal component explains the majority of the variance in the data, and its direction is given by the first eigenvector. The second principal component explains less variance and is orthogonal to the first principal component, with its direction given by the second eigenvector.\n",
    "\n",
    "  - eigen vectors are actually the directions of the axes where there is the most variance (most information) and that we call Principal Components.\n",
    "  - eigenvalues are simply the coefficients attached to eigenvectors, which give the amount of variance carried in each Principal Component."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA analysis\n",
    "In our example first eigen value is greater than second. This meas that eigen vector that corresponds to the first principal component(pc1) is v1 and the other which corresponds to (pc2) is v2.\n",
    "\n",
    "After having the principal components, to compute the percentage of variance (information) accounted for by each component, we divide the eigenvalue of each component by the sum of eigenvalues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PC1 = 97.125%, PC2 = 2.875%\n"
     ]
    }
   ],
   "source": [
    "first_component_var=(eigen_val[0]/np.sum(eigen_val))*100\n",
    "second_component_var=(eigen_val[1]/np.sum(eigen_val))*100\n",
    "print(f\"PC1 = {first_component_var:.3f}%, PC2 = {second_component_var:.3f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4 : feature vector\n",
    "As we saw in the previous step, computing the eigenvectors and ordering them by their eigenvalues in descending order, allow us to find the principal components in order of significance. In this step, what we do is, to choose whether to keep all these components or discard those of lesser significance (of low eigenvalues), and form with the remaining ones a matrix of vectors that we call Feature vector.\n",
    "\n",
    "So, the feature vector is simply a matrix that has as columns the eigenvectors of the components that we decide to keep. This makes it the first step towards dimensionality reduction, because if we choose to keep only p eigenvectors (components) out of n, the final data set will have only p dimensions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our upper example we can consider both eig vectors and construct feature vector as follows:\n",
    "\n",
    "\\begin{bmatrix}\n",
    "0.96635295 & 0.25721971\\\\\n",
    "-0.25721971&  0.96635295\n",
    "\\end{bmatrix}\n",
    "\n",
    "where each row represents a eigen vectors.\n",
    "\n",
    "Or we can consider only first eigen vector to cnstruct feature vector as follows\n",
    "\n",
    "\\begin{bmatrix}\n",
    "0.96635295\\\\ \n",
    "-0.25721971\n",
    "\\end{bmatrix}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is how two dimension can be transformed into one dimension. In real world problem the dimensions are much more higher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.96635295 -0.25721971]\n"
     ]
    }
   ],
   "source": [
    "# we are considering only first eigen vector\n",
    "featur_vec=np.array(eigen_vec[:,0])\n",
    "print(featur_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2,)\n",
      "(2, 1)\n"
     ]
    }
   ],
   "source": [
    "print(featur_vec.shape)\n",
    "featur_vec=featur_vec.reshape(2,1)\n",
    "print(featur_vec.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recast the data along the principal components axes\n",
    "\n",
    "In this last step we will reorient the data in the main datasets along the principal component axes. This can be done by myltiplying the (maindata.T * feature vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 11.37027863]\n",
      " [  4.41111415]\n",
      " [  1.44952942]\n",
      " [-17.2309222 ]]\n"
     ]
    }
   ],
   "source": [
    "Final_dataset=np.dot(data,featur_vec)\n",
    "print(Final_dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
